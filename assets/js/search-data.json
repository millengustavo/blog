{
  
    
        "post0": {
            "title": "Federated Learning of Cohorts (FLoC) - Google's solution for interest based advertising in a world without third-party cookies",
            "content": ". Photo by Food Photographer | Jennifer Pallian on Unsplash . The code is available here https://github.com/millengustavo/floc-experiment . The end of third-party cookies for advertisers . Third-party cookies have (since 1994) been a key enabler of the commercial Internet and fine-grained digital ad targeting . They helped achieve unprecedented audience segmentation and attribution - helping to connect marketing tactics with results in ways that were virtually impossible in the most traditional forms of media. . To bring users more transparency and better consent management, most browsers are ending support for third-party cookies. . Firefox 79 clears redirect tracking cookies every 24 hours | Apple teases new tracking protections and an approximate location feature in iOS 14 | Google has announced plans to stop supporting third-party cookies on its Chrome browser in 2021 | . Some alternatives are being proposed to replace the need for third-party cookies, ensuring users’ privacy, but without loss of performance for advertisers. . In this post you will learn a little more about FLoC, an alternative proposed by Google, and we will navigate through a simplified demonstration of the algorithm using a public dataset. . FLoC . Goal . “Preserve interest based advertising, but in a privacy-preserving manner” . Overview . Relies on a cohort assigning mechanism: a function that allocates a cohort id to a user based on their browsing history | This cohort id must be shared by at least k distinct users for privacy | . Privacy x Utility . “The more users share a cohort id, the harder it is to derive individual user’s behavior from across the web. On the other hand, a large cohort is more likely to have a diverse set of users, thus making it harder to use this information for fine-grained ads personalization purposes.” . Ideal cohort assignment: group together a large number of users interested in similar things . Intersections with Data Science . Federated Learning: machine learning technique that trains an algorithm across multiple decentralized edge devices or servers holding local data samples, without exchanging them | Cohort assignment algorithm should be unsupervised, since each provider has their own optimization function | . Evaluating Google’s approach on a public dataset . Let’s evaluate SimHash (originally developed to identify near duplicate documents quickly) proposed in the FLoC whitepaper as a cohort assignment mechanism using the dataset MovieLens 25M | . “MovieLens 25M movie ratings. Stable benchmark dataset. 25 million ratings and one million tag applications applied to 62,000 movies by 162,000 users.” . Installing the SimHash Python package . !git clone https://github.com/scrapinghub/python-simhash !cd python-simhash &amp;&amp; python setup.py install . import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import MultiLabelBinarizer from wordcloud import WordCloud from simhash import weighted_fingerprint, fnvhash . Downloading MovieLens 25m . !wget https://files.grouplens.org/datasets/movielens/ml-25m.zip --no-check-certificate !unzip ml-25m.zip . movies = pd.read_csv(&quot;ml-25m/movies.csv&quot;) ratings = pd.read_csv(&quot;ml-25m/ratings.csv&quot;) # join movie genres with user ratings df = ratings[[&quot;userId&quot;, &quot;movieId&quot;, &quot;rating&quot;]].merge(movies[[&quot;movieId&quot;, &quot;genres&quot;]], on=&quot;movieId&quot;) df[&quot;genres&quot;] = df[&quot;genres&quot;].apply(lambda x: x.split(&quot;|&quot;)) # create a genre per column dataset mlb = MultiLabelBinarizer(sparse_output=True) transformed_df = df.join( pd.DataFrame.sparse.from_spmatrix( mlb.fit_transform(df.pop(&quot;genres&quot;)), index=df.index, columns=mlb.classes_, ) ) # multiply user rating to each genre to give us an idea of an weighted genre vector for each user my_genres = [col for col in transformed_df.columns if col not in [&quot;userId&quot;, &quot;movieId&quot;, &quot;rating&quot;]] for genre in my_genres: transformed_df[genre] = transformed_df[&quot;rating&quot;] * transformed_df[genre] transformed_df[genre] = np.asarray(transformed_df[genre]).astype(&quot;int8&quot;) # compute each user&#39;s mean genre vector transformed_df = transformed_df.drop(columns=[&quot;rating&quot;, &quot;movieId&quot;]) transformed_df = transformed_df.groupby(by=&quot;userId&quot;).mean() . SimHash . Having computed each users’ mean genre vector preferences, we can compute the SimHash on this vector, so each user interest will be represented by some hash of all of his preferences combined (with collisions) . def simhash(v): v = dict(v) return weighted_fingerprint([(fnvhash(k), w) for k, w in v.items()]) transformed_df[&#39;hash&#39;] = transformed_df.apply(simhash, axis=1) . We can see that we have a lot of collisions using SimHash, but this is expected, since many users share similar preferences and our choice of hashing algorithm is intentional | SimHash is computationally inexpensive by design, not caring too much about hash collisions | . Defining a limited number of clusters for demonstration purposes . transformed_df[&quot;cluster&quot;] = pd.cut(transformed_df[&quot;hash&quot;], bins=5, labels=[&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;]) results = transformed_df.drop(columns=&#39;hash&#39;).groupby(&#39;cluster&#39;).mean() . Visualizing the clusters . def plot_cluster_wordcloud(cluster_name): cluster_text = results.loc[results.index == str(cluster_name)].to_dict(orient=&#39;records&#39;)[0] wordcloud = WordCloud(width=800, height=450, background_color=&quot;white&quot;).generate_from_frequencies(cluster_text) plt.figure(figsize=(16,9)) plt.imshow(wordcloud) plt.axis(&quot;off&quot;); . Cluster 1 . Action, Adventure, Western, IMAX . plot_cluster_wordcloud(1) . . Cluster 2 . Drama, Romance . plot_cluster_wordcloud(2) . . Cluster 3 . Crime, Documentary, Mistery, Film-Noir . plot_cluster_wordcloud(3) . . Cluster 4 . Horror, Sci-Fi, Thriller . plot_cluster_wordcloud(4) . . Cluster 5 . Animation, Children, Comedy, Fantasy, Musical . plot_cluster_wordcloud(5) . . Conclusion . With the growing concern for users’ privacy, some machine learning techniques have shown promise. Federated learning seems to be an interesting alternative for this type of application and it is worth studying it further. . I recommend that you read more about Privacy Sandbox, Chrome’s initiative to, according to Google, “help publishers and advertisers succeed, while protecting people’s privacy.” . References . https://www.deloittedigital.com/us/en/blog-list/2020/what-the-end-of-third-party-cookies-means-for-advertisers.html | https://venturebeat.com/2020/08/04/mozilla-firefox-79/ | https://www.theverge.com/2020/6/22/21299407/apple-ios-14-new-privacy-features-data-location-tracking-premissions-wwdc-2020 | https://blog.chromium.org/2020/01/building-more-private-web-path-towards.html | https://github.com/google/ads-privacy/blob/master/proposals/FLoC/FLOC-Whitepaper-Google.pdf | https://blog.google/products/ads-commerce/2021-01-privacy-sandbox | https://github.com/scrapinghub/python-simhash | https://towardsdatascience.com/federated-learning-of-cohorts-googles-cookie-killer-7f63b2395173 | .",
            "url": "https://millengustavo.github.io/blog/advertising/machine%20learning/python/data%20science/2021/03/14/floc-experiment.html",
            "relUrl": "/advertising/machine%20learning/python/data%20science/2021/03/14/floc-experiment.html",
            "date": " • Mar 14, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Machine Learning Engineering",
            "content": "My notes and highlights on the book. . Author: Andriy Burkov . Available here . Ch1. Introduction . When we deploy a model in production, we usually deploy an entire pipeline . Machine learning engineering (MLE): . encompasses data collection, model training, making the model available for use | includes any activity that lets ML algorithms be implemented as a part of an effective production system | . ML Engineer: . concerned with sourcing the data (from multiple locations), preprocessing it, programming features, training an effective model that will coexist in production with other processes | stable, maintanable and easily accessible | ML systems “fail silently” -&gt; must be capable of preventing such failures or to know how to detect and handle them | . When to use ML . Your problem: . too complex for coding | constantly changing | perceptive (image, text, etc) | unstudied phenomenon | has a simple objective | it is cost-effective | . When not to use ML . explainability is needed | errors are intolerable | traditional SWE is a less expensive option | all inputs and outputs can be enumerated and saved in a DB | data is hard to get or too expensive | . ML Project Life Cycle . . Ch2. Before the Project Starts . Impact of ML . High when: . ML can replace a complex part of your engineering project | there’s great benefit in getting inexpensive (but probably imperfect) predictions | . Cost of ML . Factors: . difficulty of the problem | cost of data | need for accuracy | . Nonlinear progress . Progress in ML is nonlinear. Prediction error decreases fast in the beginning, but then gradually slows down . Make sure the PO (or client) understands the constraints and risks | Log every activity and track the time it took (helps with reporting and estimations of complexity in the future) | . Why ML projects fail . lack of experienced talent | lack of support by the leadership | missing data infrastructure | data labeling challenge | siloed organizations and lack of collaboration | technically infeasible projects | lack of alignment between technical and business teams | . Ch3. Data Collections and Preparation . Train, Validation and Test sets partition . Data was randomized before the split | Split was applied to raw data | Validation and test sets follow the same distribution | Leakage was avoided | . Data Sampling strategies . random sampling | systematic sampling | stratified sampling | cluster sampling | . Data versioning is a critical element in supervised learning when the labeling is done by multiple labelers . Dataset Documentation . what the data means | how it was collected | methods used to creat it | details of train-validation-test splits | details of all pre-processing steps | explanation of any data that were excluded | format used to store it | types of attributes/features | number of examples | possible values for labels / allowable range for a numerical target | . Ch4. Feature Engineering . Good features . high predictive power | can be computed fast | reliable | uncorrelated | . The distribution of feature values in the training set should be similar to the distribution of values the production model will receive . Feature selection techniques . Cutting the long tail | Boruta | L1 regularization | . Best practices . scale features | store and document in schema files or feature stores | keep code, model and training data in sync | . “Feature extraction code is one of the most important parts of a machine learning system. It must be extensively and systematically tested” . Ch5. Supervised Model Training (Part 1) . Baseline . Baseline: a model or algorithm that provides a reference point for comparison. Establish a baseline performance on your problem before start working on a predictive model. . simple learning algorithm or | rule-based or heuristic algorithm (simple statistic) random prediction | zero rule algorithm (e.g., always predict the most common class in the training set / average if regression) | . | human baseline: Amazon Mechanical Turk (MT) service -&gt; web-platform where people solve simple tasks for a reward | . In-memory vs. out-of-memory . If the dataset can’t be fully loaded in RAM -&gt; incremental learning algorithms: can improve the model by reading data gradually (Naive Bayes, neural networks) . Precision and Recall . Precision: ratio of true positive predictions to the overall number of positive PREDICTIONS | Recall: ratio of true positive predictions to the overall number of positive EXAMPLES | . F-measure . positive real beta | beta = 2 -&gt; weighs recall twice as high as precision | beta = 0.5 -&gt; weighs recall twice as low as precision | . Precision-recall and bias-variance tradeoffs . By varying the complexity of the model, we can reach the so-called “zone of solutions”, a situation in which both bias and variance of the model are relatively low. The solution that optimizes the performance metric is usually found inside that zone . Ch6. Supervised Model Training (Part 2) . - . Ch7. Model Evaluation . Tasks . estimate legal risks of putting the model in production | understand the distribution of the data used to train the model | evaluate the performance of the model prior to deployment | monitor the performance of the deployed model | . A/B Testing . A: served the old model | B: served the new model | apply a statistical significance test to decide whether the new model is statistically different from the old model | . Multi-armed bandit . start by randomly exposing all models to the users | gradually reduce the exposure of the least-performing models until only one (the best performing) gets served most of the time | . Bootstrapping . technique (statistical procedure) to compute a statistical interval for any metric | consists of building N samples of a dataset | then training a model | and computing some statistic using each of those N samples | . Ch8. Model Deployment . Deployment patterns . statically installable binary of the entire software | positive: fast execution time for the user; don’t have to upload user data to server (user privacy); can be called when the user is offline; keeping the model operation is user’s responsibility | negative: hard to upgrade model without upgrading whole app; may have messy computational requirements; difficult to monitor the model performance | . | dynamically on the user’s device similar to static (user runs part of the system on their device), but the model is not a part of the binary code of the app | positive: better separation of concerns (easier to update); fast for the user (cheaper for the org’s servers) | negative: varies depending on strategy; difficult to monitor the model performance | . | dynamically on a server: place the model on servers and make it available as REST API or gRPC service | . | model streaming | . Deployment strategies . single: simplest -&gt; serialize new model to file, replace the old one | silent: new and old version runs in parallel during the switch | canary: pushes new version to a small fraction of users, while keep the old one running for most | multi-armed bandit (MAB): way to compare one or more versions of the model in the production env, and select the best performing one | . “The model must be applied to the end-to-end and confidence test data by simulating a regular call from the outside” . Algorithmic efficiency . important consideration in model deployment | you should only write your own code when it’s absolutely necessary | caching speeds up the application when it contains resource-consuming functions frequently called with the same parameter values | . Ch9. Model Serving, Monitoring, and Maintenance . Effective runtime . secure | correct | ensures ease of deployment and recovery | provides guarantees of model validity | avoids training/serving skew and hidden feedback loops | . Serving modes . batch: when applied to big data and some latency is tolerable | on-demand: wrapped into a REST API | . What can go wrong with the model in production . more training data made the model worse | properties of the production data changed | updated feature extraction code | resource needed for feature changed/unavailable | model is abused or under an adversarial attack | . Monitoring . automated value calculation for the performance metrics -&gt; send alert if change significantly | distribution shift | numerical instability | decreasing computational performance | logs | . Maintenance . “Most ML models must be regularly or occasionally updated” . how often? . error rate / how critical | only useful if fresh | new training data available fast | time it takes to retrain | cost to train / deploy the model | importance of update for improving the metrics | . Ch10. Conclusion . - .",
            "url": "https://millengustavo.github.io/blog/book/machine%20learning%20engineering/python/data%20science/2021/01/30/ml-engineering.html",
            "relUrl": "/book/machine%20learning%20engineering/python/data%20science/2021/01/30/ml-engineering.html",
            "date": " • Jan 30, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Learning Python Design Patterns",
            "content": "My notes and highlights on the book. . Author: Chetan Giridhar . Available here . Ch1. Introduction to design patterns . Understanding object-oriented programming . Concept of objects that have attributes (data members) and procedures (member functions) | Procedures are responsible for manipulating the attributes | Objects, which are instances of classes, interact among each other to serve the purpose of an application under development | . Classes . Define objects in attributes and behaviors (methods) | Classes consist of constructors that provide the initial state for these objects | Are like templates and hence can be easily reused | . Methods . Represent the behavior of the object | Work on attributes and also implement the desired functionality | . Major aspects of OOP . Encapsulation . An object’s behavior is kept hidden from the outside world or objects keep their state information private | Clients can’t change the object’s internal state by directly acting on them | Clients request the object by sending requests. Based on the type, objects may respond by changing their internal state using special member functions such as get and set | . Polymorphism . Can be of two types: An object provides different implementations of the method based on input parameters | The same interface can be used by objects of different types | . | In Python polymorphism is a feature built-in for the language (e.g. + operator) | . Inheritance . Indicates that one class derives (most of) its functionality from the parent class | An option to reuse functionality defined in the base class and allow independent extensions of the original software implementation | Creates hierarchy via the relationships among objects of different classes | Python supports multiple inheritance (multiple base classes) | . Abstraction . Provides a simple interface to the clients. Clients can interact with class objects and call methods defined in the interface | Abstracts the complexity of internal classes with an interface so that the client need not be aware of internal implementations | . Composition . Combine objects or classes into more complex data structures or software implementations | An object is used to call member functions in other modules thereby making base functionality available across modules without inheritance | . Object-oriented design principles . The open/close principle . Classes or objects and methods should be open for extension but closed for modifications . Make sure you write your classes or modules in a generic way | Existing classes are not changed reducing the chances of regression | Helps maintain backward compatibility | . The inversion of control principle . High-level modules shouldn’t be dependent on low-level modules; they should be dependent on abstractions. Details should depend on abstractions and not the other way round . The base module and dependent module should be decoupled with an abstraction layer in between | The details of your class should represent the abstractions | The tight coupling of modules is no more prevalent and hence no complexity/rigidity in the system | Easy to deal with dependencies across modules in a better way | . The interface segregation principle . Clients should not be force to depend on interfaces they don’t use . Forces developers to write thin interfaces and have methods that are specific to the interface | Helps you not to populate interfaces by adding unintentional methods | . The single responsibility principle . A class should have only one reason to change . If a class is taking care of two functionalities, it is better to split them | Functionality = a reason to change | Whenever there is a change in one functionality, this particular class needs to change, and nothing else | If a class has multiple functionalities, the dependent classes will have to undergo changes for multiple reasons, which gets avoided | . The substitution principle . Derived classes must be able to completely substitute the base classes . The concept of design patterns . Solutions to given problems | Design patterns are discoveries and not a invention in themselves | Is about learning from others’ successes rather than your own failures! | . Advantages of design patterns . Reusable across multiple projects | Architectural level of problems can be solved | Time-tested and well-proven, which is the experience of developers and architects | They have reliability and dependence | . Patterns for dynamic languages . Python: . Types or classes are objects at runtime | Variables can have type as a value and can be modified at runtime | Dynamic languages have more flexibility in terms of class restrictions | Everything is public by default | Design patterns can be easily implemented in dynamic languages | . Classifying patterns . Creational | Structural | Behavioral | . Classification of patterns is done based primarily on how the objects get created, how classes and objects are structured in a software application, and also covers the way objects interact among themselves . Creational patterns . Work on the basis of how objects can be created | Isolate the details of object creation | Code is independent of the type of object to be created | . Structural patterns . Design the structure of objects and classes so that they can compose to achieve larger results | Focus on simplifying the structure and identifying the relationship between classes and objects | Focus on class inheritance and composition | . Behavioral patterns . Concerned with the interaction among objects and responsibilities of objects | Objects should be able to interact and still be loosely coupled | . Ch2. The singleton design pattern . Typically used in logging or database operations, printer spoolers, thread pools, caches, dialog boxes, registry settings, and so on | Ensure that only one object of the class gets created | Provide an access point for an object that is global to the program | Control concurrent access to resources that are shared | Make the constructor private and create a static method that does the object initialization | Override the __new__ method (Python’s special method to instantiate objects) to control the object creation | Another use case: lazy instantiation. Makes sure that the object gets created when it’s actually needed | All modules are Singletons by default because of Python’s importing behavior | . Monostate Singleton pattern . All objects share the same state | Assign the __dict__ variable with the __shared_state class variable. Python uses __dict__ to store the state of every object of a class | . Singletons and metaclasses . A metaclass is a class of a class | The class is an instance of its metaclass | Programmers get an opportunity to create classes of their own type from the predefined Python classes | . Drawbacks . Singletons have a global point of access | Al classes that are dependent on global variables get tightly coupled as a change to the global data by one class can inadvertently impact the other class | . Ch3. The factory pattern - building factories to create objects . Understanding the factory pattern . Factory = a class that is responsible for creating objects of other types | The class that acts as a factory has an object and methods associated with it | The client calls this method with certain parameters; objects of desired types are created in turn and returned to the client by the factory | . Advantages . Loose coupling: object creation can be independent of the class implementation | The client only needs to know the interface, methods, and parameters that need to be passed to create objects of the desired type (simplifies implementations for the client) | Adding another class to the factory to create objects of another type can be easily done without the client changing the code | . The simple factory pattern . Not a pattern in itself | Helps create objects of different types rather than direct object instantiation | . The factory method pattern . We define an interface to create objects, but instead of the factory being responsible for the object creation, the responsibility is deferred to the subclass that decides the class to be instantiated | Creation is through inheritance and not through instantiation | Makes the design more customizable. It can return the same instance or subclass rather than an object of a certain type | . The factory method pattern defines an interface to create an object, but defers the decision ON which class to instantiate to its subclasses . Advantages . Makes the code generic and flexible, not being tied to a certain class for instantiation. We’re dependent on the interface (Product) and not on the ConcreteProduct class | Loose coupling: the code that creates the object is separate from the code that uses it | The client don’t need to bother about what argument to pass and which class to instantiate -&gt; the addition of new classes is easy and involves low maintenance | . The abstract factory pattern . Provide an interface to create families of related objects without specifying the concrete class . Makes sure that the client is isolated from the creation of objects but allowed to use the objects created | . Factory method versus abstract factory method . Factory method Abstract Factory method . Exposes a method to the client to create the objects | Contains one or more factory methods of another class | . Uses inheritance and subclass to decide which object to create | Uses composition to delegate responsibility to create objects of another class | . Is used to create one product | Is about creating families of related products | . Ch4. The façade pattern - being adaptive with façade . Understanding Structural design patterns . Describe how objects and classes can be combined to form larger structures. Structural patterns are a combination of class and object patterns | Ease the design by identifying simpler ways to realize or demonstrate relationships between entities | Class patterns: describe abstraction with the help of inheritance and provide a more useful program interface | Object patterns: describe how objects can be associated and composed to form larger objects | . Understanding the Façade design pattern . Façade hides the complexities of the internal system and provides an interface to the client that can access the system in a very simplified way . Provides an unified interface to a set of interfaces in a subsystem and defines a high-level interface that helps the client use the subsystem in an easy way | Discusses representing a complex subsystem with a single interface object -&gt; it doesn’t encapsulate the subsystem, but actually combines the underlying subsystems | Promotes the decoupling of the implementation with multiple clients | . Main participants . Façade: wrap up a complex group of subsystems so that it can provide a pleasing look to the outside world | System: represents a set of varied subsystems that make the whole system compound and difficult to view or work with | Client: interact with the façade so that it can easily communicate with the subsystem and get the work completed (doesn’t have to bother about the complex nature of the system) | . The principle of least knowledge . Design principle behind Façade pattern | Reduce the interactions between objects to just a few friend that are close enough to you | . The Law of Demeter . Design guideline: Each unit should have only limited knowledge of other units of the system | A unit should talk to its friends only | A unit should not know about the internal details of the object that it manipulates | . | . The principle of least knowledge and Law of Demeter are the same and both point to the philosophy of loose coupling . Ch5. The proxy pattern - controlling object access . Proxy: a system that intermediates between the seeker and provider. Seeker is the one that makes the request, and provider delivers the resources in response to the request . A proxy server encapsulates requests, enables privacy, and works well in distributed architectures | Proxy is a wrapper or agent object that wraps the real serving object | Provide a surrogate or placeholder for another object in order to control access to a real object | Some useful scenarios: Represents a complex system in a simpler way | Acts as a shield against malicious intentions and protect the real object | Provides a local interface for remote objects on different servers | Provides a light handle for a higher memory-consuming object | . | . Data Structure components . Proxy | Subject/RealSubject | Client | . Different types of proxies . Virtual proxy: placeholder for objects that are very heavy to instantiate | Remote proxy: provides a local representation of a real object that resides on a remote server or different address space | Protective proxy: controls access to the sensitive matter object of RealSubject | Smart proxy: interposes additional actions when an object is accessed | . Proxy Façade . Provides you with a surrogate or placeholder for another object to control access to it | Provides you with an interface to large subsystems of classes | . A Proxy object has the same interface as that of the target object and holds references to target objects | Minimizes the communication and dependencies between subsystems | . Acts as an intermediary between the client and object that is wrapped | Provides a single simplified interface | . Decorator vs Proxy . Decorator adds behavior to the object that it decorates at runtime | Proxy controls access to an object | . Disadvantages . Proxy pattern can increase the response time | . Ch6. The observer pattern - keeping objects in the know . Behavioral patterns . Focus on the responsibilities that an object has | Deal with the interaction among objects to achieve larger functionality | Objects should be able to interact with each other, but they should still be loosely coupled | . Understanding the observer design pattern . An object (Subject) maintains a list of dependents (Observers) so that the Subject can notify all the Observers about the changes that it undergoes using any of the methods defined by the Observer . Defines a one-to-many dependency between objects so that any change in one object will be notified to the other dependent objects automatically | Encapsulates the core component of the Subject | . The pull model . Subject broadcasts to all the registered Observers when there is any change | Observer is responsible for getting the changes or pulling data from the subscriber when there is an amendment | Pull model is ineffective: involves two steps: Subject notifies the Observer | Observer pulls the required data from the Subject | . | . The push model . Changes are pushed by the Subject to the Observer | Subject can send detailed information to the Observer (even though it may not be needed) -&gt; can result in sluggish response times when a large amount of data in sent by the Subject but is never actually used by the Observer | . Loose coupling and the observer pattern . Coupling refers to the degree of knowledge that one object has about the other object that it interacts with | . Loosely-coupled designs allow us to build flexbile object-oriented systems that can handle changes because they reduce the dependency between multiple objects . Reduces the risk that a change made within one element might create an unanticipated impact on the other elements | Simplifies testing, maintenance, and troubleshooting problems | System can be easily broken down into definable elements | . Ch7. The command pattern - encapsulating invocation . Behavioral design pattern in which an object is used to encapsulate all the information needed to perform an action or trigger an event at a later time . Understanding the command design pattern . A Command object knows about the Receiver objects and invokes a method of the Receiver object | Values for parameters of the receiver method are stored in the Command object | The invoker knows how to execute a command | The client creates a Command object and sets its receiver | . Intentions . Encapsulating a request as an object | Allowing the parametrization of clients with different requests | Allowing to save the requests in a queue | Providing an object-oriented callback | . Scenarios of use . Parametrizing objects depending on the action to be performed | Adding actions to a queue and executing requests at different points | Creating a structure for high-level operations that are based on smaller operations | E.g.: Redo or rollback operations | Asynchronous task execution | . | . Advantages . Decouples the classes that invoke the operation from the object that knows how to execute the operation | Provide a queue system | Extensions to add new commands are easy | A rollback system with the command pattern can be defined | . Disadvantages . High number of classes and objects working together to achieve a goal | Every individual command is a ConcreteCommand class that increases the volume of classes for implementation and maintenance | . Ch8. The templated method pattern - encapsulating algorithm . Use cases . When multiple algorithms or classes implements similar or identical logic | The implementation of algorithms in subclasses helps reduce code duplication | Multiple algorithms can be defined by letting the subclasses implement the behavior through overriding | . Intentions . Define a skeleton of an algorithm with primitive operations | Redefine certain operations of the subclass without changing the algorithm’s structure | Achieve code reuse and avoid duplicate efforts | Leverage common interfaces or implementations | . Terms . AbstractClass: Declares an interface to define the steps of the algorithm | ConcreteClass: Defines subclass-specific step definitions | template_method(): Defines the algorithm by calling the step methods | . Hooks . Hook: a method that is declared in the abstract class | Give a subclass the ability to hook into the algorithm whenever needed | Not imperative for the subclass to use hooks | . We use abstract methods when the subclass must provide the implementation, and hook is used when it is optional for the subclass to implement it . The Hollywood principle . Design principle summarized by Don’t call us, we’ll call you | Relates to the template method -&gt; it’s the high-level abstract class that arranges the steps to define the algorithm | . Advantages . No code duplication | Uses inheritance and not composition -&gt; only a few methods need to be overriden | Flexibility lets subclasses decide how implement steps in an algorithm | . Disadvantages . Confusing debugging and undestanding the sequence of flow | Documentation and strict error handling has to be done by the programmer | Maintenance can be a problem -&gt; changes to any level can disturb the implementation | . Ch9. Model-View-Controller - Compound patterns . “A compound pattern combines two or more patterns into a solution that solves a recurring or general problem” - GoF . A compound pattern is not a set of patterns working together; it is a general solution to a problem . The Model-View-Controller pattern . Model represents the data and business logic: how information is stored and queried | View is nothing but the representation: how it is presented | Controller is the one that directs the model and view to behave in a certain way: it’s the glue between the two | The view and controller are dependent on the model, but not the other way around | . Terms . Model - knowledge of the application: store and manipulate data (create, modify and delete). Has state and methods to change states, but is not aware of how the data would be seen by the client | View - the appearance: build user interfaces and data displays (it should not contain any logic of its own and just display the data it receives) | Controller - the glue: connects the model and view (it has methods that are used to route requests) | User: requests for certain results based on certain actions | . Intention . Keep the data and presentation of the data separate | Easy maintenance of the class and implementation | Flexibility to change the way in which data is stored and displayed -&gt; both are independent and hence have the flexibility to change | . The MVC pattern in the real world . Django or Rails | MTV (Model, Template, View): model is the database, templates are the views, and controllers are the views/routes | . Benefits of the MVC pattern . Easy maintenance | Loose coupling | Decrease complexity | Development efforts can run independently | . Ch10. The state design pattern . Behavioral design pattern | Sometimes referred to as an objects for states pattern | Used to develop Finite State Machines and helps to accommodate State Transaction Actions | . Understanding the state design pattern . State: an interface that encapsulates the object’s behavior. This behavior is associated with the state of the object | ConcreteState: a subclass that implements the State interface -&gt; implements the actual behavior associated with the object’s particular state | Context: the interface of interest to clients. Also maintains an instance of the ConcreteState subclass that internally defines the implementation of the object’s particular state | . Advantages . Removes the dependency on the if/else or switch/else conditional logic | Benefits of implementing polymorphic behavior, also easier to add states to support additional behavior | Improves cohesion: state-specific behaviors are aggregated into the ConcreteState classes, which are placed in one location in the code | Improves the flexibility to extend the behavior of the application and overall improves code maintenance | . Disadvantages . Class explosion: every state needs to be defined with the help of ConcreteState -&gt; might end up writing many more classes with a small functionality | Context class needs to be updated to deal with each behavior | . Ch11. AntiPatterns . Four aspects of a bad design: . Immobile: hard to reuse | Rigid: any small change may in turn result in moving too many parts of the software | Fragile: any change results in breaking the existing system fairly easy | Viscose: changes are done in the code or envinronment itself to avoid difficult architectural level changes | . An AntiPattern is an outcome of a solution to recurring problems so that the outcome is innefective and becomes counterproductive . AntiPatterns may be the result of: . A developer not knowing the software development practices | A developer not applying design patterns in the correct context | . Software development AntiPatterns . Software deviates from the original code structure due to: . The tought process of the developer evolves with development | Use cases change based on customer feedback | Data structures designed initially may undergo change with functionality or scalability considerations | . Refactoring is one of the critical parts of the software development journey, which provides developers an opportunity to relook the data structures and think about scalability and ever-evolving customer’s needs . Spaghetti code . Minimum reuse of structures is possible | Maintenance efforts are too high | Extension and flexibility to change is reduced | . Golden Hammer . One solution is obsessively applied to all software projects | The product is describe, not by the features, but the technology used in development | In the company corridors, you hear developers talking, “That could have been better than using this” | Requirements are not completed and not in sync with user expectations | . Lava Flow . Low code coverage for developed tests | Commented code without reasons | Obsolete interfaces, or developers try to work around existing code | . Copy-and-paste or cut-and-paste programming . Similar type of issues across software application | Higher maintenance costs and increased bug life cycle | Less modular code base with the same code running into a number of lines | Inheriting issues that existed in the first place | . Software architecture AntiPatterns . Software architecture looks at modeling the software that is well understood by the development and test teams, product managers, and other stakeholders . Reinventing the wheel . Too many solutions to solve one standard problem, with many of them not being well thought out | More time and resource utilization for the engineering team leading overbudgeting and more time to market | A closed system architecture (useful for only one product), duplication of efforts, and poor risk management | . Vendor lock-in . Release cycles and product maintenance cycles of a company’s product releases are directly dependent on the vendor’s release time frame | The product is developed around the technology rather than on the customer’s requirements | The product’s time to market is unreliable and doesn’t meet customer’s expectations | . Design by committee . Conflicting viewpoints between developers and architects even after the design is finalized | Overly complex design that is very difficult to document | Any change in the specification or design undergoes review by many, resulting in implementation delays | .",
            "url": "https://millengustavo.github.io/blog/book/software%20engineering/python/design%20patterns/2020/08/22/python-design-patterns.html",
            "relUrl": "/book/software%20engineering/python/design%20patterns/2020/08/22/python-design-patterns.html",
            "date": " • Aug 22, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Clean Code: A Handbook of Agile Software Craftsmanship",
            "content": "My notes and highlights on the book. . Authors: Robert C. Martin . Available here . Ch1. Clean Code . “Most managers want good code, even when they are obsessing about the schedule (…) It’s your job to defend the code with equal passion” . Clean code is focused: each function, each class, each module exposes a single-minded attitude that remains entirely undistracted, and upolluted, by the surrounding details | Code, without tests, is not clean. No matter how elegant it is, no matter how readable and accessible, if it hath not tests, it be unclean | You will read it, and it will be pretty much what you expected. It will be obvious, simple, and compelling | . Reading vs. Writing . The ratio of time spent reading vs. writing is well over 10:1 | We are constantly reading old code as part of the effort to write new code | We want the reading of code to be easy, even if it makes the writing harder | You cannot write code if you cannot read the surrounding code | If you want to go fast, get done quickly, if you want your code to be easy to write, make it easy to read | . Ch2. Meaningful Names . Use intention-revealing names . Choosing good names takes time, but saves more than it takes. Take care with your names and change them when you find better ones . Avoid disinformation . Avoid leaving false clues that obscure the meaning of code | Avoid words whose entrenched meanings vary from our intended meaning | . Make meaningful distinctions . If names must be different, then they should also mean something different . Use pronounceable names . Humans are good at words | Words are, by definition, pronounceable | . Use searchable names . Single-letter names and numeric constants have a particular problem in that they are not easy to locate across a body of text . Avoid encodings . Encoding type or scope information into names simply adds an extra burden of deciphering . Avoid mental mapping . Clarity is king . Class names . Classes and objects should have noun or noun phrase names | A class name should not be a verb | . Method names . Methods should have verb or verb phrase names . Don’t be cute . Choose clarity over entertainment value | Say what you mean. Mean what you say | . Pick one word per concept . A consistent lexicon is a great boon to the programmers who must use your code . Don’t pun . Avoid using the same word for two purposes -&gt; essentially a pun . Use solution domain names . People who read your code will be programmers | Use CS terms, algorithm names, pattern names, math terms | . Use problem domain names . Separate solution and problem domain concepts | Code that has more to do with problem domain concepts should have names drawn from the problem domain | . Add meaningful context . Most names are not meaningful in and of themselves . Don’t add gratuitous context . Shorter names are generally better than long ones, so long as they are clear | Add no more context to a name than is necessary | . Choosing good names requires good descriptive skills and a shared cultural background. This is a teaching issue rather than a technical, business, or management issue . Ch3. Functions . Small . Functions should be small . Blocks and Indenting . Blocks within if statements, else statements, while statements should be on line long -&gt; probably a function call | Keep the enclosing function small, adds documentary value | Functions should not be large enough to hold nested structures -&gt; makes easier to read and understand | . Do one thing . Functions should do one thing. They should do it well. They should do it only . Reasons to write functions: decompose a larger concept (the name of the function) into a set of steps at the next level of abstraction | Functions that do one thing cannot be divided into sections | . One level of abstraction per function . Once details are mixed with essential concepts, more details tend to accrete within the function | . The Stepdown rule . We want code to be read like a top-down narrative | A set of TO paragraphs, each describing the current level of abstraction and referencing subsequent TO paragraphs at the next level down | . Use descriptive names . Ward’s principle: “You know you are working on clean code when each routine turns out to be pretty much what you expected” . Spend time choosing a name | You should try several different names and read the code with each in place | . Function arguments . Ideal number of arguments for a function: . zero (niladic) | one (monadic) | two (dyadic) | more than that should be avoided where possible . | Arguments are hard from a testing point of view -&gt; test cases for all combinations of arguments | Output arguments are harder to understand than input arguments | Passing a boolean into a function (flag arguments) is a terrible practice -&gt; loudly proclaiming that this function does more than one thing -&gt; does one thing if the flag is true and another if the flag is false! | When a function seems to need more than two or three arguments, it is likely that some of those arguments ought to be wrapped into a class of their own -&gt; When groups of variables are passed together, they are likely part of a concept that deserves a name of its own | Side effects are lies -&gt; your functions promises to do one thing, but it also does other hidden things | Either your function should change the state of an object, or it should return some information about the object | Prefer Exceptions to returing error codes | Extract try/catch blocks into functions of their own | Functions should do one thing -&gt; error handling is one thing | Don’t repeat yourself -&gt; duplication may be the root of all evil in software | . How do you write functions like this? . Writing software is like any other kind of writing . Get your thoughts down first | Massage it until it reads well | The first draft might be clumsy and disorganized, so you restructure it and refine it until it reads the way you want it to read . Every system is built from a domain-specific language designed by the programmers to describe the system. Functions are the verbs of that language, and classes are the nouns. . Ch4. Comments . Comments are always failures. We must have them because we cannot always figure out how to express ourselves without them, but their use is not a cause for celebration | Comments lie. Not always, and not intentionally, but too often | The older a comment is, and the farther away it is from the code it describes, the more likely it is to be wrong | Truth can only be found in the code | Explain your intent in code: create a function that says the same thing as the comment you want to write | A comment may be used to amplify the importance of something that may otherwise seem inconsequential | We have good source code control systems now. Those systems will remember the code for us. We don’t have to comment it out any more. Just delete the code | Short functions don’t need much description -&gt; well-chosen name for a small function that does one thing is better than a comment header | . Ch5. Formatting . Code formatting . Too important to ignore | Is about communication -&gt; developer’s first order of business | . Small files are easier to understand than large files are . The newspaper metaphor . Source file should be like a newspaper article . Name should be simple but explanatory | The name, by itself, should be sufficient to tell us whether we are in the right module or not | . Vertical formatting . Avoid forcing the reader to hop around through the source files and classes | Dependent functions: if one function calls another, they should be vertically close, and the caller should be above the callee | . Horizontal formatting . Strive to keep your lines short | Beyond 100~120 isn’t advisable | . Ch6. Objects and Data Structures . Data/Object anti-symmetry . Objects hide their data behind abstractions and expose functions that operate on that data. Data structure expose their data and have no meaningful functions . Procedural code (code using data structures) makes it easy to add new functions without changing the existing data structures. OO code makes it easy to add new classes without changing existing functions | Procedural code makes it hard to add new data structures because all the functions must change. OO code makes it hard to add new functions because all the classes must change | . Mature programmers know that the idea that everything is an object is a myth. Sometimes you really do want simple data structures with procedures operating on them . Data transfer objects (DTO) . DTO: quintessential form of a data structure -&gt; a class with public variables and no functions . Active records . Special forms of DTOs | Data structures with public (or bean-accessed) variables; but they typically have navigational methods like save and find | . Objects . expose behavior and hide data | easy to add new kinds of objects without changing existing behaviors | hard to add new behaviors to existing objects | . Data Structures . expose data and have no significant behavior | easy to add new behaviors to existing data structures | hard to add new data structures to existing functions | . Ch7. Error Handling . Things can go wrong, and when they do, we as programmers are responsible for making sure that our code what it needs to do . Error handling is important, but if it obscures logic, it’s wrong | It is better to throw an exception when you encounter an error. The calling code is cleaner. Its logic is not obscured by error handling | . Write your Try-Catch-Finally statement first . try blocks are like transactions | Your catch has to leave your program in a consistent state, no matter what happens in the try | Try to write tests to force exceptions, and then add behavior to your handler to satisfy your tests -&gt; cause you to build the transaction scope of the try block first and help maintain the transaction nature of that scope | . Provide context with exceptions . Create informative error messages and pass them along with your exceptions | Mention the operation that failed and the type of failure | If you are logging in your application, pass along enough information to be able to log the error in your catch | . Wrapping third-party APIs is a best practice -&gt; minimize your dependencies upon it: you can choose to move to a different library in the future without much penalty; makes it easier to mock out third-party calls when you are testing your own code . Define the normal flow . Special case pattern: you create a class or configure an object so that it handles a special case for you -&gt; the client code doesn’t have to deal with exceptional behavior . Ch8. Boundaries . It’s not our job to test the third-party code, but it may be in our best interest to write tests for the third-party code we use | Learning tests: call the third-party API, as we expect to use it in our application -&gt; controlled experiments that check our understanding of that API | Clean Boundaries: code at the boundaries needs clear separation and tests that define expectations | . Avoid letting too much of our code know about the third-party particulars. It’s betters to depend on something you control than on something you don’t control, lest it end up controlling you . Ch9. Unit Tests . The three laws of TDD . First Law: You may not write production code until you have written a failing unit test | Second Law: You may not write more of a unit test than is sufficient to fail, and not compiling is failing | Third Law: You may not write more production code than is sufficient to pass the current failing test | . Keeping tests clean . Having dirty tests is equivalent to, if not worse than, having no tests | Tests must change as the production code evolves -&gt; the dirtier the tests, the harder they are to change | If your tests are dirty, you begin to lose the ability to improve the structure of that code Test code is just as important as production code. It requires thought, design, and care. It must be kept as clean as production code . | . Clean tests . Readability is perhaps even more important in unit tests than it is in production code . Clarity | Simplicity | Density of expression (say a lot with as few expressions as possible) | . BUILD-OPERATE-CHECK pattern: . First part builds up the test data | Second part operates on that test data | Third part checks that the operation yielded the expected results | . Domain-Specific Testing Language: testing language (specialized API used by the tests) -&gt; make tests expressive and succint -&gt; make the tests more convenient to write and easier to read . given-when-then convention: makes the tests even easier to read . TEMPLATE METHOD pattern -&gt; putting the given/when parts in the base classs, and the then parts in different derivatives . The number of asserts in a test ought to be minimized | We want to test a single concept in each test function | . F.I.R.S.T. . Fast: when tests run slow, you won’t want to run them frequently | Independent: you should be able to run each test independently and run the tests in any order you like | Repeatable: if your tests aren’t repeatable in any environment, then you’ll always have an excuse for why they fail | Self-Validating: you should not have to read through a log file to tell whether the tests pass (should have a boolean output -&gt; pass/fail) | Timely: unit tests should be written just before the production code that makes them pass | . Ch10. Classes . Smaller is the primary rule when it comes to designing classes | Name of the class = describe what responsibilities it fulfills | If we cannot derive a concise name for a class, then it’s likely too large -&gt; the more ambiguous the class name, the more likely it has too many responsibilities | . The Single Responsibility Principle . SRP is one of the more important concepts in OO design | States that a class or module should have one and only one, reason to change | Definition of responsibility | Guidelines for class size | A system with many small classes has no more moving parts than a system with a few large classes | . Trying to identify responsibilities (reasons to change) often helps us recognize and create better abstractions in our code . Cohesion . Classes should have a small number of instance variables | Each of the methods of a class should manipulate one or more of those variables | A class in which each variable is used by each method is maximally cohesive | Maintaining cohesion results in many small classes | . Organizing for change . Change is continual | Every change -&gt; risk that the remainder of the system no longer works as intended | Clean system -&gt; organize our classes to reduce the risk of change | . Open-Closed Principle (OCP): another key OO class design principle -&gt; Classes should be open for extension but closed for modification . Ideal system -&gt; we incorporate new features by extending the system, not by making modifications to existing code | . Dependency Inversion Principle (DIP) -&gt; classes should depend upon abstractions, not on concrete details . Ch11. Systems . Separate constructing a system from using it . Software systems should separate the startup process, when the application objects are constructed and the dependencies are “wired” together, from the runtime logic that takes over after startup . Startup process: concern that any application must address | Separation of concerns: one of the most important design techniques | Never let little, convenient idioms lead to modularity breakdown | . Separation of main . Factories . ABSTRACT FACTORY: pattern -&gt; give the application control of when to build the object, but keep the details of that construction separate from the application code | . Dependency injection (DI) . Powerful mechanism for separating construction from use | Application of Inversion of Control (IoC) to dependency management | Moves secondary responsibilities from an object to other objects that are dedicated to the purpose (supporting SRP) | The invoking object doesn’t control what kind of object is actually returned, but the invoking object still actively resolves the dependency An object should not take responsibility for instantiating dependencies itself. Instead, it should pass this responsibility to another “authoritative” mechanism (inverting control). Setup is a global concern, this authoritative mechanism will be either the “main” routine or a special-purpose container . | . Scaling up . Myth: we can get systems “right the first time” | Implement only today’s stories -&gt; then refactor and expand the system to implement new stories tomorrow = essence of iterative and incremental agility | TDD, refactoring, and the clean code they produce make this work at the code level | Software systems are unique compared to physical systems. Their archiectures can grow incrementally, if we maintain the proper separation of concerns | . Test drive the system architecture . Big Design Up Front (BDUF): harmful because it inhibits adapting to change, due to psychological resistance to discarding prior effort and because of the way architecture choices influence subsequent thinking about the design | . Optimize decision making . Modularity and separation of concerns make decentralized management and decision making possible | Give responsibilities to the most qualified persons | It is best to postpone decisions until the last possible moment -&gt; lets us make informed choices with the best possible information. A premature decision is a decision made with suboptimal knowledge | . Whether you are designing systems or individual modules, never forget to use the simplest thing that can possibly work . Ch12. Emergence . A design is “simple”, if it follows these rules: . Run all the tests | Contains no duplication | Expresses the intent of the programmer | Minimizes the number of classes and methods | . Simple design rule 1: runs all the tests . Systems that aren’t testable aren’t verifiable | A system that cannot be verified should never be deployed | Tight coupling makes it difficult to write tests | The more tests we write, the more we use principles like DIP and tools like dependency injection, interfaces, and abstraction to minimize coupling -&gt; our designs improve even more | Primary OO goals -&gt; low coupling and high cohesion | . Simple design rule 2-4: refactoring . For each few lines of code we add, we pause and reflect on the new design . No duplication . Duplication is the primary enemy of a well-designed system | It represents additional work, additional risk, and additional unnecessary complexity | TEMPLATE METHOD pattern: common technique for removing higher-level duplication | . Expressive . It’s easy to write code that we understand, because at the time we write it we’re deep in an understanding of the problem we’re trying to solve. Other maintainers of the code aren’t going to have so deep an understanding . Choose good names | Keep your functions and classes small | Use standard nomenclature | Tests primary goal = act as documentation by example | The most important way to be expressive is to try. Care is a precious resource | . Minimal classes and methods . Effort to make our classes and methods small -&gt; we might create too many tiny classes and methods -&gt; also keep our function and class counts low! | . Although it’s important to keep class and function count low, it’s more important to have tests, eliminate duplication, and express yourself . Ch13. Concurrency . Objects are abstractions of processing. Threads are abstractions of schedule - James O. Coplien . Why concurrency? . Concurrency is a decoupling strategy | Helps us decouple what gets done from when it gets done | . Myths and misconceptions . Concurrency can sometimes improve performance, but only when there is a lot of wait time that can be shared between multiple threads or multiple processors | The design of a concurrent algorithm can be remarkably different from the design of a single-threaded system | Concurrency bugs aren’t usually repeatable, so they are often ignored as one-offs instead of the true defects they are | Concurrency often requires a fundamental change in design strategy | . Concurrency defense principles . Single responsibility principle: keep your concurrency-related code separate from other code | Limit the scope of data: data encapsulation; severely limit the access of any data that may be shared | Use copies of data | Threads should be as independent as possible | . Know your execution models . Producer-Consumer | Readers-Writers | Dining Philosophers | . Others . Keep synchronized sections small | Think about shut-down early and get it working early | Write tests that have the potential to expose problems and then run them frequently, with different programatic configurations and system configurations and load | Do not ignore system failures as one-offs | Do not try to chase down nonthreading bugs and threading bugs at the same time. Make sure your code works outside of threads | Make your thread-based code especially pluggable so that you can run it in various configurations | Run your threaded code on all target platforms early and often | . Code that is simple to follow can become nightmarish when multiple threads and shared data get into the mix -&gt; you need to write clean code with rigor or else face subtle and infrequent failures .",
            "url": "https://millengustavo.github.io/blog/book/software%20engineering/2020/07/26/clean-code.html",
            "relUrl": "/book/software%20engineering/2020/07/26/clean-code.html",
            "date": " • Jul 26, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Mastering Large Datasets with Python: Parallelize and Distribute Your Python Code",
            "content": "My notes and highlights on the book. . Authors: John T. Wolohan . Available here . Ch1. Introduction . Map and reduce style of programming: . easily write parallel programs | organize the code around two functions: map and reduce | . MapReduce = framework for parallel and distributed computing; map and reduce = style of programming that allows running the work in parallel with minimal rewriting and extend the work to distributed workflows . Dask -&gt; another tool for managing large data without map and reduce . Procedural programming . Program Workflow . Starts to run | issues an instruction | instruction is executed | repeat 2 and 3 | finishes running | Parallel programming . Program workflow . Starts to run | divides up the work into chunks of instructions and data | each chunk of work is executed independently | chunks of work are reassembled | finishes running | . The map and reduce style is applicable everywhere, but its specific strengths are in areas where you may need to scale . The map function for transforming data . map: function to transform sequences of data from one type to another | Always retains the same number of objects in the output as were provided in the input | performs one-to-one transformations -&gt; is a great way to transform data so it is more suitable for use | . Declarative programming: focuses on explaining the logic of the code and not on specifying low-level details -&gt; scaling is natural, the logic stays the same . The reduce function for advanced transformations . reduce: transform a sequence of data into a data structure of any shape or size | MapReduce programming pattern relies on the map function to transform some data into another type of data and then uses the reduce function to combine that data | performs one-to-any transformations -&gt; is a great way to assemble data into a final result | . Distributed computing for speed and scale . Extension of parallel computing in which the computer resource we are dedicating to work on each chunk of a given task is its own machine . Hadoop: A distributed framework for map and reduce . Designed as an open source implementation of Google’s original MapReduce framework | Evolved into distributed computing software used widely by companies processing large amounts of data | . Spark for high-powered map, reduce, and more . Something of a sucessor to the Apache Hadoop framework that does more of its work in memory instead of by writing to file | Can run more than 100x faster than Hadoop | . AWS Elastic MapReduce (EMR) - Large datasets in the cloud . Popular way to implement Hadoop and Spark | tackle small problems with parallel programming as its cost effective | tackle large problems with parallel programming because we can procure as many resources as we need | . Ch2. Accelerating large dataset work: Map and parallel computing . map’s primary capabilities: . Replace for loops | Transform data | map evaluates only when necessary, not when called -&gt; generic map object as output | . map makes easy to parallel code -&gt; break into pieces . Pattern . Take a sequence of data | Transform it with a function | Get the outputs | . Generators instead of normal loops prevents storing all objects in memory in advance . Lazy functions for large datasets . map = lazy function = it doesn’t evaluate when we call map | Python stores the instructions for evaluating the function and runs them at the exact moment we ask for the value | Common lazy objects in Python = range function | Lazy map allows us to transform a lot of data without an unnecessarily large amount of memory or spending the time to generate it | . Parallel processing . Problems . Inability to pickle data or functions . Pickling: Python’s version of object serialization or mashalling | Storing objects from our code in an efficient binary format on the disk that can be read back by our program at a later time (pickle module) | allows us to share data across procesors or even machines, saving the instructions and data and then executing them elsewhere | Objects we can’t pickle: lambda functions, nested functions, nested classes | pathos and dill module allows us to pickle almost anything | . Order-sensitive operations . Work in parallel: not guaranteed that tasks will be finished in the same order they’re input | If work needs to be processed in a linear order -&gt; probably shouldn’t do it in parallel | Even though Python may not complete the problems in order, it still remembers the order in which it was supposed to do them -&gt; map returns in the exact order we would expect, even if it doesn’t process in that order | . State-dependent operations . Common solution for the state problem: take the internal state and make it an external variable | . Other observations . Best way to flatten a list into one big list -&gt; Python’s itertools chain function: takes an iterable of iterables and chains them together so they can all be accessed one after another -&gt; lazy by default | Best way to visualize graphs is to take it out of Python and import it into Gephi: dedicated piece of graph visualization software | . Anytime we’re converting a sequence of some type into a sequence of another type, what we’re doing can be expressed as a map -&gt; N-to-N transformation: we’re converting N data elements, into N data elements but in different format . To make this type of problem parallel only adds up to few lines of code: one import | wrangling our processors with Pool() | modifying our map statements to use Pool.map method | . | . Ch3. Function pipelines for mapping complex transformations . Helper functions and function chains . Helper functions: small, simple functions that we rely on to do complex things -&gt; break down large problems into small pieces that we can code quickly . Function chains or pipelines: the way we put helper functions to work . Creating a pipeline . Chaining helper functions together | Ways to do this: Using a sequence of maps | Chaining functions together with compose | Creating a function pipeline with pipe | . | compose and pipe are functions in the toolz package | . Compose . from toolz.functoolz import compose . Pass compose all the functions we want to include in our pipeline | Pass in reverse order because compose is going to apply them backwards | Store the output of our compose function, which is itself a function, to a variable | Call that variable or pass it along to map | . Pipe . from toolz.functoolz import pipe . pipe function will pass a value through a pipeline | pipe expects the functions to be in the order we want to apply them | pipe evaluates each of the functions and returns a results | If we want to pass it to map, we have to wrap it in a function definition | . Summary . Major advantages of creating pipelines of helper functions are that the code becomes: Readable and clear; Modular and easy to edit . Modular code play very nice with map and can readily move into parallel workflows, such as by using the Pool() | We can simplify working with nested data structures by using nested function pipelines, which we can apply with map | . Ch4. Processing large datasets with lazy workflows . Laziness . Lazy evaluation: strategy when deciding when to perform computations | Under lazy evaluation, the Python interpreter executes lazy Python code only when the program needs the results of that code | Opposite of eager evaluation, where everything is evaluated when it’s called | . Shrinking sequences with the filter function . filter: function for pruning sequences. | Takes a sequence and restricts it to only the elements that meet a given condition | Related functions to know itertools.filterfalse: get all the results that make a qualifier function return False | toolz.dicttoolz.keyfilter: filter on the keys of a dict | toolz.dicttoolz.valfilter: filter on the values of a dict | toolz.dicttoolz.itemfilter: filter on both the keys and the values of a dict | . | . Combining sequences with zip . zip: function for merging sequences. | Takes two sequences and returns a single sequence of tuples, each of which contains an element from each of the original sequences | Behaves like a zipper, it interlocks the values of Python iterables | . Lazy file searching with iglob . iglob: function for lazily reading from the filesystem. | Lazy way of querying our filesystem | Find a sequence of files on our filesystem that match a given pattern | . from glob import iglob posts = iglob(&quot;path/to/posts/2020/06/*.md&quot;) . Understanding iterators: the magic behind lazy Python . Replace data with instructions about where to find data and replace transformations with instructions for how to execute those transformations. | The computer only has to concern itself with the data it is processing right now, as opposed to the data it just processed or has to process in the future | Iterators are the base class of all the Python data types that can be iterated over | . The iteration process is defined by a special method called .__iter__(). If a class has this method and returns an object with a .__next__() method, then we can iterate over it. . One-way streets: once we call next, the item returned is removed from the sequence. We can never back up or retrieve that item again | Not meant for by-hand inspection -&gt; meant for processing big data | . Generators: functions for creating data . Class of functions in Python that lazily produce values in a sequence | We can create generators with functions using yield statements or through concise and powerful list comprehension-like generator expressions | They’re a simple way of implementing an iterator | Primary advantage of generators and lazy functions: avoiding storing more in memory than we need to | itertools.islice: take chunks from a sequence | . Lazy functions are great at processing data, but hardware still limits how quickly we can work through it . toolz.frequencies: takes a sequence in and returns a dict of items that occurred in the sequence as keys with corresponding values equal to the number of times they occurred -&gt; provides the frequencies of items in our sequence | . Simulations . For simulations -&gt; writing classes allow us to consolidate the data about each piece of the simulation | itertools.count(): returns a generator that produces an infinite sequence of increasing numbers | Unzipping = the opposite of zipping -&gt; takes a single sequence and returns two -&gt; unzip = zip(*my_sequence) | . operator.methodcaller: takes a string and returns a function that calls that method with the name of that string on any object passed to it -&gt; call class methods using functions is helpful = allows us to use functions like map and filter on them . Ch5. Accumulation operations with reduce . reduce: function for N-to-X transformations | We have a sequence and want to transform it into something that we can’t use map for | map can take care of the transformations in a very concise manner, whereas reduce can take care of the very final transformation | . Three parts of reduce . Accumulator function | Sequence: object that we can iterate through, such as lists, strings, and generators | Initializer: initial value to be passed to our accumulator (may be optional) -&gt; use an initalizer not when we want to change the value of our data, but when we want to change the type of the data | . from functools import reduce reduce(acc_fn, sequence, initializer) . Accumulator functions . Does the heavy lifting for reduce | Special type of helper function | Common prototype: take an accumulated value and the next element in the sequence | return another object, typically of the same type as the accumulated value | accumulator functions always needs to return a value | . | Accumulator functions take two variables: one for the accumulated data (often designated as acc, left, or a), and one for the next element in the sequence (designated nxt, right, or b). | . def my_add(acc, nxt): return acc + nxt # or, using lambda functions lambda acc, nxt: acc + nxt . Reductions . filter | frequencies | . Using map and reduce together . If you can decompose a problem into an N-to-X transformation, all that stands between you and a reduction that solves that problem is a well-crafted accumulation function . Using map and reduce pattern to decouple the transformation logic from the actual transformation itself: leads to highly reusable code | with large datasets -&gt; simple functions becomes paramount -&gt; we may have to wait a long time to discover we made a small error | . | . Speeding up map and reduce . Using a parallel map can counterintuitively be slower than using a lazy map in map an reduce scenarios . We can always use parallelization at the reduce level instead of at the map level | . Ch6. Speeding up map and reduce with advanced parallelization . Parallel reduce: use parallelization in the accumulation process instead of the transformation process | . Getting the most out of parallel map . Parallel map will be slower than lazy map when: . we’re going to iterate through the sequence a second time later in our workflow | size of the work done in each parallel instance is small compared to the overhead that parallelization imposes -&gt; chunksize: size of the different pieces into which we break our tasks for parallel processing | Python makes chunksize available as an option -&gt; vary according to the task at hand | . More parallel maps: .imap and starmap . .imap . .imap: for lazy parallel mapping | use .imap method to work in parallel on very large sequences efficiently | Lazy and parallel? use the .imap and .imap_unordered methods of Pool() -&gt; both methods return iterators instead of lists | .imap_unordered: behaves the same, except it doesn’t necessarily put the sequence in the right order for our iterator | . starmap . use starmap to work with complex iterables, especially those we’re likely to create using the zip function -&gt; more than one single parameter (map’s limitation) | starmap unpacks tuples as positional parameters to the function with which we’re mapping | itertools.starmap: lazy function | Pool().starmap: parallel function | . Parallel reduce for faster reductions . Parallel reduce: . break a problem into chunks | make no guarantees about order | need to pickle data | be finicky about stateful objects | run slower than its linear counterpart on small datasets | run faster than its linear counterpart on big datasets | require an accumulator function, some data, and an initial value | perform N-to-X transformations | . Parallel reduce has six parameters: an accumulation function, a sequence, an initializer value, a map, a chunksize, and a combination function - three more than the standard reduce function . Parallel reduce workflow: . break our problem into pieces | do some work | combine the work | return a result | . With parallel reduce we trade the simplicity of always having the same combination function for the flexibility of more possible transformations . Implementing parallel reduce: . Importing the proper classes and functions | Rounding up some processors | Passing our reduce function the right helper functions and variables | Python doesn’t natively support parallel reduce -&gt; pathos library | toolz.fold -&gt; parallel reduce implementation | . toolz library: functional utility library that Python never came with. High-performance version of the library = CyToolz . Ch7. Processing truly big datasets with Hadoop and Spark . Hadoop: set of tools that support distributed map and reduce style of programming through Hadoop MapReduce | Spark: analytics toolkit designed to modernize Hadoop | . Distributed computing . share tasks and data long-term across a network of computers | offers large benefits in speed when we can parallelize our work | challenges: keeping track of all our data | coordinating our work | . | . If we distribute our work prematurely, we’ll end up losing performance spending too much time talking between computers and processors. A lot of performance improvements at the high-performance limits of distributed computing revolve around optimizing communication between machines . Hadoop five modules . MapReduce: way of dividing work into parallelizable chunks | YARN: scheduler and resource manager | HDFS: file system for Hadoop | Ozone: Hadoop extension for object storage and semantic computing | Common: set of utilities that are shared across the previous four modules | YARN for job scheduling . Scheduling Oversees all of the work that is being done | Acts as a final decision maker in terms of how resources should be allocated across the cluster | . | Application management (node managers): work at the node (single-machine) level to determine how resources should be allocated within that machine federation: tie together resource managers in extremely high demand use cases where thousands of nodes are not sufficient | . | . The data storage backbone of Hadoop: HDFS . Hadoop Distributed File System (HDFS) -&gt; reliable, performant foundation for high-performance distributed computing (but with that comes complexity). Use cases: . process big datasets | be flexible in hardware choice | be protected against hardware failure | . Moving code is faster than moving data . MapReduce jobs using Python and Hadoop Streaming . Hadoop MapReduce with Python -&gt; Hadoop Streaming = utility for using Hadoop MapReduce with programming languages besides Java . Hadoop natively supports compression data: .gz, .bz2, and .snappy . Spark for interactive workflows . Analytics-oriented data processing framework designed to take advantage of higher-RAM compute clusters. Advantages for Python programmers: . direct Python interface - PySpark: allows for us to interactively explore big data through a PySpark shell REPL | can query SQL databases directly (Java Database Connectivity - JDBC) | has a DataFrame API: rows-and-columns data structure familiar to pandas -&gt; provides a convenience layer on top of the core Spark data object: the RDD (Resilient Distributed Dataset) | Spark has two high-performance data structures: RDDs, which are excellent for any type of data, and DataFrames, which are optimized for tabular data. | . Favor Spark over Hadoop when: . processing streaming data | need to get the task completed nearly instantaneously | willing to pay for high-RAM compute clusters | . PySpark for mixing Python and Spark . PySpark: we can call Spark’s Scala methods through Python just like we would a normal Python library . Ch8. Best practices for large data with Apache Streaming and mrjob . Use Hadoop to process . lots of data fast: distributed parallelization | data that’s important: low data loss | enormous amounts of data: petabyte scale | . Drawbacks . To use Hadoop with Python -&gt; Hadoop Streaming utility | Repeatedly read in string from stdin | Error messages for Java are not helpful | . Unstructured data: Logs and documents . Hadoop creators designed Hadoop to work on unstructured data -&gt; data in the form of documents | Unstructured data is notoriously unwieldly =/= tabular data | But, is one of the most common forms of data around | . JSON for passing data between mapper and reducer . JavaScript Object Notation (JSON) | Data format used for moving data in plain text between one place and another | json.dumps() and json.loads() functions from Python’s json library to achieve the transfer | Advantages: easy for humans and machines to read | provides a number of useful basic data types (string, numeric, array) | emphasis on key-value pairs that aids the loose coupling of systems | . | . mrjob for pythonic Hadoop streaming . mrjob: Python library for Hadoop Streaming that focuses on cloud compatibility for truly scalable analysis | keeps the mapper and reducer steps but wraps them up in a single worker class named mrjob | mrjob versions of map and reduce share the same type signature, taking in keys and values and outputting keys and values | mrjob enforces JSON data exchange between the mapper and reducer phases, so we need to ensure that our output data is JSON serializable. | . Ch9. PageRank with map and reduce in PySpark . PySpark’s RDD class methods: . map-like methods: replicate the function of map | reduce-like methods: replicate the function of reduce | Convenience methods: solve common problems | . Partitions are the abstraction that RDDs use to implement parallelization. The data in an RDD is split up across different partitions, and each partition is handled in memory. It is common in large data tasks to partition an RDD by a key . Map-like methods in PySpark . .map | .flatMap | .mapValues | .flatMapValues | . mapPartitions | .mapPartitionsWithIndex | . Reduce-like methods in PySpark . .reduce | .fold | .aggregate -&gt; provides all the functionality of a parallel reduce. We can provide an initializer value, an aggregation function, and a combination function | . Convenience methods in PySpark . Many of these mirror functions in functools, itertools and toolz. Some examples: . .countByKey() | .countByValue() | .distinct() | .countApproxDistinct() | .filter() | .first() | .groupBy() | .groupByKey() | .saveAsTextFile() | .take() | . Saving RDDs to text files . Excellent for a few reasons: . The data is in a human-readable, persistent format. | We can easily read this data back into Spark with the .textFile method of SparkContext. | The data is well structured for other parallel tools, such as Hadoop’s MapReduce. | We can specify a compression format for efficient data storage or transfer. | . RDD .aggregate method—returns a dict. We need an RDD so that we can take advantage of Spark’s parallelization. To get an RDD, we’ll need to explicitly convert the items of that dict into an RDD using the .parallelize method from our SparkContext: sc. . Spark programs often use characters in their method chaining to increase their readability | Using the byKey variations of methods in PySpark often results in significant speed-ups because like data is worked on by the same distributed compute worker | . Ch10. Faster decision-making with machine learning and PySpark . One of the reasons why Spark is so popular = built-in machine learning capabilities . PySpark’s machine learning capabilities live in a package called ml. This package itself contains a few different modules categorizing some of the core machine learning capabilities, including . pyspark.ml.feature — For feature transformation and creation | pyspark.ml.classification — Algorithms for judging the category in which a data point belongs | pyspark.ml.tuning — Algorithms for improving our machine learners | pyspark.ml.evaluation — Algorithms for evaluating machine leaners | pyspark.ml.util — Methods of saving and loading machine learners | . PySpark’s machine learning features expect us to have our data in a PySpark DataFrame object - not an RDD. The RDD is an abstract parallelizable data structure at the core of Spark, whereas the DataFrame is a layer on top of the RDD that provides a notion of rows and columns . Organizing the data for learning . Spark’s ml classifiers look for two columns in a DataFrame: . A label column: indicates the correct classification of the data | A features column: contains the features we’re going to use to predict that label | . Auxiliary classes . PySpark’s StringIndexer: transforms categorical data stored as category names (using strings) and indexes the names as numerical variables. StringIndexer indexes categories in order of frequency — from most common to least common. The most common category will be 0, the second most common category 1, and so on | Most data structures in Spark are immutable -&gt; property of Scala (in which Spark is written) | Spark’s ml only want one column name features -&gt; PySpark’s VectorAssembler: Transformer like StringIndexer -&gt; takes some input column names and an output column name and has methods to return a new DataFrame that has all the columns of the original, plus the new column we want to add | The feature creation classes are Transformer-class objects, and their methods return new DataFrames, rather than transforming them in place | . Evaluation . PySpark’s ml.evaluation module: . BinaryClassifierEvaluator | RegressionEvaluator | MulticlassClassificationEvaluator | . Cross-validation in PySpark . CrossValidator class: k-fold cross-validation, needs to be initialized with: . An estimator | A parameter estimator - ParamGridBuilder object | An evaluator | . Ch11. Large datasets in the cloud with Amazon Web Services and S3 . S3 is the go-to service for large datasets: . effectively unlimited storage capacity. We never have to worry about our dataset becoming too large | cloud-based. We can scale up and down quickly as necessary. | offers object storage. We can focus on organizing our data with metadata and store many different types of data. | managed service. Amazon Web Services takes care of a lot of the details for us, such as ensuring data availability and durability. They also take care of security patches and software updates. | supports versioning and life cycle policies. We can use them to update or archive our data as it ages | Objects for convenient heterogenous storage . Object storage: storage pattern that focuses on the what of the data instead of the where | With object storage we recognize objects by a unique identifier (instead of the name and directory) | Supports arbitrary metadata -&gt; we can tag our objects flexibly based on our needs (helps us find those objects later when we need to use them) | Querying tools are available for S3 that allow SQL-like querying on these metadata tags for metadata analysis | Unique identifiers -&gt; we can store heterogenous data in the same way | . Parquet: A concise tabular data store . CSV is a simple, tabular data store, and JSON is a human-readable document store. Both are common in data interchange and are often used in the storage of distributed large datasets. Parquet is a Hadoop- native tabular data format. | Parquet uses clever metadata to improve the performance of map and reduce operations. Running a job on Parquet can take as little as 1/100th the time a comparable job on a CSV or JSON file would take. Additionally, Parquet supports efficient compression. As a result, it can be stored at a fraction of the cost of CSV or JSON. | These benefits make Parquet an excellent option for data that primarily needs to be read by a machine, such as for batch analytics operations. JSON and CSV remain good options for smaller data or data that’s likely to need some human inspection. | . Boto is a library that provides Pythonic access to many of the AWS APIs. We need the access key and secret key to programmatically access AWS through boto . Ch12. MapReduce in the cloud with Amazon’s Elastic MapReduce . Convenient cloud clusters with EMR . Ways to get access to a compute cluster that support both Hadoop and Spark: . AWS: Amazon’s Elastic MapReduce | Microsoft’s Azure HDInsight | Google’s Cloud Dataproc | . AWS EMR . AWS EMR is a managed data cluster service | We specify general properties of the cluster, and AWS runs software that creates the cluster for us | When we’re done using the cluster, AWS absorbs the compute resources back into its network | Pricing model is a per-compute-unit per-second charge | There are no cost savings to doing things slowly. AWS encourages us to parallelize our problems away | . Starting EMR clusters with mrjob . We can run Hadoop jobs on EMR with the mrjob library, which allows us to write distributed MapReduce and procure cluster computing in Python. | We can use mrjob’s configuration files to describe what we want our clusters to look like, including which instances we’d like to use, where we’d like those instances to be located, and any tags we may want to add. | . Hadoop on EMR is excellent for large data processing workloads, such as batch analytics or extract-transform-load (ETL) . Machine learning in the cloud with Spark on EMR . Hadoop is great for low-memory workloads and massive data. | Spark is great for jobs that are harder to break down into map and reduce steps, and situations where we can afford higher memory machines | . Running machine learning algorithms on a truly large dataset . Get a sample of the full dataset. | Train and evaluate a few models on that dataset. | Select some models to evaluate on the full dataset. | Train several models on the full dataset in the cloud. | Run your Spark code with spark-submit utility instead of Python. The spark-submit utility queues up a Spark job, which will run in parallel locally and simulate what would happen if you ran the program on an active cluster . EC2 instance types and clusters . M-series: use for Hadoop and for testing Spark jobs | C-series: compute-heavy workloads such as Spark analytics, Batch Spark jobs | R-series: high-memory, use for streaming analytics | . Software available on EMR . JupyterHub: cluster-ready version of Jupyter Notebook -&gt; run interactive Spark and Hadoop jobs from a notebook environment | Hive: compile SQL code to Hadoop MapReduce jobs | Pig: compile Pig-latin (SQL-like) commands to run Hadoop MapReduce jobs | .",
            "url": "https://millengustavo.github.io/blog/book/software%20engineering/python/big%20data/2020/06/23/master-large-data-python.html",
            "relUrl": "/book/software%20engineering/python/big%20data/2020/06/23/master-large-data-python.html",
            "date": " • Jun 23, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "High performance Python: Practical Performant Programming for Humans",
            "content": "My notes and highlights on the book. . Authors: Micha Gorelick, Ian Ozsvald . “Every programmer can benefit from understanding how to build performant systems (…) When something becomes ten times cheaper in time or compute costs, suddenly the set of applications you can address is wider than you imagined” . Supplemental material for the book (code examples, exercises, etc.) is available for download at https://github.com/mynameisfiber/high_performance_python_2e. . Ch1. Understanding Performant Python . Why use Python? . highly expressive and easy to learn | scikit-learn wraps LIBLINEAR and LIBSVM (written in C) | numpy includes BLAS and other C and Fortran libraries | python code that properly utilizes these modules can be as fast as comparable C code | “batteries included” | enable fast prototyping of an idea | . How to be a highly performant programmer . Overall team velocity is far more important than speedups and complicated solutions. Several factors are key to this: . Good structure | Documentation | Debuggability | Shared standards | . Ch2. Profiling to Find Bottlenecks . Profiling let you make the most pragmatic decisions for the least overall effort: Code run “fast enough” and “lean enough” . “If you avoid profiling and jump to optmization, you’ll quite likely do more work in the long run. Always be driven by the results of profiling” . “Embarrassingly parallel problem”: no data is shared between points . timeit module temporarily disables the garbage collector . cProfile module . Built-in profiling tool in the standard library . profile: original and slower pure Python profiler | cProfile: same interface as profile and is written in C for a lower overhead | . Generate a hypothesis about the speed of parts of your code | Measure how wrong you are | Improve your intuition about certain coding styles | Visualizing cProfile output with Snakeviz . snakeviz: visualizer that draws the output of cProfile as a diagram -&gt; larger boxes are areas of code that take longer to run . Using line_profiler for line-by-line measurements . line_profilier: strongest tool for identifying the cause of CPU-bound problems in Python code: profile individual functions on a line-by-line basis . Be aware of the complexity of Python’s dynamic machinery . The order of evaluation for Python statements is both left to right and opportunistic: put the cheapest test on the left side of the equation . Using memory_profiler to diagnose memory usage . memory_profiler measures memory usage on a line-by-line basis: . Could we use less RAM by rewriting this function to work more efficiently? | Could we use more RAM and save CPU cycles by caching? | . Tips . Memory profiling make your code run 10-100x slower | Install psutil to memory_profiler run faster | Use memory_profiler occasionally and line_profiler more frequently | --pdb-mmem=XXX flag: pdb debugger is activate after the process exceeds XXX MB -&gt; drop you in directly at the point in your code where too many allocations are occurring | . Introspecting an existing process with PySpy . py-spy: sampling profiler, don’t require any code changes -&gt; it introspects an already-running Python process and reports in the console with a top-like display . Ch3. Lists and Tuples . Lists: dynamic arrays; mutable and allow for resizing | Tuples: static arrays; immutable and the data within them cannot be changed aftey they have been created | Tuples are cached by the Python runtime which means that we don’t need to talk to the kernel to reserve memory every time we want to use one | . Python lists have a built-in sorting algorithm that uses Tim sort -&gt; O(n) in the best case and O(nlogn) in the worst case . Once sorted, we can find our desired element using a binary search -&gt; average case of complexity of O(logn) . Dictionary lookup takes only O(1), but: . converting the data to a dictionary takes O(n) | no repeating keys may be undesirable | . bisect module: provide alternative functions, heavily optimized . “Pick the right data structure and stick with it! Although there may be more efficient data structures for particular operations, the cost of converting to those data structures may negate any efficiency boost” . Tuples are for describing multiple properties of one unchanging thing | List can be used to store collections of data about completely disparate objects | Both can take mixed types | . “Generic code will be much slower than code specifically designed to solve a particular problem” . Tuple (immutable): lightweight data structure | List (mutable): extra memory needed to store them and extra computations needed when using them | . Ch4. Dictionaries and Sets . Ideal data structures to use when your data has no intrinsic order (except for insertion order), but does have a unique object that can be used to reference it . key: reference object | value: data | . Sets do not actually contain values: is a collection of unique keys -&gt; useful for doing set operations . hashable type: implements __hash__ and either __eq__ or __cmp__ . Complexity and speed . O(1) lookups based on the arbitrary index | O(1) insertion time | Larger footprint in memory | Actual speed depends on the hashing function | . How do dictionaries and sets work? . Use hash tables to achieve O(1) lookups and insertions -&gt; clever usage of a hash function to turn an arbitrary key (i.e., a string or object) into an index for a list . load factor: how well distributed the data is throughout the hash table -&gt; related to the entropy of the hash function . Hash functions must return integers . Numerical types (int and float): hash is based on the bit value of the number they represent | Tuples and strings: hash value based on their contents | Lists: do not support hashing because their values can change | . A custom-selected hash function should be careful to evenly distribute hash values in order to avoid collisions (will degrade the performance of a hash table) -&gt; constantly “probe” the other values -&gt; worst case O(n) = searching through a list . Entropy: “how well distributed my hash function is” -&gt; max entropy = ideal hash function = minimal number of collisions . Ch5. Iterators and Generators . Python for loop deconstructed . # The Python loop for i in object: do_work(i) # Is equivalent to object_iterator = iter(object) while True: try: i = next(object_iterator) except StopIteration: break else: do_work(i) . Changing to generators instead of precomputed arrays may require algorithmic changes (sometimes not so easy to understand) | . “Many of Python’s built-in functions that operate on sequences are generators themselves. range returns a generator of values as opposed to the actual list of numbers within the specified range. Similarly, map, zip, filter, reversed, and enumerate all perform the calculation as needed and don’t store the full result” . Generators have less memory impact than list comprehension | Generators are really a way of organizing your code and having smarter loops | . Lazy generator evaluation . Single pass or online algorithms: at any point in our calculation with a generator, we have only the current value and cannot reference any other items in the sequence . itertools from the standard library provides useful functions to make generators easier to use: . islice: slicing a potentially infinite generator | chain: chain together multiple generators | takewhile: adds a condition that will end a generator | cycle: makes a finite generator infinite by constantly repeating it | . Ch6. Matrix and Vector Computation . Understanding the motivation behind your code and the intricacies of the algorithm will give you deeper insight about possible methods of optimization . Memory fragmentation . Python doesn’t natively support vectorization . Python lists store pointers to the actual data -&gt; good because it allows us to store whatever type of data inside a list, however when it comes to vector and matrix operations, this is a source of performance degradation | Python bytecode is not optimized for vectorization -&gt; for loops cannot predict when using vectorization would be benefical | . von Neumann bottleneck: limited bandwidth between memory and CPU as a result of the tiered memory architecture that modern computers use . perf Linux tool: insights into how the CPU is dealing with the program being run . array object is less suitable for math and more suitable for storing fixed-type data more efficiently in memory . numpy . numpy has all of the features we need—it stores data in contiguous chunks of memory and supports vectorized operations on its data. As a result, any arithmetic we do on numpy arrays happens in chunks without us having to explicitly loop over each element. Not only is it much easier to do matrix arithmetic this way, but it is also faster . Vectorization from numpy: may run fewer instructions per cycle, but each instruction does much more work . numexpr: making in-place operations faster and easier . numpy’s optimization of vector operations: occurs on only one operation at a time | numexpr is a module that can take an entire vector expression and compile it into very efficient code that is optimized to minimize cache misses and temporary space used. Expressions can utilize multiple CPU cores | Easy to change code to use numexpr: rewrite the expressions as strings with references to local variables | . Lessons from matrix optimizations . Always take care of any administrative things the code must do during initialization . allocating memory | reading a configuration from a file | precomputing values that will be needed throughout the lifetime of a program | . Pandas . Pandas’s internal model . Operations on columns often generate temporary intermediate arrays which consume RAM: expect a temporary memory usage of up to 3-5x your current usage | Operations can be single-threaded and limited by Python’s global interpreter lock (GIL) | Columns of the same dtype are grouped together by a BlockManager -&gt; make row-wise operations on columns of the same datatype faster | Operations on data of a single common block -&gt; view; different dtypes -&gt; can cause a copy (slower) | Pandas uses a mix of NumPy datatypes and its own extension datatypes | numpy int64 isn’t NaN aware -&gt; Pandas Int64 uses two columns of data: integers and NaN bit mask | numpy bool isn’t NaN aware -&gt; Pandas boolean | . More safety makes things run slower (checking passing appropriate data) -&gt; Developer time (and sanity) x Execution time. Checks enabled: avoid painful debugging sessions, which kill developer productivity. If we know that our data is of the correct form for our chosen algorithm, these checks will add a penalty . Building DataFrames and Series from partial results rather than concatenating . Avoid repeated calls to concat in Pandas (and to the equivalent concatenate in NumPy) | Build lists of intermediate results and then construct a Series or DataFrame from this list, rather than concatenating to an existing object | . Advice for effective pandas development . Install the optional dependencies numexpr and bottleneck for additional performance improvements | Caution against chaining too many rows of pandas operations in sequence: difficult to debug, chain only a couple of operations together to simplify your maintenance | Filter your data before calculating on the remaining rows rather than filtering after calculating | Check the schema of your DataFrames as they evolve -&gt; tool like bulwark, you can visualize confirm that your expectations are being met | Large Series with low cardinality: df[&#39;series_of_strings&#39;].astype(&#39;category&#39;) -&gt; value_counts and groupby run faster and the Series consume less RAM | Convert 8-byte float64 and int64 to smaller datatypes -&gt; 2-byte float16 or 1-byte int8 -&gt; smaller range to further save RAM | Use the del keyword to delete earlier references and clear them from memory | Pandas drop method to delete unused columns | Persist the prepared DataFrame version to disk by using to_pickle | Avoid inplace=True -&gt; are scheduled to be removed from the library over time | Modin, cuDF | Vaex: work on very large datasets that exceed RAM by using lazy evaluation while retaining a similar interface to Pandas -&gt; large datasets and string-heavy operations | . Ch7. Compiling to C . To make code run faster: . Make it do less work | Choose good algorithms | Reduce the amount of data you’re processing | Execute fewer instructions -&gt; compile your code down to machine code | . Python offers . Cython: pure C-based compiling | Numba: LLVM-based compiling | PyPy: replacement virtual machine which includes a built-in just-in-time (JIT) compiler | . What sort of speed gains are possible? . Compiling generate more gains when the code: . is mathematical | has lots of loops that repeat the same operations many times | . Unlikely to show speed up: . calls to external libraries (regexp, string operations, calls to database) | programs that are I/O-bound | . JIT versus AOT compilers . AOT (ahead of time): Cython -&gt; you’ll have a library that can instantly be used -&gt; best speedups, but requires the most manual effort | JIT (just in time): Numba, PyPy -&gt; you don’t have to do much work up front, but you have a “cold start” problem -&gt; impressive speedups with little manual intervention | . Why does type information help the code run faster? . Python is dynamically typed -&gt; keeping the code generic makes it run more slowly . “Inside a section of code that is CPU-bound, it is often the case that the types of variables do not change. This gives us an opportunity for static compilation and faster code execution” . Using a C compiler . Cython uses gcc: good choice for most platforms; well supported and quite advanced . Cython . Compiler that converts type-annotaded (C-like) Python into a compiled extension module | Wide used and mature | OpenMP support: possible to convert parallel problems into multiprocessing-aware modules | pyximport: simplified build system | Annotation option that output an HTML file -&gt; more yellow = more calls into the Python virtual machine; more white = more non-Python C code | . Lines that cost the most CPU time: . inside tight inner loops | dereferencing list, array or np.array items | mathematical operations | . cdef keyword: declare variables inside the function body. These must be declared at the top of the function, as that’s a requirement from the C language specification . Strength reduction: writing equivalent but more specialized code to solve the same problem. Trade worse flexibility (and possibly worse readability) for faster execution . memoryview: allows the same low-level access to any object that implements the buffer interface, including numpy arrays and Python arrays . Numba . JIT compiler that specializes in numpy code, which it compiles via LLVM compiler at runtime | You provide a decorator telling it which functions to focus on and then you let Numba take over | numpy arrays and nonvectorized code that iterates over many items: Numba should give you a quick and very painless win. | Numba does not bind to external C libraries (which Cython can do), but it can automatically generate code for GPUs (which Cython cannot). | OpenMP parallelization support with prange | Break your code into small (&lt;10 line) and discrete functions and tackle these one at a time | . from numba import jit @jit() def my_fn(): . PyPy . Alternative implementation of the Python language that includes a tracing just-in-time compiler | Offers a faster experience than CPython | Uses a different type of garbage collector (modified mark-and-sweep) than CPython (reference counting) = may clean up an unused object much later | PyPy can use a lot of RAM | vmprof: lightweight sampling profiler | . When to use each technology . . Numba: quick wins for little effort; young project | Cython: best results for the widest set of prolbmes; requires more effort; mix Python and C annotations | PyPy: strong option if you’re not using numpy or other hard-to-port C extensions | . Other upcoming projects . Pythran | Transonic | ShedSkin | PyCUDA | PyOpenCL | Nuitka | . Graphics Processing Units (GPUs) . Easy-to-use GPU mathematics libraries: . TensorFlow | PyTorch | . Dynamic graphs: PyTorch . Static computational graph tensor library that is particularly user-friendly and has a very intuitive API for anyone familiar with numpy . Static computational graph: performing operations on PyTorch objects creates a dynamic definition of a program that gets compiled to GPU code in the background when it is executed -&gt; changes to the Python code automatically get reflected in changes in the GPU code without an explicit compilation step needed . Basic GPU profiling . nvidia-smi: inspect the resource utilization of the GPU | Power usage is a good proxy for judging how much of the GPU’s compute power is being used -&gt; more power the GPU is drawing = more compute it is currently doing | . When to use GPUs . Task requires mainly linear algebra and matrix manipulations (multiplication, addition, Fourier transforms) | Particularly true if the calculation can happen on the GPU uninterrupted for a period of time before being copied back into system memory | GPU can run many more tasks at once than the CPU can, but each of those tasks run more slowly on the GPU than on the CPU | Not a good tool for tasks that require exceedingly large amounts of data, many conditional manipulations of the data, or changing data | . Ensure that the memory use of the problem will fit withing the GPU | Evaluate whether the algorithm requires a lot of branching conditions versus vectorized operations | Evaluate how much data needs to be moved between the GPU and the CPU | Ch8. Asynchronous I/O . I/O bound program: the speed is bounded by the efficiency of the input/output . Asynchronous I/O helps utilize the wasted I/O wait time by allowing us to perform other operations while we are in that state . Introduction to asynchronous programming . Context switch: when a program enters I/O wait, the execution is paused so that the kernel can perform the low-level operations associated with the I/O request | Callback paradigm: functions are called with an argument that is generally called the callback -&gt; instead of the function returing its value, it call the callback function with the value instead -&gt; long chains = “callback hell” | Future paradigm: an asynchronous function returns a Future object, which is a promise of a future result | asyncio standard library module and PEP 492 made the future’s mechanism native to Python | . How does async/await work? . async function (defined with async def) is called a coroutine | Coroutines are implemented with the same philosophies as generators | await is similar in function to a yield -&gt; the execution of the current function gets paused while other code is run | . Gevent . Patches the standard library with asynchronous I/O functions, | Has a Greenlets object that can be used for concurrent execution | Ideal solution for mainly CPU-based problems that sometimes involve heavy I/O | . tornado . Frequently used package for asynchronous I/O in Python | Originally developed by Facebook primarily for HTTP clients and servers | Ideal for any application that is mostly I/O-bound and where most of the application should be asynchronous | Performant web server | . aiohttp . Built entirely on the asyncio library | Provides both HTTP client and server functionality | Uses a similar API to tornado | . Batched results . Pipelining: batching results -&gt; can help lower the burden of an I/O task | Good compromise between the speeds of asynchronous I/O and the ease of writing serial programs | . Ch9. The multiprocessing module . Additional process = more communication overhead = decrease available RAM -&gt; rarely get a full n-times speedup | If you run out of RAM and the system reverts to using the disk’s swap space, any parallelization advantage will be massively lost to the slow paging of RAM back and forth to disk | Using hyperthreads: CPython uses a lot of RAM -&gt; hyperthreading is not cache friendly. Hyperthreads = added bonus and not a resource to be optimized against -&gt; adding more CPUs is more economical than tuning your code | Amdahl’s law: if only a small part of your code can be parallelized, it doesn’t matter how many CPUs you throw at it; it still won’t run much faster overall | multiprocessing module: process and thread-based parallel processing, share work over queues, and share data among processes -&gt; focus: single-machine multicore parallelism | multiprocessing: higher level, sharing Python data structures | OpenMP: works with C primitive objects once you’ve compiled to C | . Keep the parallelism as simple as possible so that your development velocity is kept high . Embarrassingly parallel: multiple Python processes all solving the same problem without communicating with one another -&gt; not much penalty will be incurred as we add more and more Python processes | . Typical jobs for the multiprocessing module: . Parallelize a CPU-bound task with Process or Pool objects | Parallelize an I/O-bound task in a Pool with threads using the dummy module | Share pickled work via a Queue | Share state between parallelized workers, including bytes, primitive datatypes, dictionaries, and lists | . Joblib: stronger cross-platform support than multiprocessing . Replacing multiprocessing with Joblib . Joblib is an improvement on multiprocessing | Enables lightweight pipelining with a focus on: easy parallel computing | transparent disk-based caching of results | . | It focuses on NumPy arrays for scientific computing | Quick wins: process a loop that could be embarrassingly parallel | expensive functions that have no side effect | able to share numpy data between processes | . | Parallel class: sets up the process pool | delayed decorator: wraps our target function so it can be applied to the instantiated Parallel object via an iterator | . Intelligent caching of function call results . Memory cache: decorator that caches functions results based on the input arguments to a disk cache . Using numpy . numpy is more cache friendly | numpy can achieve some level of additional speedup around threads by working outside the GIL | . Asynchronous systems . Require a special level of patience. Suggestions: . K.I.S.S. | Avoiding asynchronous self-contained systems if possible, as they will grow in complexity and quickly become hard to maintain | Using mature libraries like gevent that give you tried-and-tested approaches to dealing with certain problem sets | . Interprocess Communication (IPC) . Cooperation cost can be high: synchronizing data and checking the shared data | Sharing state tends to make things complicated | IPC is fairly easy but generally comes with a cost | . multiprocessing.Manager() . Lets us share higher-level Python objects between processes as managed shared objects; the lower-level objects are wrapped in proxy objects | The wrapping and safety have a speed cost but also offer great flexibility. | You can share both lower-level objects (e.g., integers and floats) and lists and dictionaries. | . Redis . Key/value in-memory storage engine. It provides its own locking and each operation is atomic, so we don’t have to worry about using locks from inside Python (or from any other interfacing language). | Lets you share state not just with other Python processes but also other tools and other machines, and even to expose that state over a web-browser interface | Redis lets you store: Lists of strings; Sets of strings; Sorted sets of strings; Hashes of strings | Stores everything in RAM and snapshots to disk | Supports master/slave replication to a cluster of instances | Widely used in industry and is mature and well trusted | . mmap . Memory-mapped (shared memory) solution | The bytes in a shared memory block are not synchronized and they come with very little overhead | Bytes act like a file -&gt; block of memory with a file-like interface | . Ch10. Clusters and Job Queues . Cluster: collection of computers working together to solve a common task . Before moving to a clustered solution: . Profile your system to understand the bottlenecks | Exploit compile solutions (Numba, Cython) | Exploit multiple cores on a single machine (Joblib, multiprocessing) | Exploit techniques for using less RAM | Really need a lot of CPUs, high resiliency, rapid speed of response, ability to process data from disks in parallel | . Benefits of clustering . Easily scale computing requirements | Improve reliability | Dynamic scaling | . Drawbacks of clustering . Change in thinking | Latency between machines | Sysadmin problems: software versions between machines, are other machines working? | Moving parts that need to be in sync | “If you don’t have a documented restart plan, you should assume you’ll have to write one at the worst possible time” | . Using a cloud-based cluster can mitigate a lot of these problems, and some cloud providers also offer a spot-priced market for cheap but temporary computing resources. . A system that’s easy to debug probably beats having a faster system | Engineering time and the cost of downtime are probably your largest expenses | . Parallel Pandas with Dask . Provide a suite of parallelization solutions that scales from a single core on a laptop to multicore machines to thousands of cores in a cluster. | “Apache Spark lite” | For Pandas users: larger-than-RAM datasets and desire for multicore parallelization | . Dask . Bag: enables parallelized computation on unstructured and semistructured data | Array: enables distributed and larger-than-RAM numpy operations | Distributed DataFrame: enables distributed and larger-than-RAM Pandas operations | Delayed: parallelize chains of arbitrary Python functions in a lazy fashion | Futures: interface that includes Queue and Lock to support task collaboration | Dask-ML: scikit-learn-like interface for scalable machine learning | . You can use Dask (and Swifter) to parallelize any side-effect-free function that you’d usually use in an apply call . npartitions = # cores | . Swifter . Builds on Dask to provide three parallelized options with very simple calls: apply, resample and rolling . Vaex . String-heavy DataFrames | Larger-than-RAM datasets | Subsets of a DataFrame -&gt; Implicit lazy evaluation | . NSQ for robust production clustering . Highly performant distributed messaging platform | Queues: type of buffer for messages | Pub/subs: describes who gets what messages (publisher/subscriber) | . Ch11. Using less RAM . Counting the amount of RAM used by Python object is tricky -&gt; if we ask the OS for a count of bytes used, it will tell us the total amount allocated to the process | Each unique object has a memory cost | . Objects for primitives are expensive . memory_profiler . %load_ext memory_profiler %memit &lt;operation&gt; . The array module stores many primitive objects cheaply . Creates a contiguos block of RAM to hold the underlying data. Which data structures: integers, floats and characters | not complex numbers or classes | . | Good to pass the array to an external process or use only some of the data (not to compute on them) | Using a regular list to store many numbers is much less efficient in RAM than using an array object | numpy arrays are almost certainly a better choice if you are doing anything heavily numeric: more datatype options | many specialized and fast functions | . | . Using less RAM in NumPy with NumExpr . NumExpr is a tool that both speeds up and reduces the size of intermediate operations . Install the optional NumExpr when using Pandas (Pandas does not tell you if you haven’t installed NumExpr) -&gt; calls to eval will run more quickly -&gt; import numpexpr: if this fails, install it! . NumExpr breaks the long vectors into shorter, cache-friendly chunks and processes each in series, so local chunks of results are calculated in a cache-friendly way | . Bytes versus Unicode . Python 3.x, all strings are Unicode by default, and if you want to deal in bytes, you’ll explicitly create a byte sequence | UTF-8 encoding of a Unicode object uses 1 byte per ASCII character and more bytes for less frequently seen characters | . More efficient tree structures to represent strings . Tries: share common prefixes | DAWG: share common prefixes and suffixes | Overlapping sequences in your strings -&gt; you’ll likely see a RAM improvement | Save RAM and time in exchange for a little additional effort in preparation | Unfamiliar data structures to many developers -&gt; isolate in a module to simplify maintenance | . Directed Acyclic Word Graph (DAWG) . Attemps to efficiently represent strings that share common prefixes and suffixes . Marisa Trie . Static trie using Cython bindings to an external library -&gt; it cannot be modified after construction . Scikit-learn’s DictVectorizer and FeatureHasher . DictVectorizer: takes a dictionary of terms and their frequences and converts them into a variable-width sparse matrix -&gt; it is possible to revert the process | FeatureHasher: converts the same dictionary of terms and frequencies into a fixed-width sparse matrix -&gt; it doesn’t store a vocabulary and instead employs a hashing algorithm to assign token frequencies to columns -&gt; can’t convert it back to the original token from hash | . ScyPy’s Sparse Matrices . Matrix in which most matrix elements are 0 | C00 matrices: simplest implementation: each non-zero element we store the value in addition to the location of the value -&gt; each non-zero value = 3 numbers stored -&gt; used only to contruct sparse matrices and not for actual computation | CSR/CSC is preferred for computation | . Push and pull of speedups with sparse arrays: balance between losing the use of efficient caching and vectorization versus not having to do a lot of the calculations associated with the zero values of the matrix . Limitations: . Low amount of support | Multiple implementations with benefits and drawbacks | May require expert knowledge | . Tips for using less RAM . “If you can avoid putting it into RAM, do. Everything you load costs you RAM” . Numeric data: switch to using numpy arrays | Very sparse arrays: SciPy’s sparse array functionality | Strings: stick to str rather than bytes | Many Unicode objects in a static structure: DAWG and trie structures | Lots of bit strings: numpy and the bitarray package | . Probabilistic Data Structures . Make trade-offs in accuracy for immense decrease in memory usage | The number of operations you can do on them is much more restricted | . “Probabilistic data structures are fantastic when you have taken the time to understand the problem and need to put something into production that can answer a very small set of questions about a very large set of data” . “lossy compression”: find an alternative representation for the data that is more compact and contains the relevant information for answering a certain set of questions | . Morris counter . Keeps track of an exponent and models the counted state as 2^exponent -&gt; provides an order of magnitude estimate . K-Minimum values . If we keep the k smallest unique hash values we have seen, we can approximate the overall spacing between hash values and infer the total number of items . idempotence: if we do the same operation, with the same inputs, on the structure multiple times, the state will not be changed | . Bloom filters . Answer the question of whether we’ve seen an item before | Work by having multiple hash values in order to represent a value as multiple integers. If we later see something with the same set of integers, we can be reasonably confident that it is the same value | No false negatives and a controllable rate of false positives | Set to have error rates below 0.5% | . Ch12. Lessons from the field .",
            "url": "https://millengustavo.github.io/blog/book/python/software%20engineering/2020/06/10/high-performance-python.html",
            "relUrl": "/book/python/software%20engineering/2020/06/10/high-performance-python.html",
            "date": " • Jun 10, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Machine Learning: tests and production",
            "content": "“Creating reliable, production-level machine learning systems brings on a host of concerns not found in small toy examples or even large offline research experiments. Testing and monitoring are key considerations for ensuring the production-readiness of an ML system, and for reducing technical debt of ML systems.” - The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction . I recently read the excellent book written by Emmanuel Ameisen: Building Machine Learning Powered Applications Going from Idea to Product . I definitely recommend the book to people involved at any stage in the process of developing and implementing products that use Machine Learning. . The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction . A good reference I found in chapter 6 of the book entitled: Debug your ML problems, was the article written by Google engineers: The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction . In the article, the authors: . “(…) present 28 specific tests and monitoring needs, drawn from experience with a wide range of production ML systems to help quantify these issues and present an easy to follow road-map to improve production readiness and pay down ML technical debt.” . As a good practice presented by Ameisen in his book when referring to reproducing successful results from the past: “Stand on the shoulders of giants”, I believe that we can learn from Google’s experience in building applications using Machine Learning. . Manually coded systems vs. ML-based systems . . Unlike manually coded systems, the behavior of machine learning systems depends on data and models that are not always possible to specify fully a priori . “(…) training data needs testing like code, and a trained ML model needs production practices like a binary does, such as debuggability, rollbacks and monitoring” . Tests . Data . . Model . . Infrastructure . . Monitoring . . Computing an ML Test Score . . Insights from applying the rubric to real systems . Checklists are helpful even for expert teams | Data dependencies can lead to outsourcing responsibility for fully understanding it | The importance of frameworks: pipeline platforms may allow building generic integration tests | . Conclusion . Deploying a machine learning system with monitoring is a very complex task. This is a problem faced by virtually all players in the market who are starting their journey with data. . A good first step on this journey is to organize your data pipeline and use managed environments in the cloud for the ML tasks: . Amazon SageMaker | Google Cloud AI Platform | Azure Machine Learning Studio | Databricks | . Even if we do not face the scale of some problems mentioned in the article, it is worth reflecting on how we can improve what we do today to reduce technical debt in the future. . Further reading . The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction | Building Machine Learning Powered Applications Going from Idea to Product - Emmanuel Ameisen | ML in Production blog | What is your machine learning test score? | MLflow: A Machine Learning Lifecycle Platform | Databricks for Machine Learning | .",
            "url": "https://millengustavo.github.io/blog/tests/machine%20learning/data%20science/deploy/2020/06/01/ml-test-production.html",
            "relUrl": "/tests/machine%20learning/data%20science/deploy/2020/06/01/ml-test-production.html",
            "date": " • Jun 1, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Building Machine Learning Powered Applications: Going from Idea to Product",
            "content": "My notes and highlights on the book. . Author: Emmanuel Ameisen . Part I. Find the Correct ML Approach . Ch1. From Product Goal to ML Framing . ML is particularly useful to build systems for which we are unable to define a heuristic solution . Start from a concrete business problem, determine whether it requires ML, then work on finding the ML approach that will allow you to iterate as rapidly as possible . Framing your product goal in an ML paradigm | Evaluating the feasibility of that ML task | Estimating the challenge of data acquisition ahead of time is crucial in order to succeed . Data availability scenarios . Labeled data exists | Weakly labeled data exists | Unlabeled data exists | We need to acquire data | . “Having an imperfect dataset is entirely fine and shouldn’t stop you. The ML process is iterative in nature, so starting with a dataset and getting some initial results is the best way forward, regardless of the data quality.” . The Simplest Approach: being the algorithm . Start with a human heuristic and then build a simple model: initial baseline = first step toward a solution -&gt; Great way to inform what to build next . What to focus on in an ML project . Find the impact bottleneck: piece of the pipeline that could provide the most value if improved . Imagine that the impact bottleneck is solved: it was worth the effort you estimate it would take? . Which modeling techniques to use . Spend the manual effort to look at inputs and outputs of your model: see if anything looks weird. Looking at your data helps you think of good heuristics, models and ways to reframe the product . Ch2. Create a Plan . Measuring Success . First model: simplest model that could address a product’s needs -&gt; generating and analyzing results is the fastest way to make progress in ML . Baseline: heuristics based on domain knowledge | Simple model | Complex model | . You don’t always need ML: even features that could benefit from ML can often simply use a heuristic for their first version (you may realize that you don’t need ML at all) . Business Performance . Product metrics: goals of your product or feature. Ultimately the only ones that matter, all other metrics should be used as tools to improve product metrics . Updating an app to make a modeling task easier . Change an interface so that a model’s results can be omitted if they are below a confidence threshold | Present a few other predictions or heuristics in addition to model’s top prediction | Communicate to users that model is still in an experimental phase and giving them opportunities to provide feedback | . “A product should be designed with reasonable assumptions of model performance in mind. If a product relies on a model being perfect to be useful, it is very likely to produce innacurate or even dangerous results” . Freshness and Distribution Shift . Distribution of the data shifts -&gt; model often needs to change in order to maintain the same level of performance . Leverage Domain Expertise . Best way to devise heuristics -&gt; see what experts are currently doing. Most practical applications are not entirely novel. How do people currently solve the problem you are trying to solve? . Second best way -&gt; look at your data. Based on your dataset, how would you solve this task if you were doing it manually? . Examining the data . EDA: process of visualizing and exploring a dataset -&gt; to get an intuition to a given business problem. Crucial part of building any data product . Stand on the Shoulders of giants . Reproduce existing results | Build on top of them | To make regular progress: start simple . Start with the simplest model that could address your requirements | Build an end-to-end prototype including this model | Judge its performance: optimization metrics and product goal | Looking at the performance of a simple model on an initial dataset is the best way to decide what task should be tackled next . Diagnose Problems . Write analysis and exploration functions: . Visualize examples the model performs the best and worst on | Explore data | Explore model results | . Part II. Build a Working Pipeline . Ch3. Build your first end-to-end pipeline . First iteration: lackluster by design. Goal: allow us to have all the pieces of a pipeline in place: . prioritize which ones to improve next | identify the impact bottleneck | . “Frequently, your product is dead even if your model is successful” - Monica Rogati . Test your workflow . Evaluate: . usefulness of the current user experience | results of your handcrafted model | . Finding the impact bottleneck . Next challenge to tackle next: . iterating on the way we present results to the users or; | improving model performance by identifying key failure points | . Ch4. Acquire an initial dataset . Understanding your data well leads to the biggest performance improvements . Iterate on datasets . Data gathering, preparation and labeling should be seen as an iterative process, just like modeling . ML engineering: engineering + ML = products . Choosing an initial dataset, regularly updating it, and augmenting it is the majority of the work . Data: best source of inspiration to develop new models and the first place to look for answers when things go wrong . Models only serve as a way to extract trends and patterns from existing data. Don’t overestimate the impact of working on the model and underestimate the value of working on the data . Before noticing predictive trends, start by examining quality . Data quality rubric . Format . Validate that you understand the way in which the data was processed . Quality . Notice the quality ahead of time -&gt; missing labels, weak labels . Quantity and distribution . Estimate: . enough data? | feature values are within reasonable range? | . Summary statistics . Identifying differences in distributions between classes of data early: will either make our modeling task easier or prevent us from overestimating the performance of a model that may just be leveraging one particularly informative feature. . Data leakage . Using training and validation data for vectorizing/preprocessing can cause data leakage -&gt; leveraging info from outside the training set to create training features . Clustering . As with dimensionality reduction: additional way to surface issues and interesting data points . Let data inform features and models . The more data you have and the less noisy your data is, the less feature engineering work you usually have to do . Feature crosses . Feature generated by multiplying (crossing) two or more features -&gt; nonlinear combination of features -&gt; allows our model to discriminate more easily . Giving your model the answer . New binary feature that takes a nonzero value only when relevant combination of values appear . Robert Munro: how do you find, label and leverage data . Uncertainty sampling . Identify examples that your model is most uncertain about and find similar examples to add to the training set . “Error model” . Use the mistakes your model makes as labels: “predicted correctly” or “predicted incorrectly”. Use the trained error model on unlabeled data and label the examples that it predicts your model will fail on . “Labeling model” . To find the best examples to label next. Identify data points that are most different from what you’ve already labeled and label those . Validation . While you should use strategies to gather data, you should always randomly sample from your test set to validate your model . Part III. Iterate on Models . Ch5. Train and evaluate your model . The simplest appropriate model . Not the best approach: try every possible model, benchmark and pick the one with the best results on a test set . Simple model . Quick to implement: won’t be your last | Understandable: debug easily | Deployable: fundamental requirement for a ML-powered application | . Model explainability and interpretability: ability for a model to expose reasons that caused it to make predictions . Test set . “While using a test set is a best practice, practitioners sometimes use the validation set as a test set. This increases the risk of biasing a model toward the validation set but can be appropriate when running only a few experiments” . Data leakage . Temporal data leakage | Sample contamination | . Always investigate the results of a model, especially if it shows surprisingly strong performance . Bias variance trade-off . Underfitting: weak performance on the training set = high bias | Overfitting: strong performance on the training set, but weak performance on the validation set = high variance | . Evaluate your model: look beyond accuracy . Contrast data and predictions | Confusion matrix: see whether our model is particularly successful on certain classes and struggles on some others | ROC Curve: plot a threshold on it to have a more concrete goal than simply getting the largest AUC score | Calibration Curve: whether our model’s outputed probability represents its confidence well. Shows the fraction of true positive examples as a function of the confidence of our classifier | Dimensionality reduction for errors: identify a region in which a model performs poorly and visualize a few data points in it | The top-k method k best performing examples: identify features that are successfully leveraged by a model | k worst performing examples: on train: identify trends in data the model fails on, identify additional features that would make them easier for a model. On validation: identify examples that significantly differ from the train data | k most uncertain examples: on train: often a symptom of conflicting labels. On validation: can help find gaps in your training data | . | . Top-k implementation: book’s Github repository . Evaluate Feature Importance . Eliminate or iterate on features that are currently not helping the model | Identify features that are suspiciously predictive, which is often a sign of data leakage | . Black-box explainers . Attempt to explain a model’s predictions independently of its inner workings, i.e. LIME and SHAP . Ch6. Debug your ML problems . Software Best Practices . KISS principle: building only what you need . Most software applications: strong test coverage = high confidence app is functioning well. ML pipelines can pass many tests, but still give entirely incorrect results. Doesn’t have just to run, it should produce accurate predictive outputs . Progressive approach, validate: . Data flow | Learning capacity | Generalization and inference | Make sure your pipeline works for a few examples, then write tests to make sure it keeps functioning as you make changes . Visualization steps . Inspect changes at regular intervals . Data loading: Verify data is formatted correctly | Cleaning and feature selection: remove any unnecessary information | Feature generation: check that the feature values are populated and that the values seem reasonable | Data formatting: shapes, vectors | Model output: first look if the predictions are the right type or shape, then check if the model is actually leveraging the input data | . Separate your concerns . Modular organization: separate each function so that you can check that it individually works before looking at the broader pipeline. Once broken down, you’ll be able to write tests . Test your ML code . Source code on book’s Github repository . Test data ingestion | Test data processing | Test model outputs | . Debug training: make your model learn . Contextualize model performance: generate an estimate of what an acceptable error for the taks is by labeling a few examples yourself . Task difficulty . The quantity and diversity of data you have: more diverse/complex the problem = more data for the model to learn from it | How predictive the features are: make the data more expressive to help the model learn better | The complexity of your model: simplest model is good to quickly iterate, but some tasks are entirely out of reach of some models | . Debug generalization: make your model useful . Data Leakage: if you are surprised by validation performance, inspect the features; fixing a leakage issue will lead to lower validation performance, but a better model | Overfitting: model performs drastically better on the training set than on the test set. Add regularization or data augmentation | Dataset redesign: use k-fold cross validation to alleviate concerns that data splits may be of unequal quality | . “If your models aren’t generalizing, your task may be too hard. There may not be enough information in your training examples to learn meaningful features that will be informative for future data points. If that is the case, then the problem you have is not well suited for ML” . Ch7. Using classifiers for writing recommendations . Part IV. Deploy and Monitor . Production ML pipelines need to be able to detect data and model failures and handle them with grace -&gt; proactively . Ch8. Considerations when deploying models . How was the data you are using collected? | What assumptions is your model making by learning from this dataset? | Is this dataset representative enough to produce a useful model? | How could the results of your work be misused? | What is the intended use and scope of your model? | . Data Concerns . Data ownership . Collection | Usage and permission | Storage | . Data bias . Datasets: results of specific data collection decisions -&gt; lead to datasets presenting a biased view of the world. ML models learn from datasets -&gt; will reproduce these biases . Measurement errors or corrupted data | Representation | Access | . Test sets . Build a test set that is inclusive, representative, and realistic -&gt; proxy for performance in production -&gt; improve the chances that every user has an equally positive experience . Models are trained on historical data -&gt; state of the world in the past. Bias most often affects populations that are already disenfranchised. Working to eliminate bias -&gt; help make systems fairer for the people who need it most . Modeling Concerns . Feedback loops . User follow a model’s recommendation -&gt; future models make the same recommendation -&gt; models enter a self-reinforcing feedback loop . To limit negative effects of feedback loops -&gt; choose a label that is less prone to creating such a loop . Inclusive model performance . Look for performance on a segment of the data, instead of only comparing aggregate performance . Adversaries . Regularly update models . Some types of attacks: . Fool models into a wrong prediction (most common) | Use a trained model to learn about the data it was trained on | . Chris Harland: Shipping Experiments . When giving advice, the cost of being wrong is very high, so precision is the most useful . Ch9. Choose Your Deployment Option . Server-side deployment . Setting up a web server that can accept requests from clients, run them through an inference pipeline, and return the results. The servers represents a central failure point for the application and can be costly if the product becomes popular . Streaming API workflow . Endpoint approach . Quick to implement | Requires infrastructure to scale linearly with the current number of users (1 user = 1 separate inference call) | Required when strong latency constraints exist (info the model needs is available only at prediction time and model’s prediction is required immediately) | . Batch Predictions . Inference pipeline as a job that can be run on multiple examples at once. Store predictions so they can be used when needed . Appropriate when you have access to the features need for a model before the model’s prediction is required | Easier to allocate and parallelize resources | Faster at inference time since results have been precomputed and only need to be retrieved (similar gains to caching) | . Hybrid Approach . Precompute as many cases as possible | At inference either retrieve precomputed results or compute them on the spot if they are not available or are outdated | Have to maintain both a batch and streaming pipeline (more complexity of the system) | . Client-side deployment . Run all computations on the client, eliminating the need for a server to run models. Models are still trained in the same manner and are sent to the device for inference . Reduces the need to build infra | Reduces the quantity of data that needs to be transferred between the device and the server Reduces network latency (app may even run without internet) | Removes the need for sensitive information to be transferred to a remote server | . | . If the time it would take to run inference on device is larger than the time it would take to transmit data to the server to be processed, consider running your model in the cloud . On-device deployment is only worthwhile if the latency, infrastructure, and privacy benefits are valuable enough to invest the engineering effort (simplifying a model) . Browser side . Some libraries use browsers to have the client perform ML tasks . Tensorflow.js: train and run inference in JavaScript in the browser for most differentiable models, even trained in different languages such as Python . Federated Learning: a hybrid apporach . Each client has their own model. Each model learns from their user’s data and send aggregated (and potentially anonymized) updates to the server. The server leverages all updates to improve its model and distills this new model back to individual clients. Each user receives a model personalized to their needs, while still benefiting from aggregate information about other users . Ch10. Build Safeguards for Models . No matter how good a model is, it will fail on some examples -&gt; engineer a system that can gracefully handle such features . Check inputs . Very different data from train | Some features missing | Unexpected types | . Input checks are part of the pipeline -&gt; change the control flow of a program based on the quality of inputs . Model outputs . Prediction falls outside an acceptable range -&gt; consider not displaying it . Acceptable outcome: not only if the outcome is plausible -&gt; also depends if the outcome would be useful for the user . Model failure fallbacks . Flag cases that are too hard and encourage user to provide an easier input (e.g. well-lit photo) . Detecting errors: . Track the confidence of a model | Build an additional model tasked with detecting examples a main model is likely to fail on | . Filtering model . ML version of input tests | Binary classifier | Estimate how well a model will perform on an example without running the model on it | Decrease the likelihood of poor results and improve resource usage | Catch: qualitatively different inputs | inputs the model struggled | adversarial inputs meant to fool the model | . | Minimum criteria: should be fast (reduce the computational burden) | should be good at eliminating hard cases | . | . The faster your filtering model is, the less effective it needs to be . Engineer for Performance . Scale to multiple users . ML is horizontally scalable = more servers = keep response time reasonable when the number of requests increases . Caching fo ML . Storing results to function calls -&gt; future calls with same parameters simply retrieve the stored results . Caching inference results . Least recently used (LRU) cache: keep track the most recent inputs to a model and their corresponding outputs . not appropriate if each input is unique | functools Python module proposes a default implementation of an LRU cache that you can use with a simple decorator | . from functools import lru_cache @lru_cache(maxsize=128) def run_model(data): # Insert slow model inference below pass . Caching by indexing . Cache other aspects of the pipeline that can be precomputed. Easy if a model does not only rely on user inputs . “Caching can improve performance, but it adds a layer of complexity. The size of the cache becomes an additional hyperparameter to tune depending on your application’s workload. In addition, any time a model or the underlying data is updated, the cache needs to be cleared in order to prevent it from serving outdated results” . Model and data life cycle management . ML application: . produces reproducible results | is resilient to model updates | is flexible enough to handle significant modelling and data processing changes | . Reproducibility . Each model/dataset pair should be assigned an unique identifier -&gt; should be logged each time a model is used in production . Resilience . production pipeline should aim to update models without significant downtime | if a new model performs poorly, we’d like to be able to roll back to the previous one | . Data Processing and DAGs . Directed acyclic graph (DAG): can be used to represent our process of going from raw data to trained model -&gt; each node represent a processing step and each step represent a dependency between two nodes . DAGs helps systematize, debug, and version a pipeline -&gt; can become a crucial time saver . Ask for feedback . Explicity asking for feedback (display model’s prediction accompanying it with a way for users to judge and correct a prediction) | Measuring implicit signals | . User feedback is a good source of training data and can be the first way to notice a degradation in performance . Chris Moody: Empowering Data Scientist to Deploy Models . Make humans and algorithms work together: spend time thinking about the right way to present information | Canary development -&gt; start deploying the new version to one instance and progressively update instances while monitoring performance | . “Ownership of the entire pipeline leads individuals to optimize for impact and reliability, rather than model complexity” . Ch11. Monitor and update models . Monitoring saves lives . Monitoring: track the health of a system. For models: performance and quality of their predictions . Monitor to inform refresh rate . Detect when a model is not fresh anymore and needs to be retrained. Retraining events happen when accuracy dips below a threshold. . Monitor to detect abuse . Anomaly detection to detect attacks . Choose what to monitor . Commonly monitor metrics such as the average time it takes to process a request, the proportion of requests that fail to be processed, and the amount of available resources . Performance Metrics . Track changes in the input distribution (feature drift) | Monitor the input distribution (summary statistics) | Monitor distribution shifts | . Conterfactual evaluation: aims to evaluate what would have happened if we hadn’t actioned a model -&gt; Not acting on a random subset of examples allow us to observe an unbiased distribution of the positive class. Comparing model predictions to true outcomes for the random data, we can begin to estimate a model’s precision and recall . Business metrics . Product metrics should be closely monitored . CI/CD for ML . CI: letting multiple developers regularly merge their code back into a central codebase | CD: improving the speed at which new versions of software can be released | . CI/CD for ML: make it easier to deploy new models or update existing ones . “Releasing updates quickly is easy; the challenge comes in guaranteeing their quality (…) There is no substitute for live performance to judge the quality of a model” . Shadow mode: deploying a new model in parallel to an existing one. When running inference, both models’ predictions are computed and stored, but the application only uses the prediction of the existing model . estimate a new models’ performance in a production environment without changing the user experience | test the infrastructure required to run inference for a new model (may be more complex) | but can’t observe the user’s response to the new model | . A/B Testing and Experimentation . Goal: maximize chances of using the best model, while minimizing the cost of trying out suboptimal models . Expose a sample of users to a new model, and the rest to another. Larger control group (current model) and a smaller treatment group (new version we want to test). Run for a sufficient amount of time -&gt; compare the results for both groups and choose the better model . Choosing groups and duration . Users in both groups should be as similar as possible -&gt; any difference in outcome = our model and not difference in cohorts | Treatment group should be: large enough: statistically meaningful conclusion | small as possible: limit exposure to a potentially worse model | . | Duration of the test: too short: not enough information | too long: risk losing users | . | . Estimating the better variant . Decide on the size of each group and the length of the experiment before running it . Building the infrastructure . Branching logic: decides which model to run depending on a given field’s value (harder if a model is accessible to logged-out users) . Other approaches . Multiarmed bandits: more flexible approach, can test variants continually and on more than two alternatives. Dynamically update which model to serve based on how well each option in performing . Contextual multiarmed bandits: go even further, by learning which model is a better option for each particular user . “The majority of work involved with building ML products consists of data and engineering work” .",
            "url": "https://millengustavo.github.io/blog/book/machine%20learning/data%20science/deploy/2020/05/22/build-ml-app.html",
            "relUrl": "/book/machine%20learning/data%20science/deploy/2020/05/22/build-ml-app.html",
            "date": " • May 22, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Leadership Strategy and Tactics: Field Manual",
            "content": "What makes leadership so hard is dealing with people, and people are crazy. And the craziest person a leader has to deal with is themselves. . Those are the notes I took when reading the book “Leadership Strategy and Tactics: Field Manual” by Jocko Willink. I recommend reading the book for a more coherent view of the context in which each note is inserted. . First Platoon: Detach . Pay attention to yourself and what is happening around you. | Avoid being fully absorbed in the minute details of any situation. | Stay aware, check yourself, avoid getting tunnel vision. | Put the team and the mission above yourself. | Prioritize and Execute. The most impactful task or the biggest problem must be addressed first | Extreme Ownership is a mind-set of not making excuses and not blaming anyone or anything else when problems occur. | The Dichotomy of Leadership describes opposing forces that are pulling leaders in contradictory directions at the same time. To lead properly, a leader must be balanced | Leadership requires relationships. The better the relationships, the more open and effective communication there is. The more communication there is, the stronger the team will be. | Communicate ideas in a simple, clear manner | Look people in the eye when talking to them, listen intently to what others say, and speak clearly with humble authority. | Pay attention to body language, facial expressions, and tone of voice. | . “There is one type of person who can never become a good leader: a person who lacks humility. People who lack humility cannot improve because they don’t acknowledge their own weaknesses.” . The good of the mission and the good of the team outweigh any personal concern a true leader has for themselves. | Good leaders do the right things for the right reasons; they work hard, support the team, and lead solid execution. | Ego is like reactive armor; the harder you push against it, the more it pushes back. | Subordinating your ego is actually the ultimate form of self-confidence. That level of confidence earns respect. | Communicate often so the bad news will sting less. | Lead from the front, especially when things are bad. As a leader, do the hard things. Don’t leave it to the troops. | Most criticism is best delivered indirectly, with the minimal amount of negativity needed to get the desired change. | A leader must be constantly improving and learning. | . Core Tenets . If you need help with something, ask for it. Subordinates understand that their leaders might not know everything. | If you want to have influence over others, you need to allow them to have influence over you. | Preemptive ownership: take ownership of things to prevent problems from unfolding in the first place | People tend to shy away from suffering; they will procrastinate and avoid getting started. But when the leader jumps in and starts attacking the job, others will jump in and get started as well. | The best ideas often come from the people on the team who are closest to the problem; those are the folks on the front line. Take a step back and let your team lead. | Overreaction is always bad. | Relationships do not mean preferential treatment. | Before diving into a problem: How will this problem impact the team’s strategic goals? Can it cause mission failure? Is it worth my time and effort to engage in it? How bad can it get if I leave it alone? | The goal is always to allow problems to get solved at the lowest level. When subordinates are solving low-level problems, it allows the leader to focus on more important, strategic issues. | . Principles . Every person’s job is absolutely critical. Explain to them what happens if they don’t do their jobs well. How their jobs fit into the big picture and the strategic mission. | If you really want to take care of your people, you need to push them. You need to make sure they understand their jobs. You need to drive them toward their goals. | . “The easy path leads to misery. The path of discipline leads to freedom.” . Optimal discipline in a team is not imposed by the leader; it is chosen by the team itself. Optimal discipline is self-discipline | A leader doesn’t have to constantly police infractions and motivate them to give their best; if there is pride, the team polices itself. | To build pride within a team, you have to put the members in situations that require unity, strength, and perseverance to get through. | No pride is built on easy wins, but a team has to win some to have some pride | Encourage the rest of your team to think and to question you. Don’t surround yourself with yes-men. They do nothing to help you or the team. | . “There are no situations and no exceptions where a subordinate is ultimately responsible for the performance of a team. It is always the leader’s fault. The exception is that it is possible to have a good team that delivers outstanding performance despite a bad leader.” . Becoming a Leader . Don’t make being chosen as a leader your goal. Instead, make your goal helping the team win. | Lack of preparation shows the team you don’t really care. So stay humble, study, ask questions, learn, and balance the dichotomy between too much humility and too much confidence. | Don’t be the leader with your hands in your pockets, but don’t be the leader with your hands in everything. | Don’t change things that are working, but don’t accept things that are not working | In everyday situations, overt leadership is not needed. It is better to give subtle direction and let the troops move forward based on their own ideas. | The best leaders usually led not by orders but by suggestion. | Indirect leadership almost always trumps direct leadership. | . Leadership Skills . “It was always safe to assume that when different people had different ideas, the idea that people liked the best was almost always their own.” . Be decisive when you need to be, but try not to make decisions until you have to. Make smaller decisions with minimum commitment to move in the direction you most highly suspect is the right one. | Prevent giving your troops the impression that your delegation is avoidance of hard work is to take on some of the harshest jobs yourself. Do some of the nasty work. | Don’t alienate yourself from the group. Become part of it and earn your influence. This is the opposite of having an aggressive attitude and attacking the group’s beliefs head-on | . Maneuvers . One of the best tools a leader has to help shape others is leadership itself; giving people responsibility and putting them in leadership positions teaches them to be better in a multitude of ways. | . “Life is the ultimate teacher of humility. If a person lives long enough and takes on true challenges, eventually they will get humbled.” . The medicine for a lack of confidence is very similar to the medicine for overconfidence: put the individual in charge. | Micromanagement is a tool, but it is not a permanent solution. Use it, but know when it has reached its limitations, and then remove or replace personnel to fix the problem. | It is always good to support your leader. If you undermine a leader, it not only hurts them, it also hurts the morale of the troops as well as you as a subordinate leader. | The best way to treat combat stress—and any stress—is to remove the affected individual from the stress-inducing environment. | To punish an individual for the infraction of an unwritten rule is usually inappropriate, unless the behavior is grievous enough that any reasonable person would deem it out of line. | No one should be surprised when they receive a punishment, and knowing what they are risking in terms of punishment will eliminate much of the need for it. | . Communication . In any leadership situation, it is critical for the leader to keep everyone on the team as informed as possible. | You have to be proactive in updating your troops. | Get aggressive and attack rumors by getting ahead of the bad news and telling your team what is going on. Be truthful, be direct, and be timely. | Explaining why is important. But the why has to tie back and connect to everyone in the chain of command. | When delivering criticism, it is important to do it with consideration and delicacy. | . “As a leader, you must remember you are being watched. And in everything you do, you must set the example.” . Praise should be given when warranted. But it must be given judiciously, and it should be tempered with a goal that requires the team to still push. | Praise is a tool, but it is a tool that must be wielded with caution. Too much and it can cause people to let up and rest on their laurels. Too little and the team can lose hope. | Ultimatums are not optimal leadership tools. Like digging in, they allow no room to maneuver. No one likes being trapped and controlled. | A leader must have control over his or her emotions. Letting emotions drive decisions is a mistake. | Reflect and Diminish means to reflect the emotions you are seeing from your subordinate but diminish them to a more controlled level. | Patience is appreciated and respected much more than a hot temper. | The less you talk, the more people listen. Don’t be the person who is always talking. Speak when you need to, but don’t talk just to talk. | There is nothing wrong with apologizing when you make a mistake. That is part of taking ownership. | . Conclusion: It is All On You, But Not About You . “Leadership is all on you. But at the same time, leadership is not about you. Not at all. Leadership is about the team. The team is more important than you are. The moment you put your own interests above the team and above the mission is the moment you fail as a leader.” . Reference: . Willink, Jocko. Leadership Strategy and Tactics: Field Manual. https://www.amazon.com/Leadership-Strategy-Tactics-Field-Manual/dp/1250226848 .",
            "url": "https://millengustavo.github.io/blog/book/leadership/management/2020/03/17/leadership.html",
            "relUrl": "/book/leadership/management/2020/03/17/leadership.html",
            "date": " • Mar 17, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Causal Impact: state-space models in settings where a randomized experiment is unavailable",
            "content": "TL;DR: There are ways of measuring the causal impact of some business intervention even in scenarios where a randomized experiment is unavailable. In this post we investigated the increase in Google trends popularity index of some search terms caused by different interventions. The same logic can be applied in business contexts such as the impact of a new product launch, the onset of an advertising campaign and other problems in economics, epidemiology, biology among others. . Jupyter notebook with the code for this post . Motivation . At work, you are responsible for making many decisions that can impact aspects of your business in different ways. Most of the time, there are simple approaches to measure the impact of these decisions. . You launched an advertising campaign and, looking at sales that increased by 5% a month later, concluded that your campaign was a success. Right? Well, did you do a random experiment? Did you consider other factors such as seasonality or sales trend? . Similar to this scenario, there are a plethora of other cases in which we may be interested in measuring the causal impact of the action. . Causal Impact by Google . There are some complex aspects of infering the causality of an intervention. In 2015 some awesome researchers from Google, published a paper entitled: “INFERRING CAUSAL IMPACT USING BAYESIAN STRUCTURAL TIME-SERIES MODELS”. . Along with the paper they also introduced CausalImpact, an R package (there is also a Python port by Dafiti) that implements their approach. In this tutorial we are going to use the Python version. . Quoting directly from the abstract of the paper: . This paper proposes to infer causal impact on the basis of a diffusion-regression state-space model that predicts the counterfactual market response in a synthetic control that would have occurred had no intervention taken place. In contrast to classical difference-in-differences schemes, state-space models make it possible to (i) infer the temporal evolution of attributable impact, (ii) incorporate empirical priors on the parameters in a fully Bayesian treatment, and (iii) flexibly accommodate multiple sources of variation, including local trends, seasonality and the time-varying influence of contemporaneous covariates . Measuring the causal impact of mentioning an unusual term on a popular brazilian TV show . Context . Big Brother Brasil (BBB) is an international TV show that is popular in Brazil and broadcast on prime time open television. One of the participants suggested, live, that another participant search Google for an unusual term that she had spoken. The unusual word was “sororidade” (“sorority”). . Many media outlets later reported that the episode caused searches for the term to increase 250%. . The episode took place on February 9, 2020 between 9 pm and 10 pm Brazilian time. . Let’s investigate the causality of the intervention in increasing interest in the term based on Google trends. . 1. Gathering the data . We are going to download the data from Google Trends. . There are two main ways of doing this: . We can navigate to the website and specify which term we are looking for, the region and timeframe | We can do this directly in Python, using a third-party library such as pytrends | . To install pytrends, switch to your preferred environment and run the command: . pip install pytrends . From the presentation of the causal impact, the author of the package suggests that we use between 5 and 10 related time series that can help to model the behavior of our time series of interest. . Therefore, we are going to download some common Brazilian terms in the same period that may help our Bayesian structural time series model to understand the search behavior of our target term. . from pytrends.request import TrendReq pytrends = TrendReq(hl=&#39;pt-BR&#39;, tz=45) kw_list = [&quot;sororidade&quot;, &quot;futebol&quot;, &quot;carro&quot;, &quot;temperatura&quot;, &quot;restaurante&quot;] pytrends.build_payload(kw_list, cat=0, timeframe=&quot;2020-02-06T08 2020-02-13T07&quot;) # the interest_over_time() method returns a pandas DataFrame df = pytrends.interest_over_time() . date sororidade futebol carro temperatura restaurante isPartial . 2020-02-06 08:00:00 | 0 | 1 | 2 | 9 | 4 | False | . 2020-02-06 09:00:00 | 0 | 2 | 4 | 11 | 6 | False | . 2020-02-06 10:00:00 | 0 | 2 | 7 | 13 | 8 | False | . 2020-02-06 11:00:00 | 0 | 2 | 8 | 17 | 10 | False | . 2020-02-06 12:00:00 | 0 | 2 | 10 | 22 | 13 | False | . 2. Organizing the data . df = df.drop(columns=[&quot;isPartial&quot;]) df.plot(figsize=[16,9]) sns.despine() plt.ylabel(&quot;Interest over time (Google Trends)&quot;) plt.xticks(rotation=45) . . # changing the zeros to 0.1 so the model converge df[&quot;sororidade&quot;] = df[&quot;sororidade&quot;].apply(lambda x: 0.1 if x==0 else x) . 3. Causal Impact . pre_period = [ pd.to_datetime(np.min(df.index.values)), pd.to_datetime(np.datetime64(&quot;2020-02-09T22:00:00.000000000&quot;)), ] post_period = [ pd.to_datetime(np.datetime64(&quot;2020-02-09T23:00:00.000000000&quot;)), pd.to_datetime(np.max(df.index.values)), ] ci = CausalImpact(df, pre_period, post_period) print(ci.summary()) . Calling the summary method on the CausalImpact object we obtain a numerical summary of the analysis. . . print(ci.summary(output=&#39;report&#39;)) . For additional guidance about the correct interpretation of the summary table, the package provides a verbal interpretation, printed using the same method but with output=&quot;report&quot;. . . ci.plot() . Explaining the plots, in the package documentation: . “By default, the plot contains three panels. The first panel shows the data and a counterfactual prediction for the post-treatment period. The second panel shows the difference between observed data and counterfactual predictions. This is the pointwise causal effect, as estimated by the model. The third panel adds up the pointwise contributions from the second panel, resulting in a plot of the cumulative effect of the intervention.” . . Just by looking at the sorority interest over time plot we may have concluded that it was obvious the causal effect. . In addition, the CausalImpact report is somewhat misleading: our series had a lot of zeros, as it is an unusual term and we have counted the metrics of interest from Google trends over time. Because of this, all changes are greatly expanded, and this reflects the increase of +6735.8% in the response variable. . Let’s look at another example to consolidate our understanding. . Another example - Amazon rainforest burns . Context . From wikipedia: . “The forest fires in the Amazon in 2019 were a series of forest fires that affected South America, mainly Brazil. At least 161.236 fires were recorded in the country from January to October 2019, 45% more compared to the same period in 2018, which reached 84% in August” . Also from the same article: . “On August 11, Amazonas declared a state of emergency. NASA images showed that on August 13, smoke from fires was visible from space […]” . We will set August 11, 2019 as the intervention period for our causality study. . Causal Impact . kw_list = [&quot;Amazônia&quot;, &quot;Pantanal&quot;, &quot;Cerrado&quot;, &quot;Caatinga&quot;, &quot;Pampas&quot;] pytrends.build_payload(kw_list, cat=0, timeframe=&#39;2019-03-01 2020-01-01&#39;) df = pytrends.interest_over_time() . date Amazônia Pantanal Cerrado Caatinga Pampas isPartial . 2019-03-03 00:00:00 | 3 | 12 | 39 | 2 | 14 | False | . 2019-03-10 00:00:00 | 4 | 12 | 42 | 4 | 13 | False | . 2019-03-17 00:00:00 | 5 | 13 | 43 | 4 | 14 | False | . 2019-03-24 00:00:00 | 5 | 13 | 42 | 4 | 15 | False | . 2019-03-31 00:00:00 | 5 | 13 | 42 | 4 | 13 | False | . . df = df.drop(columns=[&quot;isPartial&quot;]) pre_period = [ pd.to_datetime(np.min(df.index.values)), pd.to_datetime(np.datetime64(&quot;2019-08-11&quot;)), ] post_period = [ pd.to_datetime(np.datetime64(&quot;2019-09-22&quot;)), pd.to_datetime(np.max(df.index.values)), ] ci = CausalImpact(df, pre_period, post_period) print(ci.summary()) . . print(ci.summary(output=&#39;report&#39;)) . . ci.plot() . . Here we have something closer to what we find on a daily basis, but still a little exaggerated. . We see that the percentage increase in the response variable after the intervention was +51.24% and this change was statistically significant. . Final thoughts . The examples shown here help to illustrate the concept and API of the package. . I invite you to go further and try to create business scenarios in which this type of report can be useful. At work, I had no trouble finding these scenarios and it is your duty, as a data scientist, to provide data-driven value where you deem it appropriate. . References . https://google.github.io/CausalImpact/CausalImpact.html | https://github.com/dafiti/causalimpact | https://www.uol.com.br/universa/noticias/redacao/2020/02/11/sororidade-buscas-no-google-crescem-250-apos-fala-de-manu-gavassi-no-bbb.htm | https://www.huffpostbrasil.com/entry/sororidade-manu-gavassi_br_5e442423c5b61b84d3438b88 | https://emais.estadao.com.br/noticias/tv,bbb-20-buscas-por-sororidade-no-google-sobem-250-apos-fala-de-manu-gavassi,70003193892 | https://pt.wikipedia.org/wiki/Inc%C3%AAndios_florestais_na_Amaz%C3%B4nia_em_2019 | .",
            "url": "https://millengustavo.github.io/blog/data%20science/machine%20learning/causality/statistics/2020/02/19/causalimpact.html",
            "relUrl": "/data%20science/machine%20learning/causality/statistics/2020/02/19/causalimpact.html",
            "date": " • Feb 19, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Statistical forecasting: notes on regression and time series analysis",
            "content": "My notes and highlights on the book. . Author: Robert Nau . Available here . “This web site contains notes and materials for an advanced elective course on statistical forecasting that is taught at the Fuqua School of Business, Duke University. It covers linear regression and time series forecasting models as well as general principles of thoughtful data analysis.” . “I have seen the future and it is very much like the present, only longer.” – Kehlog Albran, The Profit . 1. Get to know your data . Principles and risks of forecasting . Statistical forecasting: art and science of forecasting from data, with or without knowing in advance what equation you should use . Signal vs. noise . Variable you want to predict = signal + noise . Signal: predictable component | Noise: what is left over | . Sensitive statistical tests are needed to get a better idea of whether the pattern you see in the data is really random or whether there is some signal yet to be extracted. If you fail to detect a signal that is really there, or falsely detect a signal that isn’t really there, your forecasts will be at best suboptimal and at worst dangerously misleading . random walk model: the variable takes random steps up and down as it goes forward: . if you transform by taking the period-to-period changes (the “first difference”) it becomes a time series that is described by the mean model | the confidence limits for the forecasts gets wider at longer forecast horizons | typical of random walk patterns -&gt; they don’t look random as they are! -&gt; analyze the statistical properties: momentum, mean-reversion, seasonality | . Risks of forecasting . “If you live by the crystal ball you end up eating broken glass” . Intrinsic risk: random variation beyond explanation with the data and tools available | Parameter risk: errors in estimating the parameters of the forecasting model, under the assumption that you are fitting the correct model to the data in the first place When predicting time series, more sample data is not always better -&gt; might include older data that is not as representative of current conditions. Blur of history problem: no pattern really stays the same forever . | . You can’t eliminate instrinsic risk and parameter risk, you can and should try to quantify them in relative terms -&gt; so the appropriate risk-return tradeoffs can be made when decisions are based on the forecast . Model risk: risk of choosing the wrong model. Most serious form of forecast error -&gt; can be reduced by following good statistical practices: Follow good practices for exploring the data, understand the assumptions that are behind the models and test the assumptions. | . If the errors are not pure noise -&gt; there is some pattern in them, and you could make them smaller by adjusting the model to explain that pattern . Get to know your data . Where did it come from? | Where has it been? | Is it clean or dirty? | In what units is it measured? | . Assembling, cleaning, adjusting and documenting the units of the data is often the most tedious step of forecasting . PLOT THE DATA! . You should graph your data to get a feel for its qualitative properties -&gt; your model must accommodate these features and ideally it should shed light on their underlying causes . Inflation adjustment (“deflation”) . Accomplished by dividing a monetary time series by a price index, such as the Consumer Price Index (CPI) -&gt; uncover the real growth . original series: “nominal dollars” or “current dollars” | deflated series: “constant dollars” | . Not always necessary, sometimes forecasting the nominal data or log transforming for stabilizing the variance is simpler . Inflation adjustment is only appropriated for money series. If a non-monetary series shows signs of exponential growth or increasing variance -&gt; try a logarithm transformation . Seasonal adjustment . Multiplicative adjustment . Increasing amplitude of seasonal variations is suggestive of a multiplicative seasonal pattern -&gt; can be removed by multiplicative seasonal adjustment: dividing each value of the time series by a seasonal index that is representative of normal typically observed in that season . Additive adjustment . For time series whose seasonal variations are roughly constant in magnitude, independent of the current average level of the series -&gt; adding or subtracting a quantity that represents the absolute amount by which the value in that season of the year tends to be below or above normal, as estimated from past data . Additive seasonal patterns are somewhat rare, but if applying log transform -&gt; you should use additive rather than multiplicative . Acronyms . SA: seasonally adjusted | NSA: not seasonally adjusted | SAAR: seasonally adjusted annual rate -&gt; each period’s value has been adjusted for seasonality and then multiplied by the number of periods in a year, as though the same value had been obtained in every period for a whole year | . Stationarity and differencing . Statistical stationarity . A stationary time series is one whose statistical properties such as mean, variance, autocorrelation, etc. are all constant over time. Most statistical forecasting methods are based on the assumption that the time series can be rendered approximately stationary (i.e., “stationarized”) through the use of mathematical transformations . trend-stationary: series has a stable long-run rend and tends to revert to the trend line following a disturbance -&gt; to stationarize it = detrending | difference-stationary: if the mean, variance, and autocorrelations of the original series are not constant in time, even after detrending, perhaps the statistics of the changes in the series between periods or between seasons will be constant | . Unit root test: to understand if a series is trend-stationary or difference-stationary . First-difference . Series of changes from one period to the next . random walk model: if first-difference of a series is stationary and also completely random (not autocorrelated) | ETS or ARIMA: can be used when the first-difference of a series is stationary but not completely random (its value at period t is autocorrelated with its value at earlier periods) | . The logarithm transformation . Change in natural log ≈ percentage change . Small changes in the natural log of a variable are directly interpretable as percentage changes to a very close approximation . Linearization of exponential growth and inflation . The log transformation converts the exponential growth pattern to a linear growth pattern, and it simultaneously converts the multiplicative (proportional-variance) seasonal pattern to an additive (constant-variance) seasonal pattern . Logging a series often has an effect very similar to deflating: it straightens out exponential growth patterns and reduces heteroscedasticity (i.e., stabilizes variance). Logging is therefore a “poor man’s deflator” which does not require any external data . Geometric random walk: logging the data before fitting a random walk model -&gt; commonly used for stock price data . Trend measured in natural-log units ≈ percentage growth . Usually the trend is estimated more precisely by fitting a statistical model that explicitly includes a local or global trend parameter, such as a linear trend or random-walk-with-drift or linear exponential smoothing model. When a model of this kind is fitted in conjunction with a log transformation, its trend parameter can be interpreted as a percentage growth rate. . Errors measured in natural-log units ≈ percentage errors . If you look at the error statistics in logged units, you can interpret them as percentages if they are not too large – if the standard deviation is 0.1 or less . Coefficients in log-log regressions ≈ proportional percentage changes . In many economic situations (particularly price-demand relationships), the marginal effect of one variable on the expected value of another is linear in terms of percentage changes rather than absolute changes . 2. Introduction to forecasting: the simplest models . Review of basic statistics and the simplest forecasting model: the sample mean . Historical sample mean (or constant model, or intercept-only regression): if the series consists of i.i.d. values, the sample mean should be the next value if the goal is to minimize MSE . Why squared error? . the central value around which the um of squared deviations are minimized is the sample mean | variances are additive when random variables that are statistically independent are added together | large errors often have disproportionately worse consequences than small errors, hence the squared error is more representative of the economic consequences of error | variances and covariances play a key rola in normal distribution theory and regression analysis | . nonlinear transformations of the data (e.g., log or power transformations) can often be used to turn skewed distributions into symmetric (ideally normal) ones, allowing such data to be well fitted by models that focus on mean values. . Fundamental law of forecasting risk . Variance of forecasting risk = variance of intrinsic risk + variance of parameter risk . Confidence intervals: sort like a probability, but not exactly -&gt; there’s an x% probability that your future data will fall in your x% confidence interval for the forecast . Confidence interval = forecast ± (critical t-value) × (standard error of forecast) . 95% confidence interval is (roughly) the forecat “plus-or-minus two standard errors” . A rule of thumb: when adjusted R-squared is fairly small (say, less than 20%), the percentage by which the standard error of the regression model is less than the standard error of the mean model is roughly one-half of adjusted R-squared. . t-stats, P-values, and R-squared, and other test statistics are numbers you should know how to interpret and use, but they are not the most important numbers in your analysis and they are not the bottom line: . what new things have you learned from your data? | what assumptions does your model make? | would these assumptions make sense to someone else? | would a simpler model perform almost as well? | how accurate are your model’s predictions? | how accurate it is likely to be to predict the future? | how good are the inferences and decisions? | . Notes on the random walk model . Model assumes that in each period the variable takes a random step away from its previous value, and the steps are independently and identically distributed in size (“i.i.d.”). This is equivalent to saying that the first difference of the variable is a series to which the mean model should be applied. . if you begin with a series that wanders all over the map, the first difference looks i.i.d. sequence -&gt; random walk model is a potentially good candidate . without drift: all future values will equal the last observed value | with drift: the average increase from one period to the next (estimated drift = slope = d) | . Square root of time rule: The confidence interval for a k-period-ahead random walk forecast is wider than that of a 1-period-ahead forecast by a factor of square-root-of-k . Are the daily changes statistically independent as well having a mean of zero? autocorrelation plot -&gt; random walk without drift . The geometric random walk model . The natural logarithm of the variable is assumed to walk a random walk, usually with drift . Diff-logs are interpretable as (approximate) percentage changes . it is very hard to estimate the trend in a random-walk-with-drift model based on the mean growth that was observed in the sample of data unless the sample size is very large . Fitting a random-walk-with-drift model to the logged series is equivalent to fitting the geometric random walk model to the original series. . Reasons for using the random walk model . If you see what looks like pure noise (i.i.d. variations) after performing a 1st -difference or diff-log transformation, then your data is telling you that you that it is a random walk. This isn’t very exciting in terms of the point forecasts you should make (“next month will be the same as last month, plus average growth”), but it has very important implications in terms of how much uncertainty there is in forecasting more than one period ahead. | benchmark against which to compare more complicated time series models, particularly regression models | . Mean (constant) model . Predicting a variable whose values are i.i.d. . Sample mean: by definition an unbiased predictor and minimizes the mean squared forecasting error regardless of the probability distribution -&gt; it is the value around which the sum of squared deviations of the sample data is minimized . Standard error of the mean: how accurate is the estimate of the sample mean -&gt; equals the sample stdev divided by the sqrt of the sample size . Central limit theorem -&gt; large samples: 95% confidence interval = mean +- 2*stdev . Standard error of the forecast: equal to the sample stdev * sqrt(1+1/n) . Confidence interval for a forecast is the point forecast plus-or-minus the appropriate critical t-value times the standard error of the forecast . The critical t-value for a 50% confidence interval is approximately 2/3, so a 50% confidence interval is one-third the width of a 95% confidence interval. The nice thing about a 50% confidence interval is that it is a “coin flip” as to whether the true value will fall inside or outside of it, which is extremely easy to think about . If we can find some mathematical transformation (e.g., differencing, logging, deflating, etc.) that converts the original time series into a sequence of values that are i.i.d., we can use the mean model to obtain forecasts and confidence limits for the transformed series, and then reverse the transformation to obtain corresponding forecasts and confidence limits for the original series. . Linear trend model . aka trend-line model: special case of a simple regression model in which the independent variable is just a time index variable. . R-squared = 0.143 -&gt; the variance of the regression model’s errors is 14.3% less than the variance of the mean model’s errors, i.e., the model has “explained” 14.3% of the variance in the series . If the model has succeeded in extracting all the “signal” from the data, there should be no pattern at all in the errors: the error in the next period should not be correlated with any previous errors: . lag-1 autocorrelation: should be very close to zero | Durbin-Watson statistic: ought to be very close to 2 | . trend lines have their use as visual aids, but are often poor for forecasting . Random walk model . Time series with irregular growth -&gt; predict the change from one period to the next (first difference) . autocorrelation at lag k -&gt; correlation between the variable and itself lagged by k periods . random-walk-without-drift: assumes that at each point, the series merely takes a random step away from its last recorded position, with steps whose mean value is zero -&gt; values of the autocorrelations are not significantly different than zero (95% confidence interval), no change from one period to the next, because past data provides no information about the direction of future movements | random-walk-with-drift: mean step size is some nonzero value | . In the random-walk-without-drift model, the standard error of the 1-step ahead forecast is the root-mean-squared-value of the period-to-period changes . For a random-walk-with-drift, the forecast standard error is the sample standard deviation of the period-to-period changes. . “Square root of time” rule for the errors of random walk forecasts: the standard error of a k-step-ahead forecast is larger than that of the 1-step-ahead forecast by a factor of square-root-of-k. This explains the sideways-parabola shape of the confidence bands for long-term forecasts. . Random walk may look trivial -&gt; naive model (always predict that tomorrow will be the same as today). The square-root-of-time pattern in its confidence bands for long-term forecasts is of profound importance in finance (it is the basis of the theory of options pricing), and the random walk model often provides a good benchmark against which to judge the performance of more complicated models . RWM -&gt; special case of an ARIMA model -&gt; ARIMA(0, 1, 0) . Geometric random walk model . Natural logarithm transformation: linearize exponential growth and stabilize variance of changes (“diff-log”) . It can be dangerous to estimate the average rate of return to be expected in the future (let alone anticipate short-term changes in direction), by fitting straight lines to finite samples of data! . Geometric random walk model: Application of the random walk model to the logged series implies that the forecast for the next month’s value of the original series will equal the previous month’s value plus a constant percentage increase. . In unlogged units, the 95% confidence limits for long-term forecasts are noticeably asymmetric . More general random walk forecasting models . RW model 1: basic geometric random walk -&gt; assumes series in different periods are statistically independent (uncorrelated) and also identically distributed | RW model 2: assumes the series in different periods are statistically independent but not identically distributed | RW model 3: assumes that returns in different periods are uncorrelated but not otherwise independent. The ARCH (autoregressive conditional heteroscedasticity) and GARCH (generalized ARCH) models assume that the local volatility follows an autoregressive process, which is characterized by sudden jumps in volatility with a slow reversion to an average volatility | . Three types of forecasts: estimation, validation, and the future . Out-of-sample validation: withhold some of the sample data from the model identification and estimation process, then use the model to make predictions for the hold-out data to see how accurate they are and to determine whether the statistics of their errors are similar to those that the model made within the sample of data that was fitted . Overfitting (likely when): . model with a large number of parameters fitted to a small sample of data | model has been selected from a large set of potential models precisely by minimizing the MSE in the estimation period | . Backtests: one-step-ahead forecasts in the validation period (held out during parameter estimation) . If you test a great number of models and choose the model whose errors are smallest in the validation period, you may end up overfitting the data within the validation period as well as in the estimation period . Holding data out for validation purposes is probably the single most important diagnostic test of a model: it gives the best indication of the accuracy that can be expected when forecasting the future | When you’re ready to forecast the future in real time, you should of course use all the available data for estimation, so that the most recent data is used | . Forecasts into the future are “true” forecasts that are made for time periods beyond the end of the available data . The model with the tightest confidence intervals is not always the best model -&gt; a bad model not always know it is a bad model . 3. Averaging and smoothing models . Simple moving averages . mean model: best predictor of tomorrow is the avg of everything that has happened until now | random walk model: best predictor of tomorrow is what happened today, ignoring previous history | moving average: take an average of what has happened in some window of the recent past | . moving average model: superior to the mean model in adapting to cyclical pattern and superior to the random walk model in not being too sensitive to random shocks from one period to the next . Simple moving average (SMA): . Each of the past m observations gets a weight of 1/m in the averaging formula, so as m gets larger, each individual observation in the recent past receives less weight. This implies that larger values of m will filter out more of the period-to-period noise and yield smoother-looking series of forecasts | average age of the data in the forecast is (m+1)/2 -&gt; amount by which the forecasts will tend to lag behind in trying to follow trends or respond to turning points | . Value of m tradeoff: filtering out more noise vs. being too slow to respond to trends and turning points . Comparing measures of forecast error between models . RMSE: root mean squared error: (the most common standard of goodness-of-fit, penalizes big errors relatively more than small errors because it squares them first; it is approximately the standard deviation of the errors if the mean error is close to zero) | MAE: mean absolute error (the average of the absolute values of the errors, more tolerant of the occasional big error because errors are not squared) | MAPE: mean absolute percentage error (perhaps better to focus on if the data varies over a wide range due to compound growth or inflation or seasonality, in which case you may be more concerned about measuring errors in percentage terms) | ME: mean error (this indicates whether forecasts are biased high or low—should be close to 0) | MPE: mean percentage error (ditto in percentage terms) | . Best measure for size of error = RMSE . Easier for non-specialists to understand = MAE and MAPE . SMA with a trend = SMD + drift (add a constant to the SMA forecasting equation) . Tapered Moving Average: put only half as much weight on the newest and oldest values -&gt; more robust to outliers in the data . Simple exponential smoothing . SMA problems: . putting equal weight on the last m observations and no weight on any previous observations is usually not the best way to average values that are arriving consecutively in time | would make more sense to gradually decrease the weights placed on the older values | its confidence intervals for long-horizon forecasts do not widen at all | . Simple exponential smoothing (SES) aka Exponentially weighted moving average model: addresses these shortcomings of SMA . most used time series model in business applications: . good forecast under a wide range of conditions | computationally it is extremely simple | . the SES model is an interpolation between the mean model and the random walk model with respect to the way it responds to new data. As such it might be expected to do better than either of them in situations where the random walk model over-responds and the mean model under-responds, and indeed it does . Overall the SES model is superior to the SMA model in responding a bit more quickly to the newest data while treating older data more even-handedly, when the models otherwise yield the same average age . all models are based on assumptions about how the world works, and you need to understand what the assumptions are and (ideally) you should believe in the assumptions of your chosen model and be able to explain and defend them. . Linear Exponential Smoothing (LES) . Generalization of the SES to obtain a model that computes local estimates of both level and trend -&gt; same basic logic, but now you have two smoothing constants, one for the level and one for the trend . Any smoothing model will lag behind to some extent in responding to unforeseen changes in level or trend . many time series that arise in business and economics (as well as engineering and the natural sciences) which are inherently non-seasonal or which have been seasonally adjusted display a pattern of random variations around a local mean value or a local trend line that changes slowly with time. The first difference of such a series is negatively autocorrelated at lag 1: a positive change tends to be followed by a (smaller) negative one. For time series of this type, a smoothing or averaging model is the appropriate forecasting model. . Out-of-sample validation . Aka “backtesting”: holding out some of the data while estimating parameters of alternative models, then freezing those parameter estimates and using them to make predictions for the hold-out data . You hope to find the statistics of the errors of the predictions for the hold-out data look very similar to those of the predictions for the sample data . If the data exhibits exponential growth due to compounding or inflation, then it will display greater volatility in absolute terms toward the end of the series, even if the volatility is constant in percentage terms. In situations like this you may want to use a nonlinear transformation such as logging or deflating as part of your model . it is usually best to look at MAPE’s rather than RMSE’s when asking whether a given model performed about as well in the validation period as in the estimation period. . Moving average and exponential smoothing models . Moving beyond mean models, random walk models, and linear trend models, nonseasonal patterns and trends can be extrapolated using a moving-average or smoothing model. . Which type of trend-extrapolation is best: horizontal or linear? Empirical evidence suggests that, if the data have already been adjusted (if necessary) for inflation, then it may be imprudent to extrapolate short-term linear trends very far into the future. Trends evident today may slacken in the future due to varied causes such as product obsolescence, increased competition, and cyclical downturns or upturns in an industry. For this reason, simple exponential smoothing often performs better out-of-sample than might otherwise be expected, despite its “naive” horizontal trend extrapolation. Damped trend modifications of the linear exponential smoothing model are also often used in practice to introduce a note of conservatism into its trend projections. . Forecasting with adjustments for inflation and seasonality . Deflation with prices indices | Seasonal decompositon | Time series forecasting models for seasonal data Averaging and smoothing combined with seasonal adjustment | Winters seasonal exponential smoothing model | . | . Modeling the effect of inflation . Why? . to measure real growth and estimate its dependence on other real factors | to remove much of the trend and stabilize variance before fitting the model | . How? . to “deflate” a variable, you divide it by an appropriate price index variable | e.g.: general price index, product-specific index | to “re-inflate” forecasts of a deflated series, you multiply the forecasts and confidence limits by a forecast of the price index | . Log vs. deflate . Deflation should be used when you are interested in knowing the forecast in “real” terms and/or if the inflation rate is expected to change | Logging is sufficient if you just want a forecast in “nominal” terms and inflation is expected to remain constant—inflation just gets lumped with other sources of compound growth in the model. | Logging also ensures that forecasts and confidence limits have positive values, even in the presence of downward trends and/or high volatility. | If inflation has been minimal and/or there is little overall trend or change in volatility, neither may be necessary | . Seasonality . repeating, preiodic pattern in the data that is keyed to the calendar of the clock | != than “cyclality”, which do not have a predictable periodicity | . Seasonal patterns are complex, because the calendar is not rational: months and years don’t have whole numbers of weeks, a given month does not always have the same number of trading days/weekends, Christmans day can fall on any day of the week, some major holidays are “moveable feasts” that do not occur on the same calendar dates each year . Quarterly data is easiest to handle: 4 quarters in a year, 3 months in a quarter, trading day adjustments have only minor effects. | Monthly data is more complicated: 12 months in a year, but not 4 weeks in a month; trading day adjustments may be important. | Weekly data requires special handling because a year is not exactly 52 weeks. | . Multiplicative seasonality . Most natural seasonal patterns are multiplicative . Seasonal variations are roughly constant in percentage terms | Seasonal swings get larger or smaller in absolute magnitude as the average level of the series rises or falls due to long-term trends and/or business cycle effects | . Additive seasonality . Additive seasonal pattern has constant-amplitude seasonal swings in the presence of trends and cycles. . A log transformation converts a multiplicative pattern to an additive one, so if your model includes a log transformation, use additive rather than multiplicative seasonal adjustment. | If the historical data sample has little trend and seasonal variations are not large in relative terms, additive and multiplicative adjustment yield very similar results | . Seasonal index . Represents the expected percentage of “normal” in a given month or quarter | When the seasonal indices are assumed to be stable over time, they can be estimated by the “ratio to moving average” (RMA) method | . Winters’ Seasonal Smoothing . The logic of Holt’s LES model can be extended to recursively estimate time-varying seasonal indices as well as level and trend. . Issues: . Estimation of Winters’ model is tricky | There are three separate smoothing constants to be jointly estimated by nonlinear least squares. | Initialization is also tricky, especially for the seasonal indices. | Confidence intervals sometimes come out extremely wide because the model “lacks confidence in itself.” | . In practice: . The Winters model is popular in “automatic forecasting” software, because it has a little of everything (level, trend, seasonality). | Often it works very well, but difficulties in initialization &amp; estimation can lead to strange results in some cases. | It responds to recent changes in the seasonal pattern as well as the trend, but with some danger of unstable long-term trend projections. | . 4. Linear regression models . Introduction to linear regression . Regression analysis: art and science of fitting straight lines to patterns of data . Assumptions: . The expected value of Y is a linear function of the X variables | The unexplained variations of Y are independent random variables (in particular, not “autocorrelated” if the variables are time series) | The all have the same variance (“homoscedasticity”) | They are normally distributed | No model is perfect - these assumptions will never be exactly satisfied by real-world messy data - but you hope that they are not badly wrong. The art of regression modeling is to (most importantly!) collect data that is relevant and informative with respect to your decision or inference problem, and then define your variables and construct your model in such a way that the assumptions listed above are plausible, at least as a first-order approximation to what is really happening . Correlation and regression-to-mediocrity . Regression-to-mediocrity aka regression to the mean: Purely statistical phenomenon that can be viewed as a form of selection bias. Every quantitative measurement is a combination of signal and noise. When a value above the mean is observed, it is probable that the value of the signal was above average and the value of the noise was above average. Now suppose there is some other quantity (say, some measurable trait of the offspring—not necessarily the same one) whose value depends only on the signal, not the noise, in the first quantity. Then a measurement of the second quantity should also be expected to be above the mean, but less so in relative terms, because only the above-average signal is passed on; the above-average noise is not. In fact, it is the independent noise in the second quantity that prevents variations from ever dying out over generations. . The coefficient of correlation between X and Y is the average product of their standardized values . It is a number that lies somewhere between -1 and +1, where -1 indicates a perfect negative linear relationship, +1 indicates a perfect positive linear relationship, and zero indicates no linear relationship. . A correlation of zero between X and Y does not necessarily mean that there is no relationship, just that there is no linear relationship within the historical sample of data that is being analyzed. e.g., y = xˆ2 . When we speak of “regressing” one variable on a group of others, we mean the fitting of a linear equation that minimizes the sum of squared errors in predicting that variable from the others. . Mathematics of a regression model . The coefficients and error measures for a regression model are entirely determined by the following summary statistics: means, standard deviations, and correlations of the variables, and the sample size | The correlation between Y and X is equal to the average product of their standardized values | The slope coefficient in a simple regression of Y on X is the correlation between Y and X multiplied by the ratio of their standard deviations | In a simple regression model, the percentage of variance “explained” by the model, which is called R-squared, is the square of the correlation between Y and X | The sample standard deviation of the errors is a downward-biased estimate of the size of the true unexplained deviations in Y because it does not adjust for the additional “degree of freedom” used up by estimating the slope coefficient | Adjusted R-squared, which is obtained by adjusting R-squared for the degrees if freedom for error in exactly the same way, is an unbiased estimate of the amount of variance explained | For models fitted to the same sample of the same dependent variable, adjusted R-squared always goes up when the standard error of the regression goes down. A model does not always improve when more variables are added: adjusted R-squared can go down (even go negative) if irrelevant variables are added | The standard error of a coefficient estimate is the estimated standard deviation of the error in measuring it. And the estimated height of the regression line for a given value of X has its own standard error, which is called the standard error of the mean at X. All of these standard errors are proportional to the standard error of the regression divided by the square root of the sample size | The standard error of the forecast for Y for a given value of X is the square root of the sum of squares of the standard error of the regression and the standard error of the mean at X | Two-sided confidence limits for coefficient estimates, means, and forecasts are all equal to their point estimates plus-or-minus the appropriate critical t-value times their respective standard errors. | What to look for in regression model output . Standard error of the regression (root-mean-squared error adjusted for degrees of freedom) . It is a lower bound on the standard error of any forecast generated from the model . In time series forecasting, also common to look the mean absolute error (MAE) and, for positive data, the mean absolute percentage error (MAPE) . Mean absolute scaled error -&gt; measures improvement in mean absolute error relative to a random-walk-without-drift model . Adjusted R-squared . R-squared (the fraction by which the variance of the errors is less than the variance of the dependent variable) adjusted for the number of coefficients in the model relative to the sample size in order to correct it for bias. . Adjusted R-squared is the fraction by which the square of the standard error of the regression is less than the variance of the dependent variable . Unitless statistic, but there is no absolute standard for what is a “good” value . Significance of the estimated coefficients . Are the t-statistics greater than 2 in magnitude, corresponding to p-values less than 0.05? If they are not, you should probably try to refit the model with the least significant variable excluded, which is the “backward stepwise” approach to model refinement. . Values of the estimated coefficients . In general you are interested not only in the statistical significance of an independent variable, you are also interested in its practical significance. In theory, the coefficient of a given independent variable is its proportional effect on the average value of the dependent variable, others things being equal -&gt; “bang for the buck”. . Plots of forecasts and residuals . DO NOT FAIL TO LOOK AT PLOTS OF THE FORECASTS AND ERRORS. Do the forecasts “track” the data in a satisfactory way, apart from the inevitable regression-to-the mean? (In the case of time series data, you are especially concerned with how the model fits the data at the “business end”, i.e., the most recent values.) Do the residuals appear random, or do you see some systematic patterns in their signs or magnitudes? Are they free from trends, autocorrelation, and heteroscedasticity? Are they normally distributed? There are a variety of statistical tests for these sorts of problems, but the best way to determine whether they are present and whether they are serious is to look at the pictures. . Out-of-sample validation . A good model should have small error measures in both the estimation and validation periods, compared to other models, and its validation period statistics should be similar to its own estimation period statistics. Regression models with many independent variables are especially susceptible to overfitting the data in the estimation period, so watch out for models that have suspiciously low error measures in the estimation period and disappointingly high error measures in the validation period. . Be aware that if you test a large number of models and rigorously rank them on the basis of their validation period statistics, you may end up with just as much “data snooping bias” as if you had only looked at estimation-period statistics–i.e., you may end up picking a model that is more lucky than good! The best defense against this is to choose the simplest and most intuitively plausible model that gives comparatively good results. . What’s the bottom line? How to compare models . After fitting a number of different regression or time series forecasting models to a given data set, you have many criteria by which they can be compared: . Error measures in the estimation period: root mean squared error, mean absolute error, mean absolute percentage error, mean absolute scaled error, mean error, mean percentage error | Error measures in the validation period (if you have done out-of-sample testing): Ditto | Residual diagnostics and goodness-of-fit tests: plots of actual and predicted values; plots of residuals versus time, versus predicted values, and versus other variables; residual autocorrelation plots, cross-correlation plots, and tests for normally distributed errors; measures of extreme or influential observations; tests for excessive runs, changes in mean, or changes in variance (lots of things that can be “OK” or “not OK”) | Qualitative considerations: intuitive reasonableness of the model, simplicity of the model, and above all, usefulness for decision making! | . The bottom line is that you should put the most weight on the error measures in the estimation period–most often the RMSE, but sometimes MAE or MAPE–when comparing among models. . The MASE statistic provides a very useful reality check for a model fitted to time series data: is it any better than a naive model? . You may also want to look at Cp, AIC or BIC, which more heavily penalize model complexity. But you should keep an eye on the residual diagnostic tests, cross-validation tests (if available), and qualitative considerations such as the intuitive reasonableness and simplicity of your model. . K.I.S.S. (keep it simple…): If two models are generally similar in terms of their error statistics and other diagnostics, you should prefer the one that is simpler and/or easier to understand. . 5. ARIMA models for time series forecasting . Auto-Regressive Integrated Moving Average . What ARIMA stands for . Series which needs to be differenced to be made stationary = an “integrated” (I) series | Lags of the stationarized series are called “auto-regressive” (AR) terms | Lags of the forecast errors are called “moving average” (MA) terms | . ARIMA models put it all together . Generalized random walk models fine-tuned to eliminate all residual autocorrelation | Generalized exponential smoothing models that can incorporate long-term trends and seasonality | Stationarized regression models that use lags of the dependent variables and/or lags of the forecast errors as regressors | The most general class of forecasting models for time series that can be stationarized* by transformations such as differencing, logging, and or deflating | . Construction of an ARIMA model . Stationarize the series, if necessary, by differencing (&amp; perhaps also logging, deflating, etc.) | Study the pattern of autocorrelations and partial autocorrelations to determine if lags of the stationarized series and/or lags of the forecast errors should be included in the forecasting equation | Fit the model that is suggested and check its residual diagnostics, particularly the residual ACF and PACF plots, to see if all coefficients are significant and all of the pattern has been explained. | Patterns that remain in the ACF and PACF may suggest the need for additional AR or MA terms | ARIMA terminology . ARIMA(p,d,q) -&gt; non-seasonal ARIMA: . p = number of autoregressive terms . q = number of non-seasonal differences . d = number of moving-average terms . Do you need both AR and MA terms? . in general, you don’t | If the stationarized series has positive autocorrelation at lag 1, AR terms often work best. If it has negative autocorrelation at lag 1, MA terms often work best. | An MA(1) term often works well to fine-tune the effect of a nonseasonal difference, while an AR(1) term often works well to compensate for the lack of a nonseasonal difference, so the choice between them may depend on whether a difference has been used. | . Interpretation of AR terms . Autoregressive (AR) behavior: apparently feels a “restoring force” that tends to pull it back toward its mean . Interpretation of MA terms . Moving average (MA) behavior: apparently undergoes random “shocks” whose effects are felt in two or more consecutive periods . Tools for identifying ARIMA models: ACF and PACF plots . Autocorrelation function (ACF) plot: shows the correlation of the series with itself at different lags | Partial autocorrelation function (PACF) plot: shows the amount of autocorrelation at lag k that is not explained by lower-order autocorrelations | . AR and MA “signatures” . ACF that dies out gradually and PACF that cuts off sharply after a few lags -&gt; AR signature AR series is usually positive autocorrelated at lag 1 | . | ACF that cuts off sharply after a few lags and PACF that dies out more gradually -&gt; MA signature MA series is usually negatively autocorrelated at lag 1 | . | . Whether a series displays AR or MA behavior often depends on the extent to which it has been differenced. “Underdifferenced” series -&gt; AR signature (positive autocorrelation). After one or more orders of differencing, the autocorrelation will become more negative and an MA signature will emerge . Model-fitting steps . Determine the order of differencing | Determine the numbers of AR &amp; MA terms | Fit the model—check to see if residuals are “white noise,” highest-order coefficients are significant (w/ no “unit “roots”), and forecasts look reasonable. If not, return to step 1 or 2. | Technical issues . Backforecasting: Estimation algorithm begins by forecasting backward into the past to get start-up values | Unit roots: Look at sum of AR coefficients and sum of MA coefficients—if they are too close to 1 you may want to consider higher or lower of differencing | Overdifferencing: A series that has been differenced one too many times will show very strong negative autocorrelation and a strong MA signature, probably with a unit root in MA coefficients | . Seasonal ARIMA models . Rely on seasonal lags and differences to fit the seasonal pattern. Generalizes the regression approach. . Terminology . Seasonal part of an ARIMA model is summarized by three additional numbers: . P = # of seasonal autoregressive terms | D = # of seasonal differences | Q = # of seasonal moving average terms | . “ARIMA(p,d,q)x(P,D,Q)” model . P, D and Q should never be larger than 1 . Model fitting steps . Start by trying various combinations of one seasonal difference and/or one non-seasonal difference to stationarize the series and remove gross features of seasonal pattern. | If the seasonal pattern is strong and stable, you MUST use a seasonal difference (otherwise it will “die out” in long-term forecasts) | After differencing, inspect the ACF and PACF at multiples of the seasonal period (s): Positive spikes in ACF at lag s, 2s, 3s…, single positive spike in PACF at lag s -&gt; SAR=1 | Negative spike in ACF at lag s, negative spikes in PACF at lags s, 2s, 3s,… -&gt; SMA=1 | SMA=1 often works well in conjunction with a seasonal difference. | . | . Same principles as for non-seasonal models, except focused on what happens at multiples of lag s in ACF and PACF. . Bottom-line suggestion . Strong seasonal pattern, try: . ARIMA(0,1,q)x(0,1,1) model (q=1 or 2) | ARIMA(p,0,0)x(0,1,1)+c model (p=1, 2 or 3) | If there is a significant trend and/or the seasonal pattern is multiplicative, you should also try a natural log transformation. | . Take-aways . Advantages: solid underlying theory, stable estimation of time-varying trends and seasonal patterns, relatively few parameters. | Drawbacks: no explicit seasonal indices, hard to interpret coefficients or explain “how the model works”, danger of overfitting or mis-identification if not used with care. | . Seasonal differencing in ARIMA models . Seasonal difference: series of changes from one season to the next. i.e. 12 periods in a season -&gt; seasonal difference = Y(t)-Y(t-12) . Usually removes the gross features of seasonality from a series, as well as most of the trend. . First difference of the seasonal difference = (Y(t)-Y(t-12))-(Y(t-1)-Y(t-13)) . Summary of rules for identifying ARIMA models . Direct Source for the text below . Identifying the order of differencing and the constant: . Rule 1: If the series has positive autocorrelations out to a high number of lags (say, 10 or more), then it probably needs a higher order of differencing. | Rule 2: If the lag-1 autocorrelation is zero or negative, or the autocorrelations are all small and patternless, then the series does not need a higher order of differencing. If the lag-1 autocorrelation is -0.5 or more negative, the series may be overdifferenced. BEWARE OF OVERDIFFERENCING. | Rule 3: The optimal order of differencing is often the order of differencing at which the standard deviation is lowest. (Not always, though. Slightly too much or slightly too little differencing can also be corrected with AR or MA terms. See rules 6 and 7.) | Rule 4: A model with no orders of differencing assumes that the original series is stationary (among other things, mean-reverting). A model with one order of differencing assumes that the original series has a constant average trend (e.g. a random walk or SES-type model, with or without growth). A model with two orders of total differencing assumes that the original series has a time-varying trend (e.g. a random trend or LES-type model). | Rule 5: A model with no orders of differencing normally includes a constant term (which allows for a non-zero mean value). A model with two orders of total differencing normally does not include a constant term. In a model with one order of total differencing, a constant term should be included if the series has a non-zero average trend. | . Identifying the numbers of AR and MA terms: . Rule 6: If the partial autocorrelation function (PACF) of the differenced series displays a sharp cutoff and/or the lag-1 autocorrelation is positive–i.e., if the series appears slightly “underdifferenced”–then consider adding one or more AR terms to the model. The lag beyond which the PACF cuts off is the indicated number of AR terms. | Rule 7: If the autocorrelation function (ACF) of the differenced series displays a sharp cutoff and/or the lag-1 autocorrelation is negative–i.e., if the series appears slightly “overdifferenced”–then consider adding an MA term to the model. The lag beyond which the ACF cuts off is the indicated number of MA terms. | Rule 8: It is possible for an AR term and an MA term to cancel each other’s effects, so if a mixed AR-MA model seems to fit the data, also try a model with one fewer AR term and one fewer MA term–particularly if the parameter estimates in the original model require more than 10 iterations to converge. BEWARE OF USING MULTIPLE AR TERMS AND MULTIPLE MA TERMS IN THE SAME MODEL. | Rule 9: If there is a unit root in the AR part of the model–i.e., if the sum of the AR coefficients is almost exactly 1–you should reduce the number of AR terms by one and increase the order of differencing by one. | Rule 10: If there is a unit root in the MA part of the model–i.e., if the sum of the MA coefficients is almost exactly 1–you should reduce the number of MA terms by one and reduce the order of differencing by one. | Rule 11: If the long-term forecasts* appear erratic or unstable, there may be a unit root in the AR or MA coefficients. | . Identifying the seasonal part of the model: . Rule 12: If the series has a strong and consistent seasonal pattern, then you must use an order of seasonal differencing (otherwise the model assumes that the seasonal pattern will fade away over time). However, never use more than one order of seasonal differencing or more than 2 orders of total differencing (seasonal+nonseasonal). | Rule 13: If the autocorrelation of the appropriately differenced series is positive at lag s, where s is the number of periods in a season, then consider adding an SAR term to the model. If the autocorrelation of the differenced series is negative at lag s, consider adding an SMA term to the model. The latter situation is likely to occur if a seasonal difference has been used, which should be done if the data has a stable and logical seasonal pattern. The former is likely to occur if a seasonal difference has not been used, which would only be appropriate if the seasonal pattern is not stable over time. You should try to avoid using more than one or two seasonal parameters (SAR+SMA) in the same model, as this is likely to lead to overfitting of the data and/or problems in estimation. | . A caveat about long-term forecasting in general: linear time series models such as ARIMA and exponential smoothing models predict the more distant future by making a series of one-period-ahead forecasts and plugging them in for unknown future values as they look farther ahead. However, the models are identified and optimized based on their one-period-ahead forecasting performance, and rigid extrapolation of them may not be the best way to forecast many periods ahead (say, more than one year when working with monthly or quarterly business data), particularly when the modeling assumptions are at best only approximately satisfied (which is nearly always the case). If one of your objectives is to generate long-term forecasts, it would be good to also draw on other sources of information during the model selection process and/or to optimize the parameter estimates for multi-period forecasting if your software allows it and/or use an auxiliary model (possibly one that incorporates expert opinion) for long-term forecasting. . 6. Choosing the right forecasting model . Steps in choosing a forecasting model . Deflation? . If the series show inflationary growth. Help to account for the growth pattern and reduce heteroscedasticity in the residuals. . Logarithm transformation? . If the series shows compound growth and/or a multiplicative seasonal pattern, a logarithm transformation may be helpful in addition to or lieu of deflation. Logging the data will not flatten an inflationary growth pattern, but it will straighten it out it so that it can be fitted by a linear model (e.g., a random walk or ARIMA model with constant growth, or a linear exponential smoothing model). . Convert multiplicative seasonal patterns to additive patterns, so that if you perform seasonal adjustment after logging, you should use the additive type. . Another important use for the log transformation is linearizing relationships among variables in a regression model . Seasonal adjustment? . If the series has a strong seasonal pattern which is believed to be constant from year to year, seasonal adjustment may be an appropriate way to estimate and extrapolate the pattern. . The advantage of seasonal adjustment is that it models the seasonal pattern explicitly, giving you the option of studying the seasonal indices and the seasonally adjusted data. . The disadvantage is that it requires the estimation of a large number of additional parameters (particularly for monthly data), and it provides no theoretical rationale for the calculation of “correct” confidence intervals . Independent variables? . If there are other time series which you believe to have explanatory power with respect to your series of interest (e.g., leading economic indicators or policy variables such as price, advertising, promotions, etc.) . Smoothing, averaging, or random walk? . If you have chosen to seasonally adjust the data–or if the data are not seasonal to begin with–then you may wish to use an averaging or smoothing model to fit the nonseasonal pattern which remains in the data at this point . If smoothing or averaging does not seem to be helpful–i.e., if the best predictor of the next value of the time series is simply its previous value–then a random walk model is indicated. . Brown’s linear exponential smoothing can be used to fit a series with slowly time-varying linear trends, but be cautious about extrapolating such trends very far into the future. . Holt’s linear smoothing also estimates time-varying trends, but uses separate parameters for smoothing the level and trend, which usually provides a better fit to the data than Brown’s model. . Winters Seasonal Exponential Smoothing? . Extension of exponential smoothing that simultaneously estimates time-varying level, trend, and seasonal factors using recursive equations. (Thus, if you use this model, you would not first seasonally adjust the data.) . The Winters seasonal factors can be either multiplicative or additive: normally you should choose the multiplicative option unless you have logged the data. Although the Winters model is clever and reasonably intuitive, it can be tricky to apply in practice: it has three smoothing parameters–alpha, beta, and gamma–for separately smoothing the level, trend, and seasonal factors, which must be estimated simultaneously. . ARIMA? . If you do not choose seasonal adjustment (or if the data are non-seasonal), you may wish to use the ARIMA model framework. . ARIMA models are a very general class of models that includes random walk, random trend, exponential smoothing, and autoregressive models as special cases. . The conventional wisdom is that a series is a good candidate for an ARIMA model if: . (i) it can be stationarized by a combination of differencing and other mathematical transformations such as logging, and | (ii) you have a substantial amount of data to work with: at least 4 full seasons in the case of seasonal data. (If the series cannot be adequately stationarized by differencing–e.g., if it is very irregular or seems to be qualitatively changing its behavior over time–or if you have fewer than 4 seasons of data, then you might be better off with a model that uses seasonal adjustment and some kind of simple averaging or smoothing.) | . Steps . Determine the appropriate order of differencing needed to stationarize the series and remove the gross features of seasonality | Determine whether to include a constant term in the model: usually you do include a constant term if the total order of differencing is 1 or less, otherwise you don’t | Choose the numbers of autoregressive and moving average parameters (p, d, q, P, D, Q) that are needed to eliminate any autocorrelation that remains in the residuals of the naive model (i.e., any correlation that remains after mere differencing). | It is usually better to proceed in a forward stepwise rather than backward stepwise fashion when tweaking the model specifications: start with simpler models and only add more terms if there is a clear need. . Forecasting Flow Chart . Source: https://people.duke.edu/~rnau/411flow.gif . . Automatic Forecasting Software . WARNING: Properly used, and guided by experience with “manual” model-fitting, the best of such software can put more data-analysis power at your fingertips and speed up routine forecasting applications. Carelessly used, it merely allows you to foul things up in a bigger way, obtaining results without insight while getting a false sense of security that “the computer knows best.” Automatic forecasting software is a complement to, not a substitute for, your own forecasting expertise. . How to avoid trouble: principles of good data analysis . Source: https://people.duke.edu/~rnau/notroubl.htm . Do some background research before running any numbers. Be sure you understand the objective, the theory, the jargon, and the conventional wisdom. Ask others what they know about the problem. Do more of the same web-searching that brought you to this site, and be critical of everything you find. | Be thorough in your search for data. In the end, your forecasts will contain no information that is not hidden in it somewhere. In some cases (say, designed experiments) the data may be limited in scope and already in hand, but in many cases it will be up to you to identify and collect it, and this may the most important and time-consuming part of the project. We now live in the era of “megadata”, but often the data that you need is not so easy to find. | Check the data carefully once you have it: make sure you know where the numbers came from, what they mean, how they were measured, how they are aligned in time, and whether they are accurate. A lot of cleaning and merging of data may be needed before any analysis can begin, and you should know how to do it. You may find that you need yet-more-data or better-quality data to answer the key questions, and sometimes the most useful lesson is that the organization will need to do a better job of managing its data in the future. | Use descriptive, self-explanatory names for your variables (not Y’s and X’s or cryptological character strings) so that their meaning and units are clear and so that your computer output is self-documenting to the greatest extent possible. | Once you begin your analysis, follow good modeling practices: perform exploratory data analysis, use appropriate model types, interpret their estimated parameters, check their residual diagnostics, question their assumptions, and validate them with out-of-sample testing where possible. | Make effective use of statistical graphics in your own analysis and in your presentation of your results to others, and follow rules of good graphics. Among other things: graphs should be self-explanatory and not contain visual puzzles, they should have titles that are specific to the variables and models, axes should be well-scaled and well-labeled, the data area should have a white background, grid lines (if any are needed) should not be too dark, point and line widths should be sized appropriately for the density of the data, self-promoting artwork (3-D perspective, etc.) should be avoided, and above all, the data should tell its own story. | Keep in mind that not all relationships are linear and additive, not all randomness is normally distributed, and regression models are not magic boxes that can predict anything from anything. Be aware that it may be necessary to transform some of your variables (through deflating, logging, differencing, etc.) in order to match their patterns up with each other in the way that linear models require. | When comparing models, focus on the right objectives, which are usually making the smallest possible errors in the future and deriving inferences that are genuinely useful for decision making. A good fit to past data does not always guarantee an equally good prediction of what will happen next, and statistical significance is not always the same as practical significance. | Other things being equal, KEEP IT SIMPLE and intuitively reasonable. If others don’t understand the model, they may not use it, and perhaps they shouldn’t: simple models often outperform complicated models in practice. | If you use automatic forecasting software, you are still responsible for the model that is chosen, and you should be able to explain its logic to others. The availability of such software does not make it unnecessary to know how the models work. Rather, it makes that knowledge even more important. Be aware that in automatic rankings of models, there may be only tiny differences in error stats between the winner and its near competitors, and you may need to take other factors into account in your final selection, such as simplicity, clarity, and intuition. And again, your software cannot make something out of nothing. If your data is not informative or not properly organized to begin with, automatic methods will not turn it into gold. | Leave a paper trail, i.e., keep well-annotated records of your model-fitting efforts. Don’t just save your computer files: write up notes as you go along. Someone else (who may be a sharp-penciled auditor or perhaps only yourself 12 months hence) may need to reconstruct what you did and why you did it. Intelligent naming of variables and labeling of tables and charts will make this easier. | Neither overstate nor understate the accuracy of your forecast, and do not merely give a point value. Always report standard errors and/or confidence intervals. | If different forecasting approaches lead to different results, focus on differences in their underlying assumptions, their sources of data, their intrinsic biases, and their respective margins for error. Don’t just argue over the outputs. Look for opportunities to combine independent viewpoints and information. | After all has been said, if you believe your model is more useful and better supported than the alternatives, stand up for it. The future may still be very uncertain, but decisions ought to be based on the best understanding of that uncertainty. If you don’t have confidence in your model (or anyone else’s), admit it, and keep digging. | .",
            "url": "https://millengustavo.github.io/blog/book/time%20series/forecasting/data%20science/machine%20learning/statistics/2020/02/03/statistical-forecasting.html",
            "relUrl": "/book/time%20series/forecasting/data%20science/machine%20learning/statistics/2020/02/03/statistical-forecasting.html",
            "date": " • Feb 3, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Introduction to Machine Learning with Python: A Guide for Data Scientists",
            "content": "My notes and highlights on the book. . Authors: Andreas C. Müller and Sarah Guido . 1. Introduction . Why ML? . Using handcoded rules to make decisions has two disadvantages: . logic is specific to a domain and task. Change the task slightly -&gt; rewrite the whole system | designing rules requires a deep understanding of how a decision should be made by a human expert | . Knowing your task and knowing your data . When building a ML solution: . What question(s) am I trying to answer? Do I think the data collected can answer that question? | What is the best way to phrase my question(s) as a machine learning problem? | Have I collected enough data to represent the problem I want to solve? | What features of the data did I extract, and will these enable the right predictions? | How will I measure success in my application? | How will the machine learning solution interact with other parts of my research or business product? | . Many spend a lot of time building complex ML solutions, only to find out they don’t solve the right problem. When going deep into the technical aspects of ML, it is easy to lose sight of the ultimate goals . Jupyter notebook . Interactive environment for running code in the browser . NumPy . ndarray: multidimensional (n) array with elements of the same type | high-level mathematical functions, such as linear algebra, Fourier transform, pseudorandom number generators | . SciPy . scipy.sparse: provides sparse matrices | advanced linear algebra routines, mathematical function optimization, signal processing, statistical distributions | . Matplotlib . On jupyter: %matplotlib inline | Primary scientific plotting library in Python | . Pandas . Library for data wrangling and analysis . DataFrame: allows each column to have a separate type | . mglearn . Library of utility functions wrote for this specific book. Avoid boilerplate with plotting and loading data . First things first: look at your data . Before building a ML model, inspect the data: . task easily solvable without ML | desired information may not be contained in the data | find abnormalities and peculiarities | real world: inconsistencies in the data and unexpected measurements are very common | scatter plot: pd.scatter_matrix | . 2. Supervised Learning . 3. Unsupervised Learning and Preprocessing . 4. Representing Data and Engineering Features . Feature engineering: how to represent your data best for a particular application -&gt; can have a bigger influence on the performance of a model than the exact parameters you choose . Categorical Variables . One-Hot Encoding (Dummy Variables) . Replace a categorical variable with one or more features that can have the values 0 and 1 -&gt; introduce a new feature per category . In pandas pd.get_dummies(data) automatically transform all columns that have object type or are categorical . Numbers Can Encode Categoricals . The get_dummies function in pandas treats all numbers as continuous and will not create dummy variables for them. To get around this, use scikit-learn’s OneHotEncoder . Binning, Discretization, Linear Models and Trees . One way to make linear models more powerful on continuous data is to use binning (aka discretization) of the feature to split it up into multiple features . Binning features generally has no beneficial effect for tree-based models, as these models can learn to split up the data anywhere . Interactions and Polynomials . Enrich a feature representation, particularly for linear models, is adding interaction features and polynomial features of the original data . Univariate Nonlinear Transformations . Applying mathematical functions like: . log, exp: help adjusting the relative scales in the data | sin, cos: dealing with data that encodes periodic patterns | . Most models work best when each feature (and in regression also the target) is loosely Gaussian distributed -&gt; histogram should have something resembling the familiar “bell curve” shape. Using log or exp is a hacky but simple and efficient way to achieve this -&gt; helpful when dealing with integer count data . These kind of transformations are irrelevant for tree-based models, but might be essential for linear models. Sometimes it’s also a good idea to transform the target variable in regression . Automatic Feature Selection . Adding more features makes all models more complex, and so increases the chance of overfitting . It can be good idea to reduce the number of features to only the most useful ones, and discard the rest . Univariate Statistics . Compute whether there is a statistically significant relationship between each feature and the target -&gt; the features with highest confidence are selected. Also know as analysis of variance (ANOVA) for classification . Only consider each feature individually. f_classify or f_regression tests in scikit-learn and then SelectKBest or SelectPercentile . Model-Based Feature Selection . Uses a supervised ML model to judge the importance of each feature, and keeps only the most important ones . Decision tree-based models: feature_importances_ attribute | Linear models: coefficients can capture feature importances | . Model-based considers all features at once, so can capture interactions. SelectFromModel . from sklearn.feature_selection import SelectFromModel from sklearn.ensemble import RandomForestClassifier select = SelectFromModel( RandomForestClassifier(n_estimators=100, random_state=42), threshold=&quot;median&quot;) . Iterative Feature Selection . A series of models are built, with varying numbers of features. Two methods: . starting with no features and adding one by one | starting with all features and removing one by one | . More computationally expensive . recursive feature elimination (RFE): starts with all features, builds a model, and discards the least important feature according to the model -&gt; repeat . Feature Selection: Can speed up prediction, allow for more interpretable model. In most real-world cases, is unlikely to provide large gains in performance . Utilizing Expert Knowledge . Prior knowledge about the nature of the task can be encoded in the features to aid a ML algorithm . Adding a feature does not force a machine learning algorithm to use it, and even if the holiday information turns out to be noninformative for flight prices, augmenting the data with this information doesn’t hurt. . COOL example with bike rental on the book -&gt; check to see the coefficients learned by the linear model | . 5. Model Evaluation and Improvement . Cross-Validation . Data is split repeatedly and multiple models are trained. Most common: k-fold cross-validation . Scikit-learn: cross_val_score from the model_seleciton module . High variance in the metric (e.g., accuracy) between folds -&gt; model is very dependent on the particular folds for train, or it could also be consequence of the small size of the dataset . Benefits of Cross-Validation . avoid “lucky”/”unlucky” random train_test_split | in cross-validation each example will be in the training set exactly once: each example is in one of the folds, and each fold is the test set once -&gt; the model needs to generalize well to all of the samples in the dataset for all of the cross-validation scores to be high | multiple splits provides information about how sensitive the model is to the selection of the training set -&gt; idea of the best/worst case scenarios | use data more effectively: more data usually leads to more accurate models | . Disadvantage: increased computational cost -&gt; train k models instead of one . Cross-validation does not return a model, its purpose is only to evaluate how well a given algorithm will generalize when trained on a specific dataset . Stratified k-Fold Cross-Validation . Stratified k-fold cross-validation: split the data such that the proportions between classes are the same in each fold as they are in the whole dataset . Results in more reliable estimates of generalization performance . For regression scikit-learn uses the standard k-fold cross-validation by default . Leave-one-out cross-validation . k-fold cross-validation where each fold is a single sample. LeaveOneOut on sklearn.model_selection . Can be very time consuming for large datasets, but sometimes provides better estimates on small datasets . Shuffle-split cross-validation . Each split samples train_size many points for the training set and test_size many (disjoint) points for the test set. This splitting is repeated n_iter times . allows for control over the number of iterations independently of the training and test sizes | allows for using only part of the data in each iteration by providing train_size and test_size that don’t add up to one -&gt; subsampling like that can be useful for experimenting with large datasets | . ShuffleSplit and StratifiedShuffleSplit on sklearn . Cross-validation with groups . When there are groups in the data that are highly related . GroupKFold: takes an array of groups as arguments -&gt; indicates groups in the data that should not be split when creating the training and test sets, and should not be confused with the class label . GroupKFold: important for medical applications (multiple samples for same patient), also speech recognition . Grid Search . Trying all possible combinations of the parameters of interest . The danger of overfitting the parameters and the validation set . To avoid this split the data in three sets: the training set to build the model, the validation (or development) set to select the parameters of the model, and the test set to evaluate the performance of the selected parameters . After selecting the best parameters using the validation set, rebuild the model using the parameter settings found, but now training on both the training data and the validation data . Important to keep the distinction of training, validation and test sets clear! Evaluating more than one model on the test set and choosing the better of the two will result in an overly optimistic estimate of how accurate the model is -&gt; “Leak” information from the test set into the model . Grid Search with Cross-Validation . Beautiful plot from mglearn: . mglearn.plots.plot_cross_val_selection() . GridSearchCV on sklearn: implemented in the form of an estimator -&gt; not only searchs for the best parameters, but also automatically fits a new model on the whole training dataset with the parameters that yielded the best CV performance . best_score_ != score: first stores the mean CV accuracy performed in the training set, second evaluate the output of the predict method of the model trained on the whole training set! . Analyzing the result of cross-validation . # cool example of a heatmap using SVM mglearn.tools.heatmap(scores, xlabel=&#39;gamma&#39;, xticklabels=param_grid[&#39;gamma&#39;], ylabel=&#39;C&#39;, yticklabels=param_grid[&#39;C&#39;], cmap=&quot;viridis&quot;) . Optimum values for each parameter on the edges of the plot: parameters not large enough! . Nested cross-validation . An outer loop over splits of the data into training and test sets. For each of them, a grid search is run (which might result in different best parameters for each split in the outer loop). Then, for each outer split, the test set score using the best settings is reported . Rarely used in practice | Useful for evaluating how well a given model works on a particular dataset | Computationally expensive procedure | . # example of nested CV scores = cross_val_score(GridSearchCV(SVC(), param_grid, cv=5), iris.data, iris.target, cv=5) . Evaluation Metrics and Scoring . Keep the end goal in mind . Business metric: We are interested in using the predictions as part of a larger decision-making process, you should think about the high-level goal of the application | Business impact: consequences of choosing a particular algorithm for a ML application | . When choosing a model or adjusting parameters, you should pick the model or parameter values that have the most positive influence on the business metric . Sometimes infeasible to put models in production just for testing purposes (high business risk): find some surrogate evaluation procedure (as close as possible to the business goal), using an evaluation metric that is easier to compute . Metrics for binary classification . Kinds of errors . false positive: incorrect positive prediction, type I error | false negative: incorrect negative prediction, type II error | . Imbalanced datasets . Accuracy is an inadequate measure for quantifying predictive performance in most imbalanced settings . pred_most_frequent: model that make predictions to the most frequent class | pred_dummy: random predictions | . Confusion matrices . confusion_matrix . rows: true classes | columns: predicted classes | . negative class | TN | FP | . positive class | FN | TP | . - | predicted negative | predicted positive | . Accuracy = (TP+TN)/(TP+TN+FP+FN) . | Precision = (TP)/(TP+FP) . | . Precision is used when the goal is to limit the number of false positives. AKA positive predictive value (PPV) . Recall = (TP)/(TP+FN) | . Recall is used when we need to identify all positive samples; that is, when it is important to avoid false negatives. AKA sensitivity, hit rate, or true positive rate (TPR) . F-score or f-measure or f1-score -&gt; F = 2(precisionrecall)/(precision+recall) | . Harmonic mean of precision and recall . from sklearn.metrics import classification_report . Taking uncertainty into account . You can change the decision threshold depending on the problem | calibration: a calibrated model is a model that provides an accurate measure of its uncertainty | . Precision-recall curves and ROC curves . Setting a requirement on a classifier like 90% recall is often called setting the operation point . Precision-recall curve: look at all possible thresholds, or all possible trade-offs of precision and recalls at once. precision_recall_curve . Average precision: Area under the precision-recall curve. average_precision_score . Receiver operating characteristics (ROC) curve: consider all possible thresholds for a given classifier, shows the false positive rate (FPR) against the true positive rate (TPR). roc_curve . TPR = recall = (TP)/(TP+FN) | FPR = (FP)/(FP+TN) | . Average precision always returns a value between 0 (worst) and 1 (best). Predicting randomly always produces an AUC of 0.5 -&gt; AUC is much better than accuracy as a metric for imbalanced datasets . AUC: evaluating the ranking of positive samples. Probability that a randomly picked point of the positive class will have a higher score according to the classifier than a randomly picked point from the negative class . AUC does not make use of the default threshold, so adjusting the decision threshold might be necessary to obtain useful classification results from a model with a high AUC . Metrics for Multiclass Classification . Metrics for multiclass classification are derived from binary, but averaged over all classes . For imbalanced datasets: multiclass f-score -&gt; one binary f-score per class (that being the positive) and others being the negative -&gt; then average these per-class f-scores . Regression metrics . Rˆ2 is enough for most applications | Mean squared error (MSE) | Mean absolute error (MAE) | . Using evaluation metrics in model selection . scoring parameters for classification: . accuracy | roc_auc | average_precision | f1, f1_macro, f1_micro, f1_weighted | . for regression: . r2 | mean_squared_error | mean_absolute_error | . for more, see: sklearn.metrics.scores . 6. Algorithm Chains and Pipelines . ML algorithms requires chaining together many different processing steps and ML models. Pipeline class simplify the process of building chains of transformations and models . Parameter selection with preprocessing . mglearn.plots.plot_improper_processing() . Splitting the dataset during cross-validation should be done before doing any preprocessing. Any process that extracts knowledge from the dataset should only ever be applied to the training portion of the dataset, so any CV should be the “outermost loop” in your processing . Building Pipelines . from sklearn.pipeline import Pipeline pipe = Pipeline([(&quot;scaler&quot;, StandardScaler()), (&quot;lr&quot;, LogisticRegression())]) pipe.fit(X_train, y_train) pipe.score(X_test, y_test) . reduce the code needed for “preprocessing + classification” | main benefit: now you can use this single estimator in cross_val_score or GridSearchCV | . grid = GridSearchCV(pipe, param_grid=param_grid, cv=5) grid.fit(X_train, y_train) print(grid.best_score_) print(grid.best_params_) . For each split in the CV, the Scaler is refit with only the training splits and no information is leaked from the test split in to the parameter search . Information Leakage . The impact varies depending on the preprocessing step: . estimating the scale of the data using the test fold usually doesn’t have a terrible impact | using the test fold in feature extraction and feature selection can lead to substantial differences in outcomes | . The General Pipeline Interface . not restricted to preprocessing and classification | only requirement: all estimators but the last step need to have a transform method | during Pipeline.fit, the pipeline calls fit and then transform on each step; for the last step, just fit is called | when predicting, we similarly transform the data using all but the last step, and then call predict on the last step | there is no requirement to have predict in the last step; the last step is only required to have a fit method | . Convenient pipeline creation with make_pipeline . from sklearn.pipeline import make_pipeline pipe_short = make_pipeline(StandardScaler(), LogisticRegression()) . Accessing step attributes . named_steps attribute -&gt; dictionary from the step names to estimators . components = pipe.named_steps[&quot;pca&quot;].components_ . grid.best_estimator_.named_steps[&quot;logisticregression&quot;].coef_ . Grid-Searching Which Model to Use . pipe = Pipeline([(&quot;preprocessing&quot;, StandardScaler()), (&quot;classifier&quot;, SVC())]) from sklearn.ensemble import RandomForestClassifier param_grid = [ {&#39;classifier&#39;: [SVC()], &#39;preprocessing&#39;: [StandardScaler(), None], &#39;classifier__gamma&#39;: [0.001, 0.01, 0.1, 1, 10, 100], &#39;classifier__C&#39;: [0.001, 0.01, 0.1, 1, 10, 100]}, {&#39;classifier&#39;: [RandomForestClassifier(n_estimators=100)], &#39;preprocessing&#39;: [None], &#39;classifier__max_features&#39;: [1, 2, 3]}] X_train, X_test, y_train, y_test = train_test_split( cancer.data, cancer.target, random_state=0) grid = GridSearchCV(pipe, param_grid, cv=5) grid.fit(X_train, y_train) . 7. Working with Text Data . 8. Wrapping Up . Approaching a ML problem . It may be tempting to jump in and starting solving you data-related problem by running your favorite algorithm . To make effective use of ML, we need to take a step back and consider the problem at large. First, you should think what kind of question you want to answer . It is best if you can measure the performance of your algorithm directly using a business metric . Collecting more or different data or changing the task formulation slightly might provide a much higher payoff than running endless grid searches to tune parameters . Humans in the loop . Many applications are dominated by “simple cases”, for which an algorithm can make a decision, with relatively few “complicated cases”, which can be rerouted to a human . From prototype to production . Production systems have differente requirements from one-off analysis scripts | reliability, predictability, runtime, and memory requirements gain relevance | simplicity is key | . Testing production systems . offline evaluation: test set collected beforehand | online/live testing: consequences of employing the algorithm in the overall system are evaluated | . A/B testing: enables us to evaluate the algorithms “in the wild”, which might help us to discover unexpected consequences when users are interacting with our model . Bandit algorithms: more elaborate mechanisms for online testing that go beyond A/B testing .",
            "url": "https://millengustavo.github.io/blog/book/machine%20learning/data%20science/2020/01/22/intro-ml-python.html",
            "relUrl": "/book/machine%20learning/data%20science/2020/01/22/intro-ml-python.html",
            "date": " • Jan 22, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "The Hundred-Page Machine Learning Book",
            "content": "My notes and highlights on the book. . Author: Andriy Burkov . 1 Introduction . What is Machine Learning . Process of solving a practical problem: . Gathering a dataset | Algorithmically building a statistical model based on that dataset to be used somehow to solve the practical problem | Supervised Learning . Dataset is a collection of labeled examples . Goal is to use the dataset to produce a model that takes a feature vector as input and outputs information that allows deducing the label for this feature vector . Unsupervised Learning . Dataset is a collection of unlabeled examples . Goal is to create a model that takes a feature vector as input and either transforms it into another vector or into a value that can be used to solve a practical problem . Semi-supervised Learning . Dataset contains both labeled and unlabeled examples. Usually unlabeled quantity » labeled quantity . Goal is the same as supervised learning. When you add unlabeled examples, you add more information about your problem, a larger sample reflects better the probability distribution the data we labeled came from . Reinforcement Learning . Machine “lives” in an environment and is capable of perceiving the state as a vector of features. Machine can execute actions in every state. Different actions bring different rewards and could also move the machine to another state. . The goal of RL algorithm is to learn a policy. A policy is a function that takes the feature vector of a state as input and outputs an optimal action to execute. The action is optimal if it maximizes the expected average reward . Why the Model Works on New Data . PAC (“probably approximately correct”) learning: theory that helps to analyze whether and under what conditions a learning algorithm will probably output an approximately correct classifier . 2 Notation and Definitions . 3 Fundamental Algorithms . 4 Anatomy of a Learning Algorithm . Building Blocks of a Learning Algorithm . a loss function | an optimization criterion based on the loss function | an optimization routine leveraging training data to find a solution to the optimization criterion | Gradient Descent . Iterative optimization algorithm for finding the minimum of a function . Find a local minimum: Starts at some random point and takes steps proportional to the negative of the gradient of the function at the current point . Gradient descent proceeds in epochs. Epoch: using the training set entirely to update each parameter . The learning rate controls the size of an update . Regular gradient descent is sensitive to the choice of the learning rate and slow for large datasets . Minibatch stochastic gradient descent (SGD): speed up the computation by approximating the gradient descent using smaller batches (subsets) of the training data. . Upgrades to SGD: . Adagrad | Momentum | RMSprop | Adam | . How Machine Learning Engineers Work . You don’t implement algorithms yourself, you use libraries, most of which are open source -&gt; scikit-learn . Learning Algorithms’ Particularities . different hyperparameters | some can accept categorical features | some allow the data analyst to provide weightings for each class -&gt; influence the decision boundary | some given a feature vector only output the class -&gt; others the probability | some allow for online learning | some can be used for both classification and regression | . 5 Basic Practice . Feature Engineering . Transforming raw data into a dataset. Labor-intensive process, demands creativity and domain knowledge . Highly informative features = high predictive power . Low bias: predicts the training data well . One-Hot Encoding . Transform categorical feature into several binary ones -&gt; increase the dimensionality of the feature vectors . Binning . Also called bucketing. Convert a continuous feature into multiple binary features (bins or buckets), based on value range . Can help the learning algorithm to learn using fewer examples . Normalization . Converting the actual range of values into a standard range of values, typically in the interval [-1, 1] or [0, 1] . Can increase speed of learning. Avoid numerical overflow . Standardization . Also called z-score normalization. Values are rescaled so that they have the properties of a standard normal distribution with mean=0 and stdev=1 . If feature has outliers -&gt; prefer standardization than normalization . Feature rescaling -&gt; usually benefical to most learning algorithms . Dealing with Missing Features . Remove the examples (if data big enough) | Using algorithm that can deal with missing features | Data imputation | . Data Imputation Techniques . Average | Median | Value outside the normal range (i.e., -1 for data in [0, 1]) | Use the missing value as target for a regression problem | If data large enough: add binary indicator (another column) for each feature with missing value | . Use the same data imputation technique to fill the missing values on the test set you used to complete the training data . Learning Algorithm Selection . Explainability | In-memory vs. out-of-memory | Number of features and examples | Categorical vs. numerical features | Nonlinearity of the data | Training speed | Prediction speed | Test on the validation set | . Three Sets . Training set | Validation set | Test set | Shuffle the examples and split the dataset into three subsets. Training set is usually the biggest one, use it to build the model. Validation and test sets are roughly the same sizes, much smaller than the training set. The learning algorithm cannot use these two subsets to build the model -&gt; those two are also often called holdout sets . Why two holdout sets? We use the validation set to choose the learning algorithm and find the best hyperparameters. We use the test set to assess the model before putting it in production . Underfitting and Overfitting . High bias: model makes many mistakes on the training data -&gt; underfitting. Reasons: . model is too simple for the data | the features are not informative enough | . Solutions: . try a more complex model | engineer features with higher predictive power | . Overfitting: model predicts very well the training data but poorly the data from at least one of the two holdout sets. Reasons: . model is too complex for the data | too many features but a small number of training examples | . High variance: error of the model due to its sensitivity to small fluctuations in the training set . The model learn the idiosyncrasies of the training set: the noise in the values of features, the sampling imperfection (due to small dataset size) and other artifacts extrinsic to the decision problem at hand but present in the training set . Solutions: . try a simpler model | reduce the dimensionality of the dataset | add more training data | regularize the model | . Regularization . Methods that force the learning algorithm to build a less complex model. Often leads to slightly higher bias but significantly reduces the variance -&gt; bias-variance tradeoff . L1 regularization: produces a sparse model, most of its parameters equal to zero. Makes feature selection by deciding which features are essential for prediction. Lasso regularization | L2 regularization: penalizes larger weights, if your only goal is to decrease variance, L2 usually gives better results. L2 also has the advantage of being differentiable, so gradient descent can be used for optimizing the objective function. Ridge Regularization | Elastic net regularization: combine L1 and L2 | . Neural networks also benefit from two other regularization techniques: . Dropout | Batch Normalization | . Also non-mathematical methods have a regularization effect: data augmentation and early stopping . Model Performance Assessment . Model generalizes well: model performs well on predicting the test set . Overfitting: error on the test data is substantially higher then the error obtained in the training data . Confusion Matrix . Table that summarizes how successful the classification model is at predicting examples belonging to various classes . Used to calculate two other metrics: precision and recall . Precision/Recall . Precision: ratio of correct positive predictions to the overall number of positive predictions: TP/(TP+FP) | Recall: ratio of correct positive predictions to the overall number of positive examples in the dataset: TP/(TP+FN) | . In practice, almost always have to choose between high precision or high recall -&gt; usually impossible to have both . assign a higher weighting to the examples of a specific class | tune hyperparameters to maximize precision or recall on the validation set | vary the decision threshold for algorithms that return probabilities of classes | . Accuracy . Number of correctly classified examples divided by the total number of classified examples: (TP+TN)/(TP+TN+FP+FN) . Useful metric when errors in predicting all classes are equally important . Cost-Sensitive Accuracy . When different classes have different importances . Assign a cost (positive number) to both types of mistakes: FP and FN. Then compute the counts TP, TN, FP, FN as usual and multiply the counst for FP and FN by the corresponding cost before calculating the accuracy normally . Area under the ROC Curve (AUC) . ROC curve (“receiver operating characteristic”, comes from radar engineering): use a combination of the true positive rate (define exactly as recall) and false positive rate (proportion of negative examples predicted incorrectly) to build up a summary picture of the classification performance . TPR = TP/(TP+FN) . FPR = FP/(FP+TN) . ROC curvers can only be used to assess classifiers that return some confidence score (or a probability) of prediction . The higher the area under the ROC curve (AUC) the better the classifier. AUC &gt; 0.5 -&gt; better than a random classifier. AUC = 1 -&gt; perfect classifier -&gt; TPR closer to 1 while keeping FPR near 0 . Hyperparameter Tuning . Grid Search: when you have enough data for a validation set and the number of hyperparameters and their range is not too large | Random Search: instead of providing discrete set of values to explore, you provide a statistical distribution for each hyperparameter from which values are randomly samples and set the total number of combinations you want to try | Bayesian hyperparameter optimization: use past evaluation results to choose the next values to evaluate | Gradient-based techniques | Evolutionary optimization techniques | . Cross-Validation . When you have few training examples, it could be prohibitive to have both validation and test set. You would prefer to use more data to train the model. In such case, you only split your data into training and test. Then you use cross-validation on the training set to simulate a validation set . 6 Neural Networks and Deep Learning . 7 Problems and Solutions . 8 Advanced Practice . Handling Imbalanced Datasets . Set the cost of misclassification of examples of the minority class higher | oversampling -&gt; make multiple copies of the example of some class | undersampling -&gt; randomly remove from training set some examples of the majority class | synthetic minority oversampling technique (SMOTE) | adaptive synthetic sampling method (ADASYN) | algorithms less sensitive to imbalanced datasets: Decision trees, Random Forest, Gradient Boosting | . Combining Models . Ensemble models, typically combine models of the same nature. Boost performance by combining hundreds of weak models. We can sometimes get an additional performance gain by combining strong models made with different learning algorithms (two or three models): . averaging | majority vote | stacking | . Stacking: building a meta-model that takes the output of base models as input. Make sure your stacked model performs better on the validation set than each of the base models you stacked. When several uncorrelated strong models agree they are more likely to agree on the correct outcome . Training Neural Networks . Challenge to convert your data into the input the network can work with (i.e., resize images, word embeddings) | The choice of specific NN architecture is a difficult one | Decide the number of layers, their type and size | Regularization | . Advanced Regularization . For NNs, besides L1 and L2 regularization: . Dropout: for each pass, temporarily exclude at random some units frrom the computation | Early Stopping: stop training once observe a decreased performance on the validation set | Batch Normalization: standardize the outputs of each layer | Data augmentation: create a synthetic example from an original by applying various transformations | . Handling Multiple Inputs . Multimodal data -&gt; e.g., input is an image and text and binary output indicates whether the text describes this image . It’s hard to adapt shallow learning algorithms to work with multimodal data -&gt; train one shallow model on the image and another one in the text . Handling Multiple Outputs . Some problems you would like to predict multiple outputs for one input -&gt; sometimes can convert into a multi-label classification problem -&gt; Subnetworks . Transfer Learning . Pick an existing model trained on some dataset, and adapt this model to predict examples from another dataset, different from the one the model was built on . Build a deep model on the original big dataset | Compile a much smaller labeled dataset for your second model | Remove the last one or several layers from the first model | Replace the removed layers with new layers adapted for the new problem | “Freeze” the parameters of the layers remaining from the first model | Use your smaller labeled dataset and gradient descent to train the parameters of only the new layers | Algorithmic Efficiency . Big O notation: classify algorithms according to how their running time or space requirements grow as the input size grows. Complexity measured in the worst case . avoid using loops whenever possible | use appropriate data structures (e.g., if order is not important, use a set instead of a list) | use dict (hashmap) -&gt; allows you to define a collection of key-value pairs with very fast lookups for keys | use Scientific Python packages -&gt; many methods implemented in C | if you need to iterate over a vast collection of elements, use generators that create a function that return one element at a time rather than all the elements at once | cProfile package to find inefficiencies | multiprocessing package | PyPy, Numba | . 9 Unsupervised Learning . 10 Other Forms of Learning . Metric Learning . You can create a metric that would work better for your dataset . One-shot learning with siamese networks and triplet loss can be seen as metric learning problem . Learning to Rank . Supervised learning problem (e.g., optimization of search results returned by a search engine for a query) . Three approaches: . pointwise | parwise | listwise | . State of the art rank learning algorithm: LambdaMART. Listwise approach -&gt; one popular metric that combines both precision and recall is called mean average precision (MAP) . In typical supervised learning algorithm, we optimize the cost instead of the metric (usually metrics are not differentiable). In LambdaMART the metric is optmized directly . Learning to Recommend . Content-based filtering: learning what users like based on the description of the content they consume -&gt; user can be trapped in a “filter bubble” | Collaborative filtering: recommendations to one user are computed based on what other users consume or rate -&gt; content of the item consumed is ignored -&gt; huge and extremely sparse matrix | . Real-world recommender systems -&gt; hybrid approach . Factorization Machines (FM) . Explicity designed for sparse datasets. Users and items are encoded as one-hot vectors . Denoising Autoencoders (DAE) . NN that reconstructs its input from the bottleneck layer. Ideal tool to build a recommender system: input is corrupted by noise while the output shouldn’t be . Idea: new items a user could like are seen as if they were removed from the complete set by some corruption process -&gt; goal of the denoising autoencoder is to reconstruct those removed items . Another effective collaborative-filtering model is an FFNN with two inputs and one output . Self-Supervised Learning: Word Embeddings . Word embeddings: feature vectors that represent words -&gt; similar words have similar feature vectors . word2vec: pretrained embeddings for many languages are available to download online. skip-gram . Self-supervised: the labeled examples get extracted from the unlabeled data such as text . 11 Conclusion . Topic Modeling . Prevalent unsupervised learning problem. Latent Dirichlet Allocation (LDA) -&gt; You decide how many topics are in your collection, the algorithm assigns a topic to each word in this collection. To extract the topics from a document -&gt; count how many words of each topic are present in that document . Gaussian Process (GP) . Supervised learning method that competes with kernel regression . Generalized Linear Models (GLM) . Generalization of the linear regression to modeling various forms of dependency between the input feature vector and the target . Probabilistic Graphical Models (PGM) . One example: Conditional Random Fields (CRF) -&gt; model the input sequence of words and relationships between the features and labels in this sequence as a sequential dependency graph . Graph: structure consisting of a colletion of nodes and edges that join a pair of nodes . PGMs are also know under names of Bayesian networks, belief networks and probabilistic independence networks . Markov Chain Monte Carlo (MCMC) . If you work with graphical models and want to sample examples from a very complex distribution defined by the dependency graph. MCMC is a class of algorithms for sampling from any probability distribution defined mathematically . Generative Adversarial Networks (GAN) . Class of NN used in unsupervised learning. System of two neural networks contesting with each other in a zero-sum game setting . Genetic Algorithms (GA) . Numerical optimization technique used to optimize undifferentiable optimization objective functions. Use concepts from evolutionary biology to search for a global optimum (minimum or maximum) of an optimization problem, by mimicking evolutionary biological processes . GA allow finding solutions to any measurable optimization criteria (i.e., optimize hyperparameters of a learning algorithm -&gt; typically much slower than gradient-based optimization techniques) . Reinforcement Learning (RL) . Solves a very specific kind of problem where the decision making is sequential. There’s an agent acting in a unknown environment. Each action brings a reward and moves the agent to another state of the envinronment. The goal of the agent is to optimize its long-term reward .",
            "url": "https://millengustavo.github.io/blog/book/machine%20learning/data%20science/2020/01/14/hundred-page-ml.html",
            "relUrl": "/book/machine%20learning/data%20science/2020/01/14/hundred-page-ml.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Forecasting at Scale",
            "content": "My notes and highlights on the paper. . Authors: Sean J. Taylor and Benjamin Letham . Source . 1. Introduction . Analysts who can produce high quality forecasts are quite rare because forecasting is a specialized skill requiring substantial experience . | Time series model flexible enough for a wide range of business time series, yet configurable by people with little knowledge about time series models and methods . | Automated means of evaluating and comparing forecasts, as well as detecting when they are likely to be performing poorly . | . 2. Features of Business Time Series . Auto ARIMA: prone to large trend errors and fail to capture any seasonality | Exponential smoothing and seasonal naive: capture weekly seasonality, but miss longer-term seasonality | . Tuning these methods requires a thorough understading of how the underlying time series models work . 3. The Prophet Forecasting Model . Designed to have intuitive parameters that can be adjusted without knowing the details of the underlying model | Decomposable time series model with three main model components: trend, seasonality and holidays | Also an error term: any idiosyncratic changes which are not accomodated by the model (assumption: normally distributed) | Similar to a generalized additive model (GAM): class of regression models with potentially non-linear smoothers applied to the regressors | Frame the forecasting problem as a curve-fitting exercise != time series models that explicitly account for the temporal dependence structure in the data | . Practical advantages . Flexibility | Measurements do not need to be regularly spaced and do not need to interpolate missing values (e.g., remove outliers) | Fitting is fast | Easily interpretable parameters. Easy to extend the model to include new components | Tailored to time series with piecewise trends, multiple seasonality, floating holidays | . 3.1 Trend Model . Saturating growth model | Piecewise linear model | . Changepoints can be specified or may be automatically selected given a set of candidates. A parameter controls the flexibility of the model altering its rate . The uncertainty in the trend is measured assuming the future will se the same average frequency and magnitude of rate changes that were seen in the history . 3.2 Seasonality . Fourier series to provide a flexible model of periodic effects . 3.3 Holidays and Events . Analyst provide a custom list of past and future events, identified by the event or holiday’s unique name . Include additional parameters for the days surrounding the holiday, essentially treating each of the days in the window around the holiday as a holiday itself . 3.4 Model fitting . Benefit of a decomposable model: allow us to look at each component of the forecast separately . Default values are appropriate to most forecast problems . 3.5 Analyist-in-the-Loop Modeling . Capacities | Changepoints | Holidays and seasonality | Smoothing parameters | . Parameters to increase/decrease trend flexibility and strength of the seasonality component . Analyist-in-the-Loop Modeling: blends the advantages of statistical and judgemental forecasts. Lets analysts apply judgement to forecasts through a small set of intuitive model parameters and options, while retaining the ability to fall back on fully automated statistical forecasting when necessary . 4. Automating Evaluation of Forecasts . Use of baseline forecasts | Modeling forecast accuracy: mean absolute percentage error (MAPE) -&gt; for interpretability | . Simulated Historical Forecasts . Produce K forecasts at various cutoff points in the history | Simulate the errors we would have made hade we used this forecasting method at those points in the past | Simple, easy to explain and relatively uncontroversial for generating insight into forecast errors | . Issues to be aware: . The more simulated forecasts we make, the more correlated their estimates of error are | Forecasting methods can perform better or worse with more data | . Identifying Large Forecast Errors . When the forecast has large errors relative to the baselines, the model may be misspecified. Analysts can adjust the trend model or the seasonality, as needed | Large errors for all methods on a particular date are suggestive of outliers. Analysts can identify outliers and remove them. | When the SHF error for a method increases sharply from one cutoff to the next, it could indicate that the data generating process has changed. Adding changepoints or modeling different phases separately may address the issue. | . 5. Conclusion . Prophet in summary: . Simple, modular regression model that often works well with default parameters, and that allows analysts to select the components that are relevant to their forecasting problem and easily make adjustmens as needed | Has a system for measuring and tracking forecast accuracy, and flagging forecasted that should be checked manually to help analysts make incremental improvements | .",
            "url": "https://millengustavo.github.io/blog/book/time%20series/forecasting/data%20science/machine%20learning/statistics/2020/01/13/prophet-paper.html",
            "relUrl": "/book/time%20series/forecasting/data%20science/machine%20learning/statistics/2020/01/13/prophet-paper.html",
            "date": " • Jan 13, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Forecasting: Principles and Practice",
            "content": "My notes and highlights on the book. . Authors: Rob J Hyndman and George Athanasopoulos . Available for free here (online) . 1. Getting Started . Forecasting is difficult. Businesses that do it well have a big advantage over those whose forecasts fail . Important aid to effective and efficient planning | . predictability: . how well we understand the factors | how much data we have | our forecasts affect the thing? | . Naïve method: using the most recent observation as a forecast . Qualitative x Quantitative forecasting . qualitative: no data available | quantitative: numerical info about the past is available; reasonable to assume that some aspects of the past will continue in the future | . Explanatory models / mixed models / dynamic regression models / longitudinal models / transfer function models / linear system models: incorporates information about other vars rather than only historical values of the variable to forecast . Drawbacks of explanatory models: . system may not be understood | necessary to know/forecast the future values of various predictors | main concern may be only to predict what, not why | time series model may give more accurate forecasts | . Basic steps of a forecasting task . Problem definition | Gathering information | Preliminary (exploratory) analysis | Choosing and fitting models | Using and evaluating a forecasting model | 2. Time series graphics . Graphs enable: . patterns | unusual observations | changes over time | relationships between vars | . Time plot: observations against time of observation . trend: long-term increase/decrease in the data | seasonal: always of a fixed and known frequency (factors: time of the year and/or day of the week) | cyclic: rises/falls that are not of a fixed frequency | . Correlation . Ranges between -1 and 1 . Correlation coefficient only measures the strenght of the linear relationship . Scatterplot matrix: relationships between all pairs of variables . Autocorrelation Function (ACF) . Measures the relationship between lagged values of a time series . Correlogram: autocorrelation coefficients plot . Data with trend, ACs for small lags -&gt; large and positive because observations nearby in time are also nearby in size. So the ACF of trended time series -&gt; positive values that slowly decrease as the lags increase. . Data seasonal, the ACs will be larger for the seasonal lags (at multiples of the seasonal frequency) than for other lags. . Data are both trended and seasonal -&gt; combination of these effects . White noise: time series that show no autocorrelation. 95% of the spikes in the ACF lie within +-2/sqrt(T) where T is the length of the time series . 3. The forecaster’s toolbox . Simple methods (benchmarks mostly) . Average method: future values are equal to the average (or “mean”) | Naïve method: equal to the value of the last observation -&gt; optimal when data follow a random walk = random walk forecasts | Seasonal naïve method: equal to the last observed value from the same season of the year | Drift method: drawing a line between the first and last observations, and extrapolating it into the future | . Any forecasting methods we develop will be compared to these simple methods -&gt; test if it is worth considering . Transformations and adjustments . Adjusting the data can lead to simpler forecasting task . Calendar adjustments: e.g., removing variation of days between months (use average per day instead of monthly) | Population adjustments: data affected by population changes, it is best to use per-capita data rather than the totals | Inflation adjustments: data affected by the value of money are best adjusted before modelling (price indexes, Consumer Price Index - CPI) | Mathematical transformations: data show variation that increases/decreases with the level of the series, math transf. can be useful. e.g., log transformation (interpretable), power transformations, Box-Cox transformations. Often no transformation is needed. Transformations sometimes make little difference to the forecasts but have a large effect on prediction intervals | . Residuals . What is left over after fitting a model . Residuals are uncorrelated. Or there is information left in the residuals -&gt; should be used in computing forecasts | Residuals have zero mean. Or the forecasts are biases. (easy to fix) | Residuals have constant variance | Residuals are normally distributed | . Portmanteau test: test for a group of autocorrelations. e.g., Box-Pierce test, Ljung-Box test . Training and Test sets . Size of the test set (hold-out set or out-of-sample data): about 20% of the total sample. Ideally be at least as large as the maximum forecast horizon required . residuals are calculated in the training set | forecast errors are calculated in the test set | . Forecast errors . Scale-dependent errors: forecast errors are on the same scale as the data. e.g., MAE, RMSE. | Percentage errors: unit-free, freq. used to compare forecast performance between data sets. e.g., MAPE (mean absolute percentage error), sMAPE (not recommended by Hyndman) | Scaled errors: scale the errors based on the training MAE -&gt; MASE (mean absolute scaled error) | . Time series cross-validation . Series of test sets, each consisting of a single observation. Corresponding training set consists only of observations that occurred prior to the observation that forms the test set -&gt; forecast accuracy computed by avg over the test sets -&gt; “evaluation on a rolling forecasting origin” . A good way to choose the best forecasting model is to find the model with the smallest RMSE computed using time series cross-validation. . Prediction intervals . One-step: stdev of the forecast distribution ~= stdev of the residuals | Multi-step: intervals increase in length as the forecast horizon increases | From bootstrapped residuals: when a normal distribution for the forecast errors is an unreasonable assumption -&gt; use bootstrapping (only assume errors are uncorrelated) | With transformations: should be computed on the transformed scale | . 4. Judgemental forecasts . Used when: . no available data | data available, statistical forecasts are generated, and these are then adjusted used judgement | data available, statistical and judgmental forecasts are generated independently and then combined | . Accuracy of judmental forecasting (lack of historical data) increases when the forecaster has: . important domain knowledge | more timely, up-to-date information | . JF is subjective and comes with limitations. But implementing systematic and well-structured approachs can confine these limitations and markedly improve forecast accuracy . Limitations . Subjective | Can be inconsistent | Depends heavily on human cognition (be aware of the multiple cognitive biases) | Anchoring effect -&gt; subsequent forecasts tend to converge or be close to an inital familiar reference point (create systematic bias) | . Key principles . Set the forecasting task clearly and concisely (avoid emotive terms and irrelevant information) | Implement a systematic approach | Document and justify (accountability -&gt; reduces bias) | Systematically evaluate forecasts | Segregate forecasters and users | . The Delphi method . Forecasts from a group are generally more accurate than those from individuals -&gt; construct consensus forecasts from a group of experts in a structured iterative manner . Forecasting by analogy . Thinking and discussing analogous products or situations can generate useful information (e.g., market value of a house by comparing it to similar properties sold in the area) . Scenario forecasting . Generate forecasts based on plausible scenarios (e.g.: “best”, “middle”, “worst”) . New product forecasting . Sales force composite: aggregate forecast for each outlet/branch/store of a company generated by salespeople | Executive opinion (remember accountability!) | Customer intentions: questionnaires, beware of varying correlation between intention and behaviour | . Judgemental adjustments . Use adjustments sparingly | Apply a structure approach | . 5. Time series regression models . Forecast the time series of interest assuming it has a linear relationship with other time series . Linear Regression . Assumptions about the errors: . mean = 0, otherwise forecasts systematically biased | not autocorrelated, otherwise there is still information to be exploited | unrelated to the predictor vars, otherwise more info should be included in the systematic part of the model | . Goodness-of-fit . Coefficient of determination (R²): proportion of variation in the forecast variable that is accounted by the regression model. Range between 0 (worst) and 1 (best) . Validating performance on the test data is much better than measuring R² on the training data . Standard error of the regression . “Residual standard error” . Evaluating the regression model . Residuals (training-set errors): difference between observed and fitted values. Average of the residuals = 0 and correlations between residuals and predictor = 0 . ACF plot of residuals . If there is autocorrelation in the residuals -&gt; info left over which should be accounted in the model -&gt; usually have larger prediction intervals . Breusch-Godfrey: Lagrange Multiplier (LM) test for serial correlation -&gt; similar to Ljung-Box test, but specifically for regression models . Histogram of residuals . Check whether the residuals are normally distributed, not essential, but makes the calculation of prediction intervals easier . Residual plots against predictors . If the scatterplots show a pattern, the relationship may be nonlinear and the model will need to be modified . Also plot the residuals against predictors that are not in the model -&gt; if show pattern, the corresponding predictor may need to be added to the model . Residual plots against fitted values . If pattern is observed, there may be “heteroscedasticity” in the errors -&gt; variance of the residuals may not be constant -&gt; transformation of the forecast variable (e.g., logarithm or square root) . Outliers and influential observations . outliers -&gt; obs with extreme values compared to the majority of the data | influential observations -&gt; obs that have a large influence on the estimated coeff of a regression model | . Spurious regression . More often than not, time series data are “non-stationary” -&gt; values do not fluctuate around a constant mean or with a constant variance . Regressing non-stationary time series can lead to spurious regressions. Signs: High R², high residual autocorrelation . Some useful predictors . Trend | Dummy variables: when a predictor is a categorical variable | Intervention variables: when the effect lasts only for one period -&gt; “spike” variable | Trading days | Distributed lags | Fourier series: alternative to seasonal dummy vars, especially for long seasonal periods. Regression model containing Fourier terms -&gt; harmonic regression | . Selecting predictors . Not recommended . Plot forecast var against a particular predictor -&gt; not always possible to see a relationship without accounting for other predictors | Multiple linear regression with all predictors and disregard vars with high p-value | . Recommended . Adjusted R² (tends to select too many predictors) | Cross-validation (CV) | Akaike’s Information Criterion (AIC) | Corrected Akaike’s Information Criterion (AICc) | Schwarz’s Bayesian Information Criterion (BIC) | . Best subset regression: fit all potential regression models and choose the best (based on the criteria above) -&gt; “all possible subsets” regression . Stepwise regression . Backwards stepwise regression: starts with all predictors, remove one at a time, keep the model if improves, iterate until no further improvement -&gt; bad if n_predictors is very large | Forward stepwise regression: starts with one, add one at a time, the one the most improves is kept, iterate… | . Not guaranteed to lead to the best possible model . Ex-ante vs ex-post forecasts . Ex-ante: made using only info available in advance | Ex-post: made using later info on the predictors (not genuine forecasts, but useful for studying the behaviour of forecasting models) | . Matrix formulation . Correlation, causation and forecasting . It is important not to confuse correlation with causation, or causation with forecasting . Correlations are useful for forecasting, even when there is no causal relationship. Often a better model is possible if a causal mechanism can be determined. . Confounded predictors . Two vars are confounded when their effects on the forecast variable cannot be separated . Confounding: not a problem for forecasting. However, it becomes a problem with scenario forecasting -&gt; take account of the relationships between predictors. Also a problem if some historical analysis of the contributions of various predictors is required. . Multicollinearity and forecasting . Multicollinearity: when similar info is provided by 2+ predictors in a multiple regression (e.g., dummy var trap). Generally not a problem if you are not interested in the specific contributions of each predictor, except when there is perfect correlation . 6. Time series decomposition . Three types of time series patterns: . trend-cycle (or just trend) | seasonality | remainder (anything else in the time series) | . Time series components . Additive decomposition: if the magnitude of the seasonal fluctuations, or the variation around the trend-cycle, does not vary with the level of the time series | Multiplicative decomposition: when the variation in the seasonal pattern or the variation around the trend-cycle, appears to be proportional to the level of the time series -&gt; common with economic time series | . Alternative to using multiplicative: first transform the data -&gt; variation appears to be stable over time -&gt; use additive . Seasonally adjusted data: if the seasonal component is removed from the original data . Seasonally adjusted series contain the remainder component as well as the trend-cycle. Therefore, they are not “smooth”, and “downturns” or “upturns” can be misleading. If the purpose is to look for turning points in a series, and interpret any changes in direction, then it is better to use the trend-cycle component rather than the seasonally adjusted data. . Moving averages . First step in classical decomposition -&gt; using MA to estimate the trend-cycle. The order of the moving average determines the smoothness of the trend-cycle estimate (larger order = smoother curve). . Odd order -&gt; symmetric MA . Applying MA to a MA -&gt; symmetric MA . Most common use of centred MAs -&gt; estimating the trend-cycle form seasonal data . A 2×12-MA can be used to estimate the trend-cycle of monthly data and a 7-MA can be used to estimate the trend-cycle of daily data with a weekly seasonality. Other choices for the order of the MA will usually result in trend-cycle estimates being contaminated by the seasonality in the data. . Weighted moving averages . Combinations of moving averages result in weighted moving averages . Advantage: yield a smoother estimate of the trend-cycle . Classical decomposition (not recommended) . Starting point for most other methods of time series decomposition . don’t estimate trend-cycle/remainder for first and last few observations | tends to over-smooth rapid rises and falls | unable to capture seasonal changes over time | not robust to unusual values | . X11 decomposition . for quarterly and monthly data . SEATS decomposition . “Seasonal Extraction in ARIMA Time Series”: works only with quarterly and monthly data . STL decomposition . “Seasonal and Trend decomposition using Loess” . handle any type of seasonality | seasonal component is allowed to change over time | smoothness of the trend-cycle can be controlled by the user | can be robust to outliers | doesn’t handle trading day or calendar variation automatically | . Measuring strength of trend and seasonality . Useful when you have a large collection of time series and you need to find the series with the most trend or the most seasonality . Forecasting with decomposition . Decomposition is primarily useful for studying time series data and exploring historical changes over time, but can also be used in forecasting . Forecast the seasonal component and the seasonally adjusted component separately: . assume the seasonal component is unchanging (or changing slowly) -&gt; seasonal naïve method | to forecast the seasonally adjusted component -&gt; any non-seasonal forecasting method may be used | . 7. Exponential smoothing . Forecasts produced using exponential smoothing methods are weighted averages of past observations, with the weights decaying exponentially as the observations get older. In other words, the more recent the observation the higher the associated weight . Simple exponential smoothing (SES) . For data with no clear trend or seasonal pattern . Optimisation . Exponential smoothing method requires the smoothing parameters and the initial values to be chosen . subjective manner | estimate from observed data -&gt; minimising the SSE (sum of squared errors) | . Trend methods . Holt’s linear trend method . Extends simple exponential smoothing to allow the forecasting of data with a trend . Damped trend methods . Introduce a parameter that “dampens” the trend to a flat line some time in the future . As forecasting tasks can vary by many dimensions (length of forecast horizon, size of test set, forecast error measures, frequency of data, etc.), it is unlikely that one method will be better than all others for all forecasting scenarios. What we require from a forecasting method are consistently sensible forecasts, and these should be frequently evaluated against the task at hand. . Holt-Winters’ seasonal method . Extends Holt’s method to capture seasonality . Additive | Multiplicative | . Additive preferred when the seasonal variations are roughly constant. Multiplicative preferred when the seasonal variations are changing proportional to the level of the series . A taxonomy of exponential smoothing methods . By considering variations in the combinations of the trend and seasonal components, nine exponential smoothing methods are possible . Innovations state space models for exponential smoothing . Statistical models generate point forecasts, but can also generate forecast intervals -&gt; stochastic (or random) data generating process that can produce an entire forecast distribution . State space models: model consists of a measurement equation (observed data) and state equations (unobserved components or states: level, trend, seasonal change over time) . ETS(A,N,N): simple exponential smoothing with additive errors | ETS(M,N,N): simple exponential smoothing with multiplicative errors | ETS(A,A,N): Holt’s linear method with additive errors | ETS(M,A,N): Holt’s linear method with multiplicative errors | Other ETS models | . Estimation and model selection . Estimating ETS models . minimize SSE | maximize “likelihood” (probability of the data arising from the specified model) | . Model selection . Information criteria can be used for model selection on the ETS statistical framework . AIC | AICc | BIC | . Forecasting with ETS models . ETS point forecasts are equal to the medians of the forecast distributions . Prediction intervals . Big advantage: prediction intervals can also be generated . 8. ARIMA models . Exponential smoothing -&gt; describe the trend and seasonality . ARIMA -&gt; describe the autocorrelations . Stationarity and differencing . Stationary time series: properties do not depend on the time at which the series is observed. Time series with trends, or with seasonality, are not stationary. White noise is stationary . Stationary time series will have no predictable patterns in the long-term -&gt; time plot roughly horizontal, with constant variance . Differencing . Differencing: computing the differences between consecutive observations = “first differences” -&gt; one way to make a non-stationary time series stationary . Transformations such as logarithms can help to stabilise the variance of a time series. Differencing can help stabilise the mean of a time series by removing changes in the level of a time series, and therefore eliminating (or reducing) trend and seasonality. . ACF Plot: For a stationary time series, the ACF will drop to zero relatively quickly, while the ACF of non-stationary data decreases slowly. . Random walk model . Widely used for non-stationary data -&gt; financial, economic data . y_t = y_(t-1) + white_noise_t . Typically have: . long periods of apparent trends up or down | sudden and unpredictable changes in direction | . Underpins naïve forecasts . Second-order differencing . When difference data not appear stationary. “Change in the changes”. Almost never necessary to go beyond second-order. . Seasonal differencing . Seasonal difference: difference between an observation and the previous observation from the same season . There is a degree of subjectivity in selecting which differences to apply. It is important that if differencing is used, the differences are interpretable. . Unit root tests . Statistical hypothesis tests of stationarity that are designed for determining whether differencing is required. . Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test: null hypothesis = data stationary -&gt; p-value &lt; 0.05 -&gt; differencing is required | . Backshift notation . L -&gt; “lag” . B -&gt; “backshift” . B¹²y_t = y_(t-12) . Autoregressive models . Forecast the variable of interest using a linear combination of past values of the variable . AR(p) model: an autoregressive model of order p . Moving average models . Uses past forecast errors in a regression-like model . MA(q) model: a moving average model of order q . Moving average models should not be confused with the moving average smoothing. A moving average model is used for forecasting future values, while moving average smoothing is used for estimating the trend-cycle of past values . MA model is called invertible . Non-seasonal ARIMA models . Combine differencing with autoregression and a moving average model . ARIMA(p,d,q) model . p = order of the autoregressive part | d = degree of first differencing involved | q = order of the moving average part | . ACF and PACF plots . Sometimes used to determine appropriate values for p and q . Extensible explanation here . Estimation and order selection . Maximum likelihood estimation (MLE) . Finds the values of the parameters which maximise the probability of obtaining the data that we have observed . Information Criteria . Good models are obtained by minimising the Akaike’s Information Criterion (AIC) or AICc or BIC . Tend not to be good for selecting d, but only for selecting p and q . Forecasting . The prediction intervals for ARIMA models are based on assumptions that the residuals are uncorrelated and normally distributed. If either of these assumptions does not hold, then the prediction intervals may be incorrect. For this reason, always plot the ACF and histogram of the residuals to check the assumptions before producing prediction intervals. . Seasonal ARIMA models . Formed by including additional seasonal terms in the ARIMA models . The seasonal part of the model consists of terms that are similar to the non-seasonal components of the model, but involve backshifts of the seasonal period . The seasonal part of an AR or MA model will be seen in the seasonal lags of the PACF and ACF . Good examples in the book here . ARIMA vs ETS . Myth: ARIMA models are more general than exponential smoothing . All ETS models are non-stationary, while some ARIMA models are stationary . ETS models with seasonality or non-damped trend or both have two unit roots (i.e., they need two levels of differencing to make them stationary). All other ETS models have one unit root (they need one level of differencing to make them stationary). . 9. Dynamic regression models . Extend ARIMA models in order to allow other information to be included in the models . Estimation . Minimizing sum of squared errors or maximum likelihood estimation can be used . Forecasting . Forecast the regression part of the model and the ARIMA part of the model, and combine the results . In order to obtain forecasts we first need to forecast the predictors. When the predictors are known into the future, this is straightforward. But when the predictors are themselves unknown, we must either model them separately, or use assumed future values for each predictor. . Stochastic and deterministic trends . There is an implicit assumption with deterministic trends that the slope of the trend is not going to change over time. On the other hand, stochastic trends can change, and the estimated growth is only assumed to be the average growth over the historical period, not necessarily the rate of growth that will be observed into the future. Consequently, it is safer to forecast with stochastic trends, especially for longer forecast horizons, as the prediction intervals allow for greater uncertainty in future growth. . Dynamic harmonic regression . Long seasonal periods -&gt; a dynamic regression with Fourier terms is often better than other models . “Disadvantage”: the seasonal pattern is not allowed to change over time (usually remarkably constant) . Lagged predictors . Sometimes, the impact of a predictor which is included in a regression model will not be simple and immediate. In these situations, we need to allow for lagged effects of the predictor. . 10. Forecasting hierarchical or grouped time series . Time series can often be naturally disaggregated by various attributes of interest. These categories are nested within the larger group categories, and so the collection of time series follow a hierarchical aggregation structure -&gt; “hierarchical time series” . Often arise due to geographic divisions . Challenge: require forecasts that are coherent across the aggregation structure . 11. Advanced forecasting methods . Complex seasonality . Higher frequency time series often exhibit more complicated seasonal patterns . Dynamic harmonic regression with multiple seasonal periods . Multiple seasonalities -&gt; add Fourier terms for each seasonal period . TBATS models . Combination of Fourier terms with an exponential smoothing state space model and a Box-Cox transformation, in a completely automated manner. Seasonality is allowed to change slowly over time. Can be slow to estimate, especially with long time series. Do not allow for covariates . Vector autoregressions . Other models: unidirectional relationship -&gt; forecast variable is influenced by the predictor variables . Many cases: all variables affect each other . Feedback relationships are allowed for in the vector autoregressive (VAR) framework. All variables are treated symmetrically, they are all modelled as if they all influence each other equally -&gt; all variables are treated as “endogenous” . Despite being atheoretical, VARs are useful in several contexts: . forecasting a collection of related variables | testing whether one variable is useful in forecasting another (basis of Grande causality tests) | impulse response analysis | forecast error variance decomposition | . Neural network models . Allow complex nonlinear relationships between the response variable and its predictors . The predictors (or inputs) form the bottom layer, and the forecasts (or outputs) form the top layer. There may also be intermediate layers containing “hidden neurons”. . Neural network autoregression . NNAR model: lagged values of the time series can be used as inputs to a neural network . When it comes to forecasting, the network is applied iteratively. For forecasting one step ahead, we simply use the available historical inputs. For forecasting two steps ahead, we use the one-step forecast as an input, along with the historical data. . Bootstrapping and bagging . There are at least four sources of uncertainty in forecasting using time series models: . The random error term; | The parameter estimates; | The choice of model for the historical data; | The continuation of the historical data generating process into the future. | Baggeg forecasts . If we produce forecasts from each of the additional time series, and average the resulting forecasts, we get better forecasts than if we simply forecast the original time series directly. This is called “bagging” which stands for “bootstrap aggregating” . 12. Some practical forecasting issues . Weekly data . Difficult to work because the seasonal period (number of weeks in a year) is both large and non-integer (~52.18). Even if approximate to 52, most methods won’t handle such a large seasonal period efficiently . STL decomposition along with a non-seasonal method applied to the seasonally adjusted data | Dynamic harmonic regression model | TBATS model | . Daily and sub-daily data . Often involve multiple seasonal patterns -&gt; we need to use a method that handles such complex seasonality . Ensuring forecasts stay within limits . Transform the data using a scaled logit transform which maps (floor, cap) to the whole real line . y = log((x-a)/(b-x)) . Forecast combinations . The results have been virtually unanimous: combining multiple forecasts leads to increased forecast accuracy. In many cases one can make dramatic performance improvements by simply averaging the forecasts. . While there has been considerable research on using weighted averages, or some other more complicated combination approach, using a simple average has proven hard to beat. . Prediction intervals for aggregates . If the point forecasts are means, then adding them up will give a good estimate of the total. But prediction intervals are more tricky due to the correlations between forecast errors. . A general solution is to use simulations . Backcasting . Forecast in reverse time . Very long and very short time series . Forecasting very short time series . The sample size required increases with the number of parameters to be estimated, and the amount of noise in the data. . With short series, there is not enough data to allow some observations to be withheld for testing purposes, and even time series cross validation can be difficult to apply. The AICc is particularly useful here, because it is a proxy for the one-step forecast out-of-sample MSE . Forecasting very long time series . Most time series models do not work well for very long time series. The problem is that real data do not come from the models we use. Also the optimisation of the parameters becomes more time consuming. . If we are only interested in forecasting the next few observations, one simple approach is to throw away the earliest observations and only fit a model to the most recent observations. . Dealing with missing values and outliers . Missing values . It is worth considering whether the missingness will induce bias in the forecasting model. When it is not random, use a dynamic regression model, with dummy variables . Some methods allow for missing values without any problems. Whey they cause errors: . assuming long enough series, we could just take the section of data after the last missing value | we could replace the missing values with estimates (i.e., interpolation) | . Outliers . All methods considered in the book will not work well if there are extreme outliers in the data -&gt; replace them with missing or with a more consistent estimate . Simply replacing outliers without thinking about why they have occurred is a dangerous practice. They may provide useful information about the process that produced the data, and which should be taken into account when forecasting. . Further reading: “Forecasting in practice” .",
            "url": "https://millengustavo.github.io/blog/book/time%20series/forecasting/data%20science/machine%20learning/statistics/2020/01/07/hyndman-forecasting.html",
            "relUrl": "/book/time%20series/forecasting/data%20science/machine%20learning/statistics/2020/01/07/hyndman-forecasting.html",
            "date": " • Jan 7, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Python Machine Learning",
            "content": "My notes and highlights on the book. . Author: Sebastian Rashcka . Ch1. Giving Computers the Ability to Learn from Data . Self-learning algorithms: derive knowledge from data in order to make predictions efficiently . Three types of ML: . Supervised Learning: Labeled data, direct feedback, predict outcome/future | Unsupervised Learning: No labels, no feedback, find hidden structure in data | Reinforcement Learning: Decision process, reward system, learn series of actions | . Supervised Learning . Discrete class labels -&gt; classification | Outcome signal is a continuous value -&gt; regression | . Predictor variables = “features” . Response variable = “target” . Solving interactive problems with reinforcement learning . Goal: develop a system (agent) that improve its performance based on interactions with the environment . Related to supervised learning, but the feedback is not the ground truth label, but a measure of how well the action was measured by a reward function . The agent tries to maximize the reward through a series of interactions with the environment . Discovering hidden structures with unsupervised learning . Clustering . Allows us to organize data into meaningful subgroups (clusters) without having any prior knowledge of their group memberships. Sometimes called unsupervised classification . Dimensionality reduction for data compression . Commonly used approach in feature preprocessing to remove noise from data, which can degrade the predictive performance of certain algorithms, and compress the data onto a smaller dimensional subspace while retaining most of the relevant information . Useful for visualizing the data -&gt; High-dimensional feature set to 2D or 3D . A roadmap for building ML systems . Preprocessing - getting data into shape: scaling, dimensionality reduction, randomly divide the dataset into training and test sets | Training and selecting a predictive model: decide a metric to measure performance, compare a handful of different algorithms in order to train and select the best performing model, cross-validation, hyperparameter optimization | Evaluating models and predicting unseen data instances: how well the model performs on unseen data (generalization error) | . Ch2. Training Simple Machine Learning Algorithms for Classification . Perceptron . The convergence of the perceptron is only guaranteed if the two classes are linearly separable and the learning rate is sufficiently small . Numpy x Python for loop structures: Vectorization means that an elemental arithmetic operation is automatically applied to all elements in an array. By formulating our arithmetic operations as a sequence of instructions on an array, rather than performing a set of operations for each element at a time, we can make better use of our modern CPU architectures with single instruction, multiple data (SIMD) support. Furthermore, NumPy uses highly optimized linear algebra libraries, such as Basic Linear Algebra Subprograms (BLAS) and Linear Algebra Package (LAPACK), that have been written in C or Fortran . Adaptive linear neurons and the convergence of learning . ADAptive LInear NEuron (Adaline): weights are updated based on a linear activation function rather than a unit step function -&gt; Widrow-Hoff rule . Minimizing cost functions with gradient descent . Objective function: often a cost function that we want to minimize . Gradient descent: powerful optimization algorithm to find the weights that minimize the cost function -&gt; climbing down a hill until a local or global cost minimum is reached -&gt; take steps in the opposite direction of the gradient. Step size defined by the learning rate and slope of the gradient . A logistic regression model is closely related to Adaline, the only difference being its activation and cost function . Improving gradient descent through feature scaling . Gradient descent is one of the many algorithms that benefit from feature scaling . Standardization: gives the data properties of a standard normal distribution -&gt; zero-mean and unit variance . Large-scale machine learning and stochastic gradient descent . Batch gradient descent: gradient is calculated from the whole training dataset | Stochastic gradient descent (SGD): iterative/online gradient descent. Each gradient is calculated on a single training example. | . Advantages: . typically reaches convergence much faster because of more frequent weight updates | can escape shallow local minima more readily if we are working with nonlinear cost functions | can be used for online learning -&gt; model trained on the fly as new data arrives | . With SGD it is important to present training data in a random order; also shuffle the training dataset for every epoch to prevent cycles . Mini-batch gradient descent: batch gradient descent to smaller subsets of the training data. Compromise between SGD and batch -&gt; vectorized operations can improve the computational efficiency . Ch3. A Tour of Machine Learning Classifiers Using scikit-learn . Ch4. Building Good Training Datasets – Data Preprocessing . Missing Data . Unfortunately, most computational tools are unable to handle such missing values or will produce unpredictable results if we simply ignore them. Therefore, it is crucial that we take care of those missing values before we proceed with further analyses. . Nowadays, most scikit-learn functions support DataFrame objects as inputs, but since NumPy array handling is more mature in the scikit-learn API, it is recommended to use NumPy arrays when possible. . # only drop rows where NaN appear in specific columns (here: &#39;C&#39;) &gt;&gt;&gt; df.dropna(subset=[&#39;C&#39;]) . Categorical Features . When we are talking about categorical data, we have to further distinguish between ordinal and nominal features. Unfortunately, there is no convenient function that can automatically derive the correct order of the labels of our size feature, so we have to define the mapping manually. . We can simply define a reverse-mapping dictionary, inv_size_mapping = {v: k for k, v in size_mapping.items()} . Most estimators for classification in scikit-learn convert class labels to integers internally, but it is considered good practice to provide class labels as integer arrays to avoid technical glitches. . Although the color values don’t come in any particular order, a learning algorithm will now assume that green is larger than blue , and red is larger than green . Although this assumption is incorrect, the algorithm could still produce useful results. However, those results would not be optimal. . A common workaround for this problem is to use a technique called one-hot encoding. The idea behind this approach is to create a new dummy feature for each unique value in the nominal feature column. . When we are using one-hot encoding datasets, we have to keep in mind that this introduces multicollinearity, which can be an issue for certain methods (for instance, methods that require matrix inversion). If features are highly correlated, matrices are computationally difficult to invert, which can lead to numerically unstable estimates. To reduce the correlation among variables, we can simply remove one feature column from the one-hot encoded array. . Providing the class label array y as an argument to stratify ensures that both training and test datasets have the same class proportions as the original dataset. . Instead of discarding the allocated test data after model training and evaluation, it is a common practice to retrain a classifier on the entire dataset, as it can improve the predictive performance of the model. . Feature Scaling . Feature scaling is a crucial step in our preprocessing pipeline that can easily be forgotten. Decision trees and random forests are two of the very few machine learning algorithms where we don’t need to worry about feature scaling. Those algorithms are scale invariant. However, the majority of machine learning and optimization algorithms behave much better if features are on the same scale . There are two common approaches to bringing different features onto the same scale: normalization and standardization . Standardization can be more practical for many machine learning algorithms, especially for optimization algorithms such as gradient descent. The reason is that many linear models initialize the weights to 0 or small random values close to 0. Using standardization, we center the feature columns at mean 0 with standard deviation 1 so that the feature columns have the same parameters as a standard normal distribution (zero mean and unit variance), which makes it easier to learn the weights. . Standardization maintains useful information about outliers and makes the algorithm less sensitive to them in contrast to min-max scaling, which scales the data to a limited range of values . We fit the StandardScaler class only once—on the training data—and use those parameters to transform the test dataset or any new data point . The RobustScaler is especially helpful and recommended if we are working with small datasets that contain many outliers. If the machine learning algorithm applied to this dataset is prone to overfitting , the RobustScaler can be a good choice . Overfitting means the model fits the parameters too closely with regard to the particular observations in the training dataset, but does not generalize well to new data; we say that the model has a high variance . The reason for the overfitting is that our model is too complex for the given training data. . Common ways to reduce overfitting by regularization and dimensionality reduction via feature selection, which leads to simpler models by requiring fewer parameters to be fitted to the data. . Regularization . L1 regularization usually yields sparse feature vectors and most feature weights will be zero. Sparsity can be useful in practice if we have a high-dimensional dataset with many features that are irrelevant, especially in cases where we have more irrelevant dimensions than training examples. . | L2 regularization adds a penalty term to the cost function that effectively results in less extreme weight values compared to a model trained with an unregularized cost function. . | . There are two main categories of dimensionality reduction techniques: feature selection and feature extraction . Via feature selection, we select a subset of the original features, whereas in feature extraction, we derive information from the feature set to construct a new feature subspace. . Greedy algorithms make locally optimal choices at each stage of a combinatorial search problem and generally yield a suboptimal solution to the problem, in contrast to exhaustive search algorithms , which evaluate all possible combinations and are guaranteed to find the optimal solution . By reducing the number of features, we shrank the size of the dataset, which can be useful in real-world applications that may involve expensive data collection steps. Also, by substantially reducing the number of features, we obtain simpler models, which are easier to interpret. . Random Forest feature importance . Using a random forest, we can measure the feature importance as the averaged impurity decrease computed from all decision trees in the forest, without making any assumptions about whether our data is linearly separable or not. . As far as interpretability is concerned, the random forest technique comes with an important gotcha that is worth mentioning. If two or more features are highly correlated, one feature may be ranked very highly while the information on the other feature(s) may not be fully captured. . SelectFromModel object that selects features based on a user-specified threshold after model fitting, which is useful if we want to use the RandomForestClassifier as a feature selector and intermediate step in a scikit-learn Pipeline object, . Ch5. Compressing Data via Dimensionality Reduction . Feature Extraction . Feature extraction can be understood as an approach to data compression with the goal of maintaining most of the relevant information. . Feature extraction is not only used to improve storage space or the computational efficiency of the learning algorithm, but can also improve the predictive performance by reducing the curse of dimensionality —especially if we are working with non-regularized models. . PCA . PCA aims to find the directions of maximum variance in high-dimensional data and projects the data onto a new subspace with equal or fewer dimensions than the original one . Even if the input features are correlated, the resulting principal components will be mutually orthogonal (uncorrelated). . PCA directions are highly sensitive to data scaling, and we need to standardize the features prior to PCA if the features were measured on different scales and we want to assign equal importance to all features . LDA . The general concept behind LDA is very similar to PCA, but whereas PCA attempts to find the orthogonal component axes of maximum variance in a dataset, the goal in LDA is to find the feature subspace that optimizes class separability . Kernel PCA . If we are dealing with nonlinear problems, which we may encounter rather frequently in real-world applications, linear transformation techniques for dimensionality reduction, such as PCA and LDA, may not be the best choice. . Using the kernel trick, we can compute the similarity between two high-dimension feature vectors in the original feature space . Ch6. Learning Best Practices for Model Evaluation and Hyperparameter Tuning . We have to reuse the parameters that were obtained during the fitting of the training data to scale and compress any new data, such as the examples in the separate test dataset . Combining transformers and estimators in a pipeline . There is no limit to the number of intermediate steps in a pipeline; however, the last pipeline element has to be an estimator. . If we reuse the same test dataset over and over again during model selection, it will become part of our training data and thus the model will be more likely to overfit. Despite this issue, many people still use the test dataset for model selection, which is not a good machine learning practice . Using k-fold cross-validation to assess model performance . A better way of using the holdout method for model selection is to separate the data into three parts: a training dataset, a validation dataset, and a test dataset. The training dataset is used to fit the different models, and the performance on the validation dataset is then used for the model selection. The advantage of having a test dataset that the model hasn’t seen before during the training and model selection steps is that we can obtain a less biased estimate of its ability to generalize to new data. . A disadvantage of the holdout method is that the performance estimate may be very sensitive to how we partition the training dataset into the training and validation subsets; the estimate will vary for different examples of the data . K-fold cross-validation . In k-fold cross-validation, we randomly split the training dataset into k folds without replacement, where k – 1 folds are used for the model training, and one fold is used for performance evaluation. This procedure is repeated k times so that we obtain k models and performance estimates. . We use k-fold cross-validation for model tuning, that is, finding the optimal hyperparameter values that yield a satisfying generalization performance, which is estimated from evaluating the model performance on the test folds. . Once we have found satisfactory hyperparameter values, we can retrain the model on the complete training dataset and obtain a final performance estimate using the independent test dataset. The rationale behind fitting a model to the whole training dataset after k-fold cross-validation is that providing more training examples to a learning algorithm usually results in a more accurate and robust model. . Since k-fold cross-validation is a resampling technique without replacement, the advantage of this approach is that each example will be used for training and validation (as part of a test fold) exactly once, which yields a lower-variance estimate of the model performance than the holdout method. . Choosing K . A good standard value for k in k-fold cross-validation is 10, as empirical evidence shows. For instance, experiments by Ron Kohavi on various real-world datasets suggest that 10-fold cross-validation offers the best tradeoff between bias and variance ( A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection , Kohavi, Ron , International Joint Conference on Artificial Intelligence (IJCAI) , 14 (12): 1137-43, 1995 ). . Small training sets -&gt; increase the number of folds. If we increase the value of k , more training data will be used in each iteration, which results in a lower pessimistic bias toward estimating the generalization performance by averaging the individual model estimates. . Large datasets -&gt; smaller value for k , for example, k = 5, and still obtain an accurate estimate of the average performance of the model while reducing the computational cost of refitting and evaluating the model on the different folds . Stratified cross-validation . A slight improvement over the standard k-fold cross-validation approach is stratified k-fold cross-validation, which can yield better bias and variance estimates, especially in cases of unequal class proportions . By plotting the model training and validation accuracies as functions of the training dataset size, we can easily detect whether the model suffers from high variance or high bias, and whether the collection of more data could help to address this problem . Bias x Variance . High bias: This model has both low training and cross-validation accuracy, which indicates that it underfits the training data. Common ways to address this issue are to increase the number of parameters of the model, for example, by collecting or constructing additional features, or by decreasing the degree of regularization . High variance: which is indicated by the large gap between the training and cross-validation accuracy. To address this problem of overfitting, we can collect more training data, reduce the complexity of the model, or increase the regularization parameter . For unregularized models, it can also help to decrease the number of features via feature selection . While collecting more training data usually tends to decrease the chance of overfitting, it may not always help, for example, if the training data is extremely noisy or the model is already very close to optimal. . Debugging algorithms with learning and validation curves . Validation curves are a useful tool for improving the performance of a model by addressing issues such as overfitting or underfitting. Validation curves are related to learning curves, but instead of plotting the training and test accuracies as functions of the sample size, we vary the values of the model parameters . Fine-tuning machine learning models . The grid search approach is quite simple: it’s a brute-force exhaustive search paradigm where we specify a list of values for different hyperparameters, and the computer evaluates the model performance for each combination to obtain the optimal combination of values from this set . Randomized search usually performs about as well as grid search but is much more cost- and time-effective. In particular, if we only sample 60 parameter combinations via randomized search, we already have a 95 percent probability of obtaining solutions within 5 percent of the optimal performance ( Random search for hyper-parameter optimization . Bergstra J , Bengio Y . Journal of Machine Learning Research . pp. 281-305, 2012). . Algorithm selection with nested cross-validation . If we want to select among different machine learning algorithms, though, another recommended approach is nested cross-validation. In a nice study on the bias in error estimation, Sudhir Varma and Richard Simon concluded that the true error of the estimate is almost unbiased relative to the test dataset when nested cross-validation is used ( Bias in Error Estimation When Using Cross-Validation for Model Selection , BMC Bioinformatics , S. Varma and R. Simon , 7(1): 91, 2006 ). . In nested cross-validation, we have an outer k-fold cross-validation loop to split the data into training and test folds, and an inner loop is used to select the model using k-fold cross-validation on the training fold. After model selection, the test fold is then used to evaluate the model performance . Looking at different performance evaluation metrics . A confusion matrix is simply a square matrix that reports the counts of the true positive ( TP ), true negative ( TN ), false positive ( FP ), and false negative ( FN ) predictions of a classifier . The true positive rate ( TPR ) and false positive rate ( FPR ) are performance metrics that are especially useful for imbalanced class problems . Receiver operating characteristic ( ROC ) graphs are useful tools to select models for classification based on their performance with respect to the FPR and TPR, which are computed by shifting the decision threshold of the classifier. The diagonal of a ROC graph can be interpreted as random guessing , and classification models that fall below the diagonal are considered as worse than random guessing. A perfect classifier would fall into the top-left corner of the graph with a TPR of 1 and an FPR of 0. Based on the ROC curve, we can then compute the so-called ROC area under the curve ( ROC AUC ) to characterize the performance of a classification model. . Scoring metrics for multiclass classification . Micro-averaging is useful if we want to weight each instance or prediction equally, whereas macro-averaging weights all classes equally to evaluate the overall performance of a classifier with regard to the most frequent class labels. If we are using binary performance metrics to evaluate multiclass classification models in scikit-learn, a normalized or weighted variant of the macro-average is used by default . Dealing with class imbalance . Class imbalance is a quite common problem when working with real-world data—examples from one class or multiple classes are over-represented in a dataset . The algorithm implicitly learns a model that optimizes the predictions based on the most abundant class in the dataset, in order to minimize the cost or maximize the reward during training. . One way to deal with imbalanced class proportions during model fitting is to assign a larger penalty to wrong predictions on the minority class. . Other popular strategies for dealing with class imbalance include upsampling the minority class, downsampling the majority class, and the generation of synthetic training examples. Unfortunately, there’s no universally best solution or technique that works best across different problem domains. Thus, in practice, it is recommended to try out different strategies on a given problem, evaluate the results, and choose the technique that seems most appropriate . SMOTE . Synthetic Minority Over-sampling Technique ( SMOTE ), and you can learn more about this technique in the original research article by Nitesh Chawla and others: SMOTE: Synthetic Minority Over-sampling Technique , Journal of Artificial Intelligence Research , 16: 321-357, 2002 . It is also highly recommended to check out imbalanced-learn , a Python library that is entirely focused on imbalanced datasets, including an implementation of SMOTE . Ch7. Combining different models for Ensemble Learning . The goal of ensemble methods is to combine different classifiers into a meta-classifier that has better generalization performance than each individual classifier alone. . Voting . Majority voting simply means that we select the class label that has been predicted by the majority of classifiers, that is, received more than 50 percent of the votes. . To predict a class label via simple majority or plurality voting, we can combine the predicted class labels of each individual classifier, , and select the class label, , that received the most votes . Stacking . The stacking algorithm can be understood as a two-level ensemble, where the first level consists of individual classifiers that feed their predictions to the second level, where another classifier (typically logistic regression) is fit to the level-one classifier predictions to make the final predictions. The stacking algorithm has been described in more detail by David H. Wolpert in Stacked generalization , Neural Networks , 5(2):241–259, 1992 . . Bagging . Instead of using the same training dataset to fit the individual classifiers in the ensemble, we draw bootstrap samples (random samples with replacement) from the initial training dataset, which is why bagging is also known as bootstrap aggregating . . Random forests are a special case of bagging where we also use random feature subsets when fitting the individual decision trees. . Bagging was first proposed by Leo Breiman in a technical report in 1994; he also showed that bagging can improve the accuracy of unstable models and decrease the degree of overfitting. . Bagging algorithm can be an effective approach to reducing the variance of a model. However, bagging is ineffective in reducing model bias, that is, models that are too simple to capture the trend in the data well. This is why we want to perform bagging on an ensemble of classifiers with low bias, for example, unpruned decision trees. . Boosting . In boosting, the ensemble consists of very simple base classifiers, also often referred to as weak learners , which often only have a slight performance advantage over random guessing—a typical example of a weak learner is a decision tree stump. . The key concept behind boosting is to focus on training examples that are hard to classify, that is, to let the weak learners subsequently learn from misclassified training examples to improve the performance of the ensemble. . In contrast to bagging, the initial formulation of the boosting algorithm uses random subsets of training examples drawn from the training dataset without replacement . Boosting can lead to a decrease in bias as well as variance compared to bagging models . Boosting algorithms such as AdaBoost are also known for their high variance, that is, the tendency to overfit the training data . It is worth noting that ensemble learning increases the computational complexity compared to individual classifiers. In practice, we need to think carefully about whether we want to pay the price of increased computational costs for an often relatively modest improvement in predictive performance. An often-cited example of this tradeoff is the famous $1 million Netflix Prize , which was won using ensemble techniques. The details about the algorithm were published in The BigChaos Solution to the Netflix Grand Prize by A. Toescher , M. Jahrer , and R. M. Bell , Netflix Prize documentation , 2009 . Gradient Boosting . Another popular variant of boosting is gradient boosting . AdaBoost and gradient boosting share the main overall concept: boosting weak learners (such as decision tree stumps) to strong learners. The two approaches, adaptive and gradient boosting, differ mainly with regard to how the weights are updated and how the (weak) classifiers are combined . XGBoost . XGBoost, which is essentially a computationally efficient implementation of the original gradient boost algorithm ( XGBoost: A scalable tree boosting system . Tianqi Chen and Carlos Guestrin . Proceeding of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . ACM 2016 , pp. 785-794) . HistGradientBoosting . Scikit-learn now also includes a substantially faster version of gradient boosting in version 0.21, HistGradientBoostingClassifier , which is even faster than XGBoost . Ensemble methods combine different classification models to cancel out their individual weaknesses, which often results in stable and well-performing models that are very attractive for industrial applications as well as machine learning competitions. . Ch8. Applying Machine Learning to Sentiment Analysis . Sentiment analysis, sometimes also called opinion mining , is a popular subdiscipline of the broader field of NLP; it is concerned with analyzing the polarity of documents. A popular task in sentiment analysis is the classification of documents based on the expressed opinions or emotions of the authors with regard to a particular topic. . To visualize the progress and estimated time until completion, use the Python Progress Indicator ( PyPrind , https://pypi.python.org/pypi/PyPrind/ ) . Bag-of-words . The idea behind bag-of-words is quite simple and can be summarized as follows: We create a vocabulary of unique tokens—for example, words—from the entire set of documents. We construct a feature vector from each document that contains the counts of how often each word occurs in the particular document. . To construct a bag-of-words model based on the word counts in the respective documents, we can use the CountVectorizer class implemented in scikit-learn . Values in the feature vectors are also called the raw term frequencies : tf ( t , d )—the number of times a term, t , occurs in a document, d . It should be noted that, in the bag-of-words model, the word or term order in a sentence or document does not matter. The order in which the term frequencies appear in the feature vector is derived from the vocabulary indices, which are usually assigned alphabetically. . N-grams . The sequence of items in the bag-of-words model that we just created is also called the 1-gram or unigram model—each item or token in the vocabulary represents a single word. More generally, contiguous sequences of items in NLP—words, letters, or symbols—are also called n-grams . TF-IDF . Frequently occurring words typically don’t contain useful or discriminatory information . Term frequency-inverse document frequency ( tf-idf ), which can be used to downweight these frequently occurring words in the feature vectors. The tf-idf can be defined as the product of the term frequency and the inverse document frequency . Cleaning text . The first important step—before we build our bag-of-words model—is to clean the text data by stripping it of all unwanted characters. . One way to tokenize documents is to split them into individual words by splitting the cleaned documents at their whitespace characters . Stemming . In the context of tokenization, another useful technique is word stemming , which is the process of transforming a word into its root form. It allows us to map related words to the same stem . The Porter stemming algorithm is probably the oldest and simplest stemming algorithm. Other popular stemming algorithms include the newer Snowball stemmer (Porter2 or English stemmer) and the Lancaster stemmer (Paice/Husk stemmer) . Lemmatization . While stemming can create non-real words, such as ‘thu’ (from ‘thus’ ), as shown in the previous example, a technique called lemmatization aims to obtain the canonical (grammatically correct) forms of individual words—the so-called lemmas . . Lemmatization is computationally more difficult and expensive compared to stemming and, in practice, it has been observed that stemming and lemmatization have little impact on the performance of text classification . Stop-word removal . Stop-words are simply those words that are extremely common in all sorts of texts and probably bear no (or only a little) useful information that can be used to distinguish between different classes of documents. Examples of stop-words are is , and , has , and like . Removing stop-words can be useful if we are working with raw or normalized term frequencies rather than tf-idfs, which are already downweighting frequently occurring words. . TfidfVectorizer , which combines CountVectorizer with the TfidfTransformer . Naive Bayes Classifier . A still very popular classifier for text classification is the naïve Bayes classifier, which gained popularity in applications of email spam filtering. Naïve Bayes classifiers are easy to implement, computationally efficient, and tend to perform particularly well on relatively small datasets . Out-of-core learning . Out-of-core learning allows us to work with large datasets by fitting the classifier incrementally on smaller batches of a dataset. . Unfortunately, we can’t use CountVectorizer for out-of-core learning since it requires holding the complete vocabulary in memory. Also, TfidfVectorizer needs to keep all the feature vectors of the training dataset in memory to calculate the inverse document frequencies. However, another useful vectorizer for text processing implemented in scikit-learn is HashingVectorizer . HashingVectorizer is data-independent . Out-of-core learning is very memory efficient . word2vec . A more modern alternative to the bag-of-words model is word2vec , an algorithm that Google released in 2013 ( Efficient Estimation of Word Representations in Vector Space , T. Mikolov , K. Chen , G. Corrado , and J. Dean , arXiv preprint arXiv:1301.3781, 2013 ). The word2vec algorithm is an unsupervised learning algorithm based on neural networks that attempts to automatically learn the relationship between words. The idea behind word2vec is to put words that have similar meanings into similar clusters, and via clever vector-spacing, the model can reproduce certain words using simple vector math, for example, king – man + woman = queen . . Topic Modeling . Topic modeling describes the broad task of assigning topics to unlabeled text documents. For example, a typical application would be the categorization of documents in a large text corpus of newspaper articles. In applications of topic modeling, we then aim to assign category labels to those articles, for example, sports, finance, world news, politics, local news, and so forth . Latent Dirichlet Allocation ( LDA ) . LDA is a generative probabilistic model that tries to find groups of words that appear frequently together across different documents . We must define the number of topics beforehand—the number of topics is a hyperparameter of LDA that has to be specified manually. . The scikit-learn library’s implementation of LDA uses the expectation-maximization ( EM ) algorithm to update its parameter estimates iteratively . Ch9. Embedding a Machine Learning Model into a Web Application . Serializing fitted scikit-learn estimators . One option for model persistence: Python’s in-built pickle module | protocol=4 to choose the latest and most efficient pickle protocol | joblib: lib, more efficient way to serialize NumPy arrays | . Pickle can be a security risk: not secured against malicious code. Pickle was designed to serialize arbitraty objects, the unpickling process will execute code that has been stored in a pickle file . SQLite . SQLite database can be understood as a single, self-contained database file that allows us to directly access storage files . free DB browser for SQLite app (https://sqlitebrowser.org/dl/) -&gt; nice GUI for working with SQLite databases . Flask . Written in Python, provides a convenient interface for embedding existing Python code . Jinja2 . Web templates . PythonAnywhere . Lets us run a single web application free of charge . Ch10. Predicting Continuous Target Variables with Regression Analysis . Linear Regression . Regression Line: best-fitting line . Offsets/Residuals: vertical lines from the regression line to the training examples -&gt; errors of our prediction . Visualizing the important characteristics of a dataset . Scatterplot matrix: pair-wise correlations between the different features -&gt; scatterplotmatrix on MLxtend (https://github.com/rasbt/mlxtend) | . Training a linear regression model does not require that the explanatory or target variables are normally distributed -&gt; only requirement for certain statistics and hypothesis tests . Looking at relationships using a correlation matrix . Correlation matrix: square matrix that contains the Pearson product-moment correlation coefficient (Pearson’s r) -&gt; linear dependence between pairs of features | Correlation coefficients are in range -1 to 1. 1 -&gt; perfect positive correlation, 0 -&gt; no correlation and -1: perfect negative correlation | . To fit a linear regression model, we are interested in those features that have a high correlation with our target variable . Estimating the coefficient of a regression model via scikit-learn . The linear regression implementation in scikit-learn works better with unstandardized variables . Fitting a robust regression model using RANSAC . Linear regression models can be heavily impacted by the presence of outliers. In certain situations, a very small subset of our data can have a big effect on the estimated model coefficients . | As an alternative to throwing out outliers, we will look at a robust method of regression using the RANdom SAmple Consensus (RANSAC) algorithm, which fits a regression model to a subset of the data, the so-called inliers . | . Evaluating the performance of linear regression models . Plot the residuals (the differences or vertical distances between the actual and predicted values) versus the predicted values to diagnose our regression model. | Residual plots are a commonly used graphical tool for diagnosing regression models. They can help to detect nonlinearity and outliers, and check whether the errors are randomly distributed | . Good regression model: errors randomly distributed and the residuals randomly scattered around the centerline . MSE: useful for comparing differente regression models or for tuning their parameters via grid search and cross-validation . | Rˆ2: coefficient of determination. Standardized version of the MSE -&gt; better interpretability of the model’s performance. Rˆ2 is the fraction of response variance that is captured by the model . | . Using regularized methods for regression . Regularization is one approach to tackling the problem of overfitting by adding additional information, and thereby shrinking the parameter values of the model to induce a penalty against complexity. . | The most popular approaches to regularized linear regression are the so-called Ridge Regression, least absolute shrinkage and selection operator (LASSO), and elastic Net . | . Saturation of a model occurs if the number of training examples is equal to the number of features, which is a form of overparameterization. As a consequence, a saturated model can always fit the training data perfectly but is merely a form of interpolation and thus is not expected to generalize well . Dealing with nonlinear relationships using random forests . In the context of decision tree regression, the MSE is often referred to as within-node variance, which is why the splitting criterion is also better known as variance reduction . If the distribution of the residuals does not seem to be completely random around the zero center point -&gt; the model was not able to capture all the exploratory information . The error of the predictions should not be related to any of the information contained in the explanatory variables; rather, it should reflect the randomness of the real-world distributions or patterns. If we find patterns in the prediction errors, for example, by inspecting the residual plot, it means that the residual plots contain predictive information . | Improve the model by transforming variables, tuning the hyperparameters of the learning algorithm, choosing simpler or more complex models, removing outliers, or including additional variables . | . Ch11. Working with Unlabeled Data - Clustering Analysis . Clustering: find natural grouping in data -&gt; items in the same cluster are more similar to each other than to those from different clusters . Grouping objects by similarity using k-means . k-means . belongs to the category of prototype-based clustering -&gt; each cluster is represented by a prototype (centroid for average or medoid for most representative) | good at identifying clustrs with a spherical shape | drawback -&gt; have to specify number of clusters, k, a priori | Make sure that the features are measured on the same scale -&gt; apply z-score standardization or min-max scaling | clusters do not overlap and are not hierarchical | assume that there is at least one item in each cluster | . k-means++ . smarter way of placing the inital cluster centroids | . Hard versus soft clustering . Hard clustering: each example assigned to exactly one cluster. (e.g., k-means) . Soft clustering (fuzzy clustering): assign an example to one or more clusters (e.g., fuzzy C-means = FCM = soft k-means = fuzzy k-means) . Both k-means and FCM produce very similar clustering outputs . Using the elbow method to find the optimal number of clusters . withing-cluster SSE (distortion) -&gt; inertia_ attribute after fitting KMeans using scikit-learn | elbow method: identify the value of k where the distortion begins to increase most rapidly | . Quantifying the quality of clustering via silhouette plots . silhouette coefficient: range -1 to 1. 0 if the cluster separation and cohesion are equal. 1 (ideal) if how dissimilar an example is from other cluster » how similar it is to the other examples in its own cluster | silhouette plot with visibly different lengths and widths -&gt; evidence of bad or at least suboptimal clustering | . Organizing clusters as a hierarchical tree . allows us to plot dendograms | do not need to specify the number of clusters upfront | can be agglomerative (starts with every point as a cluster) or divisive (starts with one cluster and split iteratively) | . Grouping clusters in a bottom-up fashion . Algorithms for agglomerative hierarchical clustering: . single linkage | complete linkage | average linkage | Ward’s linkage | . Dendograms are often used in combination with a heat map -&gt; represent individual values in the data array or matrix containing our training examples with a color code . Locating regions of high density via DBSCAN . DBSCAN -&gt; density-based spatial clustering of applications with noise . no assumptions about spherical clusters | no partition of the data into hierarchies that require a manual cut-off point | assigns cluster labels based on dense regions of points -&gt; density: number of points within a specified radius | doesn’t necessarily assign each point to a cluster -&gt; is capable of removing noise points | two hyperparameters to be optimized to yield good results -&gt; MinPts and eta | . Disadvantage of the 3 algorithms presented: increasing number of features assuming fixed number of training examples -&gt; curse of dimensionality increases . It is common practice to apply dimensionality reduction techniques prior to performing clustering -&gt; PCA or Kernel-PCA . Also common to compress data down to two-dimensional subspaces -&gt; visualization helps evaluating the results . Graph-based clustering: not covered in the book. e.g, spectral clustering . Ch12. Implementing a Multilayer Artificial Neural Network from Scratch . Single-layer naming convention: Adaline consists of two layers, one input, and one output. It is called single-layer network because of its single link between the input and output layers . Introducing the multilayer neural network architecture . Adding additional hidden layers: the error gradients, become increasingly small as more layers are added to a network -&gt; vanishing gradient problem . Activating a neural network via forward propagation . Forward propagate the patterns of training data to generate an output | Calculate the error to minimize using a cost function between outputs and targets | Backpropagate the error, find its derivative wrt each weight in the network, update the model | Multiple epochs of 1-3, then forward propagation to calculate the output and apply a threshold function to obtain the predicted class labels | MLP: typical example of a feedforward ANN -&gt; each layer serves as the input to the next layer without loops . Gradient-based optimization is much more stable under normalized inputs (ranging from -1 to 1). Also, Batch Normalization for improving convergence . Efficient method of save multidimensional NumPy arrays to disk -&gt; NumPy’s savez/savez_compressed function -&gt; analogous to pickle, but optimized for np arrays. To load: np.load(file.npz) . Training (deep) NN is expensive -&gt; stop it early in certain conditions, e.g., starts overfitting, not improving . Common tricks to improve performance: . Skip-connections | Learning rate schedulers | Attaching loss functions to earlier layers | . Developing your understanding of backpropagation . Very computationally efficient approach to compute the partial derivatives of a complex cost function in multilayer NNs -&gt; Goal: use those derivatives to learn the weight coefficients for parametrizing such a multilayer artificial NN. . Backpropagation is a special case of a reverse-mode automatic differentiation. Matrix-vector multiplication is computationally much cheaper than matrix-matrix multiplication . About the convergence in neural networks . The output function has a rough surface and the optimization algorithm can easily become trapped in local minima . By increasing the learning rate, we can more readily escape such local minima. But, we cal also increase the chance of over-shooting the global optimum if the learning rate is too large . Ch13. Parallelizing Neural Network Training with TensorFlow . Ch14. Going Deeper - The Mechanics of TensorFlow . Ch15. Classifying Images with Deep Convolutional Neural Networks . Ch16. Modeling Sequential Data Using Recurrent Neural Networks . Ch17. Generative Adversarial Networks for Synthesizing New Data . Ch18. Reinforcement Learning for Decision Making in Complex Environments .",
            "url": "https://millengustavo.github.io/blog/book/machine%20learning/data%20science/2019/12/26/python-ml.html",
            "relUrl": "/book/machine%20learning/data%20science/2019/12/26/python-ml.html",
            "date": " • Dec 26, 2019"
        }
        
    
  
    
        ,"post16": {
            "title": "Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow",
            "content": "My notes and highlights on the book. . Author: Aurélien Geron . Part I, The Fundamentals of Machine Learning . CH1. The Machine Learning Landscape . Machine Learning is great for: . Problems for which existing solutions require a lot of fine-tuning or long lists of rules: one Machine Learning algorithm can often simplify code and perform better than the traditional approach. | Complex problems for which using a traditional approach yields no good solution: the best Machine Learning techniques can perhaps find a solution. | Fluctuating environments: a Machine Learning system can adapt to new data. | Getting insights about complex problems and large amounts of data. | . Broad categories of ML systems: . Trained with human supervision? (supervised, unsupervised, semisupervised, and Reinforcement Learning) | Can learn incrementally on the fly? (online versus batch learning) | Whether they work by simply comparing new data points to known data points, or instead by detecting patterns in the training data and building a predictive model, much like scientists do (instance-based versus model-based learning) | . Supervised/Unsupervised Learning . Supervised: the training set you feed to the algorithm includes the desired solutions (labels). e.g.: k-NN, Linear Regression, Logistic Regression, SVM, Decision Trees, Random Forests, Neural networks | Unsupervised: the training data is unlabeled. e.g.: K-Means, DBSCAN, HCA, One-class SVM, Isolation Forest, PCA, Kernel PCA, LLE, t-SNE, Apriori, Eclat | . Tip: It’s a good idea to reduce the dimension of your training data before feeding to another ML algorithm (e.g. supervised). Run faster, use less disk/memory, may perform better . Semisupervised: data is partially labeled. e.g.: deep belief networks (DBNs) are based on unsupervised components called restricted Boltzmann machines (RBMs) | Reinforcement: agent, rewards, penalties and policy | . Batch and Online Learning . Batch: incapable of learning incrementally, must be trained using all the available data (offline learning). This process can be automated, but the training process can take many hours/resources | Online: train incrementally by feeding data sequentially (individually or mini-batches). Out-of-core: loads part of the data, train, repeat until has run on all of the data (usually done offline, so online~incremental). Learning rate: high = rapid adaption, quickly forget old data; low = more inertia, learn slowly, less sensitive to noise. | . Instance-Based x Model-Based Learning . Instance-based: learns the examples by heart, generalize by using similarity measures | Model-based: build a model of the examples and use that model to make predictions | . Data versus algorithms (2001 Microsoft researchers paper): “these results suggest that we may want to reconsider the trade-off between spending time and money on algorithm development versus spending it on corpus development.” . Nonrepresentative Training Data . It is crucial that your training data be representative of the new cases you want to generalize to . Small sample: high chance of sampling noise | Large sample: if sampling method is flawed = sampling bias | . Poor-Quality Data . Training data full of errors, outliers and noise (e.g. poor-quality measurements) -&gt; often worth the effort of cleaning the training data. . Outliers: discard or fix the errors manually may help | Missing: ignore the attribute, the instances, fill the missing values, train one model with the feature and one without it | . Irrelevant Features . Feature engineering: . Feature selection: most useful features | Feature extraction: combining existing features to produce a more useful one (e.g. dimensionality reduction) | Creating new features by gathering new data | . Overfitting the Training Data . The model performs well on the training data, but it does not generalize well . noisy training set | small training set (sampling noise) | . Overfitting due to model being too complex relative to the amount and noise of the training data. Solutions: . Simplify the model (fewer parameters), reduce number of attributes, constrain the model (regularization) | Gather more training data | Reduce noise (e.g. fix data errors, remove outliers) | . Underfitting the Training Data . Model is too simple to learn the underlying structure of the data. . Solutions: . Select a more powerful model, with more parameters | Feed better features to the learning algorithm (feature engineering) | Reduce the constraints of the model (e.g. reduce the regularization hyperparameter) | . Testing and Validating . Split your data into two sets: training and test. The error rate is called generalization error (out-of-sample error), by evaluating your model on the test set, you get an estimate of this error . Training error low, but generalization error high -&gt; model is overfitting the training data . Hyperparameter Tuning and Model Selection . Holdout validation: hold out part of the training set to evaluate several candidate models and select the best one. The held-out set is called validation set (or development set, or dev set). After the validation process, you train the best model on the full training set (including the validation set), and this gives you the final model. Lastly, you evaluate this final model on the test set to get an estimate of the generalization error . If the validation set is too small, model evaluations will be imprecise -&gt; suboptimal model by mistake. . If the validation set is too large, the remaining training set will be much smaller than the full training set. . Solution: perform repeated cross-validation, using many validation sets. Each model is evaluated once per validation set after it is trained on the rest of the data. By averaging out all the evaluations of a model, you have a more accurate measure of its performance (but… more training time). . Data Mismatch . The validation set and test set must be as representative as possible of the data you expect to use in production. . If this happens, hold out some training data in another set -&gt; train-dev set. If after the model trained on the training set performs well on the train-dev, then the model is not overfitting. If it performs poorly on the validation set, it’s probably a data mismatch problem. Conversely, if the model performs poorly on the train-dev set, it must have overfitted the training set. . NO FREE LUNCH THEOREM: If you make absolutely no assumption about the data, then there is no reason to prefer one model over any other. There is no model that is a priori guaranteed to work better A model is a simplified version of the observations. The simplifications are meant to discard the superfluous details that are unlikely to generalize to new instances . CH2. End-to-End Machine Learning Project . This chapter presents an example ml project: . Look at the big picture | Pull out your ML project checklist . Frame the problem: what exactly the business objective is. How does the company expect to use and benefit from this model? . PIPELINES: Sequence of data processing components = data pipeline. Each component is fairly self-contained . What the current solution looks like? | Frame the problem: supervised? classification/regression? batch/online? etc Select a Performance Measure: RMSE, MAE, accuracy, etc . Check the Assumptions (by you or others) . | . Get the data | Create an isolated environment . python3 -m pip install --user -U virtualenv virtualenv my_env source my_env/bin/activate # deactivate . Data Structure . # Pandas DataFrame methods .head() .info() .describe() .value_counts() %matplotlib inline .hist() . Create a Test Set (20% or less if the dataset is very large) . WARNING: before you look at the data any further, you need to create a test set, put it aside, and never look at it -&gt; avoid the data snooping bias python from sklearn.model_selection import train_test_split . train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42) . ### Option: Stratified sampling python from sklearn.model_selection import StratifiedShuffleSplit split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) for train_index, test_index in split.split(housing, housing[&quot;income_cat&quot;]): strat_train_set = housing.loc[train_index] strat_test_set = housing.loc[test_index] for set_ in (strat_train_set, strat_test_set): set_.drop(&quot;income_cat&quot;, axis=1, inplace=True) . Discover and visualize the data to gain insights | Put the test set aside and only explore the training set. If the training set is large, you may want to sample an exploration set . # make a copy to avoid harming the training set housing = strat_train_set.copy() . We are very good at spotting patterns in pictures . Look for Correlations . corr_matrix = housing.corr() # how each attribute correlates with one specific corr_matrix[&quot;specific_attr&quot;].sort_values(ascending=False) # alternative from pandas.plotting import scatter_matrix scatter_matrix(housing[attributes], figsize=(12, 8)) . WARNING: Correlation coefficient only measures linear correlation, it may completely miss out on non-linear relationships! . Experimenting with Attribute Combinations . Attributes with tail-heavy distribution? You may want to transform then (e.g. logarithm) . After engineering features, you may want to look at the correlations again to check if the features created are more correlated with the target. . This is an iterative process: get a prototype up and running, analyze its output, come back to this exploration step . Prepare the data for ML algorithms | Instead of doing this manually, you should write functions for this purpose: reproductibility, reuse in your live system, quickly try various transformations to see which combination works best . Data Cleaning . Missing values . # pandas methods .dropna() .drop() .fillna() . WARNING: if you choose to fill the missing values, save the value you computed to fill the training set. You will need it later to replace missing values in the test set. python from sklearn.impute import SimpleImputer . imputer = SimpleImputer(strategy=”median”) . the median can only be used on numerical attributes . imputer.fit(housing_num) X = imputer.transform(housing_num) . housing_num_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index) . ### Handling Text and Categorical Attributes - Ordinal Encoder python from sklearn.preprocessing import OrdinalEncoder ordinal_encoder = OrdinalEncoder() housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat) # problem: ML algorithms will assume two nearby values are more similar than two distant values. That may not be the case . One Hot Encoder from sklearn.preprocessing import OneHotEncoder cat_encoder = OneHotEncoder() housing_cat_1hot = cat_encoder.fit_transform(housing_cat) . TIP: attributes with large number of possible categories = large number of input features. You may want to replace the categorical input with useful numerical features related to it . | . Feature Scaling . With few exceptions, ML algorithms don’t perform well when the input numerical attributes have very different scales . Min-max scaling (normalization) -&gt; values are shifted and rescaled so that they end up ranging from 0 to 1. (x_i - min_x) / (max_x - min_x). MinMaxScaler on Scikit-Learn | Standardization (zero mean) -&gt; (x_i - mean_x) / std_x. Doesn’t bound values to a specific range, but is much less affected by outliers. StandardScaler on Scikit-Learn. | . Transformation Pipelines . Scikit-Learn provides the Pipeline class to help with sequences of transformations . from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler num_pipeline = Pipeline([ (&#39;imputer&#39;, SimpleImputer(strategy=&quot;median&quot;)), (&#39;attribs_adder&#39;, CombinedAttributesAdder()), (&#39;std_scaler&#39;, StandardScaler()), ]) housing_num_tr = num_pipeline.fit_transform(housing_num) . We have handled numerical and categorical columns separately, but Scikit-Learn has a single transformer able to handle all columns python from sklearn.compose import ColumnTransformer | . num_attribs = list(housing_num) cat_attribs = [“ocean_proximity”] . full_pipeline = ColumnTransformer([ (“num”, num_pipeline, num_attribs), (“cat”, OneHotEncoder(), cat_attribs), ]) . housing_prepared = full_pipeline.fit_transform(housing) . &gt; **Tip**: by default, the remaining columns will be dropped. To avoid this, you can specify &quot;passthrough&quot; 5. **Select a model and train it** ### Training and Evaluating on the Training Set python from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(housing_prepared, housing_labels) from sklearn.metrics import mean_squared_error housing_predictions = lin_reg.predict(housing_prepared) lin_mse = mean_squared_error(housing_labels, housing_predictions) lin_rmse = np.sqrt(lin_mse) . Better Evaluation Using Cross-Validation . from sklearn.model_selection import cross_val_score scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=&quot;neg_mean_squared_error&quot;, cv=10) tree_rmse_scores = np.sqrt(-scores) # cross_val_score expects a utility fn (greater is better) # rather than a cost fn (lower is better) . Try out many other models from various categories, without spending too much time tweaking the hyperparameters. The goal is to shortlist a few (two to five) promising models. . Tip: save the models you experiment python import joblib . joblib.dump(my_model, “my_model.pkl”) . and later . my_model_loaded = joblib.load(“my_model.pkl”) . 6. **Fine-tune your model** ### Grid Search python from sklearn.model_selection import GridSearchCV param_grid = [ {&#39;n_estimators&#39;: [3, 10, 30], &#39;max_features&#39;: [2, 4, 6, 8]}, {&#39;bootstrap&#39;: [False], &#39;n_estimators&#39;: [3, 10], &#39;max_features&#39;: [2, 3, 4]}, ] forest_reg = RandomForestRegressor() grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring=&#39;neg_mean_squared_error&#39;, return_train_score=True) grid_search.fit(housing_prepared, housing_labels) # and after grid_search.best_params_ grid_search.best_estimator_ . Randomized Search . When the hyperparameter search space is large, it is often preferable to use RandomizedSearchCV . Ensemble Methods . Another way to fine-tune your system is to try to combine the models that perform best. The group (ensemble) will often perform better than the best individual model . Analyze the Best Models and Their Errors . feature_importances = grid_search.best_estimator_.feature_importances_ &gt;&gt;&gt; extra_attribs = [&quot;rooms_per_hhold&quot;, &quot;pop_per_hhold&quot;, &quot;bedrooms_per_room&quot;] &gt;&gt;&gt; cat_encoder = full_pipeline.named_transformers_[&quot;cat&quot;] &gt;&gt;&gt; cat_one_hot_attribs = list(cat_encoder.categories_[0]) &gt;&gt;&gt; attributes = num_attribs + extra_attribs + cat_one_hot_attribs &gt;&gt;&gt; sorted(zip(feature_importances, attributes), reverse=True) [(0.3661589806181342, &#39;median_income&#39;), (0.1647809935615905, &#39;INLAND&#39;), (0.10879295677551573, &#39;pop_per_hhold&#39;), (0.07334423551601242, &#39;longitude&#39;), (0.0629090704826203, &#39;latitude&#39;), (0.05641917918195401, &#39;rooms_per_hhold&#39;), (0.05335107734767581, &#39;bedrooms_per_room&#39;), (0.041143798478729635, &#39;housing_median_age&#39;), (0.014874280890402767, &#39;population&#39;), (0.014672685420543237, &#39;total_rooms&#39;), (0.014257599323407807, &#39;households&#39;), (0.014106483453584102, &#39;total_bedrooms&#39;), (0.010311488326303787, &#39;&lt;1H OCEAN&#39;), (0.002856474637320158, &#39;NEAR OCEAN&#39;), (0.00196041559947807, &#39;NEAR BAY&#39;), (6.028038672736599e-05, &#39;ISLAND&#39;)] . You may want to try dropping less useful features or understand errors your system makes . Evaluate Your System on the Test Set . Get the predictors and the labels from your test set, run your full_pipeline to transform the data (call transform(), not fit_transform()), and evaluate the final model on the test set: . final_model = grid_search.best_estimator_ X_test = strat_test_set.drop(&quot;median_house_value&quot;, axis=1) y_test = strat_test_set[&quot;median_house_value&quot;].copy() X_test_prepared = full_pipeline.transform(X_test) final_predictions = final_model.predict(X_test_prepared) final_mse = mean_squared_error(y_test, final_predictions) final_rmse = np.sqrt(final_mse) # =&gt; evaluates to 47,730.2 . Computing a 95% confidence interval for the generalization . &gt;&gt; from scipy import stats &gt;&gt; confidence = 0.95 &gt;&gt; squared_errors = (final_predictions - y_test) ** 2 &gt;&gt; np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1, ... loc=squared_errors.mean(), ... scale=stats.sem(squared_errors))) ... array([45685.10470776, 49691.25001878]) . If you did a lot of hyperparameter tuning, the performance may be worse than your cv (slightly overfit to the training set). Resist the temptation to tweak the hyperparameters to make the numbers look good on the test set . Present your solution | Project prelaunch phase . What you have learned, what worked/did not, what assumptions were made, what your system’s limitations are | Document everything | Create nice presentations with clear visualizations and easy-to-remember statements | . Launch, monitor, and maintain your system Get your solution ready for production (e.g., polish the code, write documentation and tests, and so on). | Then you can deploy your model to your production environment. . One way to do this is to save the trained Scikit-Learn model (e.g., using joblib), including the full preprocessing and prediction pipeline, then load this trained model within your production environment and use it to make predictions by calling its predict() method. . But deployment is not the end of the story. You also need to write monitoring code to check your system’s live performance at regular intervals and trigger alerts when it drops . If the data keeps evolving, you will need to update your datasets and retrain your model regularly. You should probably automate the whole process as much as possible. Here are a few things you can automate: . Collect fresh data regularly and label it (e.g., using human raters). | Write a script to train the model and fine-tune the hyperparameters automatically. This script could run automatically, for example every day or every week, depending on your needs. | Write another script that will evaluate both the new model and the previous model on the updated test set, and deploy the model to production if the performance has not decreased (if it did, make sure you investigate why) | Evaluate the model’s input data quality (missing features, stdev drifts too far from the training set, new categories) | Keep backups of every model you create and have the process and tools in place to roll back to a previous model quickly | Keep backups of every version of the datasets too | . CH3. Classification . Some learning algorithms are sensitive to the order of the training instances, and they perform poorly if they get many similar instances in a row. Shuffling the dataset ensures that this won’t happen . Stochastic Gradient Descent (SGD) classifier . SGDClassifier on sklearn. Has the advantage of being capable of handling very large datasets efficiently. Deals with training instances independently, one at a time (suited for online learning) . Performance Measures . Measuring Accuracy using Cross-Validation . The snippet below does roughly the same thing as cross_val_score() from sklearn, but with stratified sampling . from sklearn.model_selection import StratifiedKFold from sklearn.base import clone skfolds = StratifiedKFold(n_splits=3, random_state=42) for train_index, test_index in skfolds.split(X_train, y_train_5): clone_clf = clone(sgd_clf) X_train_folds = X_train[train_index] y_train_folds = y_train_5[train_index] X_test_fold = X_train[test_index] y_test_fold = y_train_5[test_index] clone_clf.fit(X_train_folds, y_train_folds) y_pred = clone_clf.predict(X_test_fold) n_correct = sum(y_pred == y_test_fold) print(n_correct / len(y_pred)) # prints 0.9502, 0.96565, and 0.96495” . High accuracy can be deceiving if you are dealing with skewed datasets (i.e., when some classes are much more frequent than others) . Confusion Matrix . Count the number of times instances of class A are classified as class B . from sklearn.metrics import confusion_matrix confusion_matrix(y_train_5, y_train_pred) . Each row represents an actual class, while each column represents a predicted class . Precision . Accuracy of the positive predictions . precision = TP/(TP + FP) . TP is the number of true positives, and FP is the number of false positives . Recall or Sensitivity, or True Positive Rate (TPR) . Ratio of positive instances that are correctly detected by the classifier . recall = TP/(TP + FN) . FN is the number of false negatives . from sklearn.metrics import precision_score, recall_score precision_score(y_train_5, y_train_pred) recall_score(y_train_5, y_train_pred) . F1 Score . Harmonic mean of the precision and recall. The harmonic mean gives much more weight to low values, so the classifier will only get a high F1 score if both recall and precision are high . F1 = 2(precisionrecall)/(precision+recall) . from sklearn.metrics import f1_score f1_score(y_train_5, y_train_pred) . The F1 score favors classifiers that have similar precision and recall . Precision/Recall trade-off . Precision/recall trade-off: increasing precision reduces recall, and vice versa. e.g., videos safe for kids: prefer reject many good videos (low recall), but keeps only safe ones (high precision) . Scikit-Learn gives you access to the decision scores that it uses to make predicitions, .decision_function() method, which returns a score for each instance and then use any threshold you want to make predictions based on those scores . For RandomForestClassifier for example, the method to use is .predict_proba(), which returns an array conatining a row per instance and a column per class, each containing the probability that the given instance belongs to the given class. . y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method=&quot;decision_function&quot;) from sklearn.metrics import precision_recall_curve precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores) . You can plot precision_recall_vs_threshold and choose a good threshold for your project, or plot precision directly against recall (generally you select a precision/recall just before the drop in the plot) . # define threshold threshold_90_precision = thresholds[np.argmax(precisions &gt;= 0.90)] # make predictions y_train_pred_90 = (y_scores &gt;= threshold_90_precision) # check results &gt;&gt;&gt; precision_score(y_train_5, y_train_pred_90) 0.9000380083618396 &gt;&gt;&gt; recall_score(y_train_5, y_train_pred_90) 0.4368197749492714 . The ROC Curve . Receiver operating characteristic (ROC) curve. Plots the true positive rate (recall) against the false positive rate (FPR). The FPR is the ratio of negative instances that are incorrectly classified as positive. It is equal to 1 - true negative rate (TNR, or specificity) which is the ratio of negative instances that are correctly classified as negative . ROC curve plots sensitivity (recall) versus 1 - specificity (.roc_curve()) . The higher the recall (TPR), the more false positives (FPR) the classifier produces. The purely random classifier is the diagonal line in the plot, a good classifier stays as far away from that line as possible (toward the top-left corner) . Area under the curve (AUC) . A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5 (.roc_auc_score()) . You should prefer the PR curve whenever the positive class is rare or when you care more about the false positives than the false negatives. Otherwise, use the ROC curve. . Binary classifiers . Choose the appropriate metrics | Evaluate your classifiers using cross-validation | Select the precision/recall trade-off that fits your needs | Use ROC curves and ROC AUC scores to compare various models | Multiclass Classification . Some algorithms are not capable of handling multiple classes natively (e.g., Logistic Regression, SVM). For 10 classes you would train 10 binary classifiers and select the class whose classifier outputs the highest score. This is the one-versus-the-rest (OvR) strategy (also called one-versus-all) . One-versus-one (OvO) strategy: trains a binary classifier for every pair of digits. (N*(N-1))/2)classifiers! Good strategy for SVM that scales poorly with the size of the training set . Scikit-Learn detects when you try to use a binary classification algorithm for a multiclass classification task, and it automatically runs OvR or OvO, depending on the algorithm . Error Analysis . Analyzing the confusion matrix often gives you insights into ways to improve your classifier. . Multilabel Classification . Outputs multiple binary tags e.g., face recognition with Alice, Bob and Charlie; only Alice and Charlie in a picture -&gt; output [1, 0, 1] . Evaluate a multilabel classifier: One approach is to measure the F1 score for each individual label, then simply compute the average score . Multioutput Classification (multioutput-multiclass classification) . Generalization of multilabel classification where each label can be multiclass (i.e., it can have more than two possible values) . CH4. Training Models . Linear Regression . A linear model makes a prediction by simply computing a weighted sum of the input features, plus a constant called the bias term (also called the intercept term) . A closed-form solution, a mathematical equation that gives the result directly . Gradient Descent . Generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function. . It measures the local gradient of the error function with regard to the parameter vector θ, and it goes in the direction of descending gradient. Once the gradient is zero, you have reached a minimum! . The size of the steps, is determined by the learning rate hyperparameter. If the learning rate is too small, then the algorithm will have to go through many iterations to converge, which will take a long time. If the learning rate is too high, you might jump across the valley and end up on the other side, possibly even higher up than you were before. This might make the algorithm diverge, with larger and larger values, failing to find a good solution . The MSE cost function for a Linear Regression is a convex function: if you pick any two points on the curve, the line segment joining them never crosses the curve. This implies that there are no local minima, just one global minimum. It is also a continuous function with a slope that never changes abruptly. Consequence: Gradient Descent is guaranteed to approach arbitrarily close the global minimum (if you wait long enough and if the learning rate is not too high). . When using Gradient Descent, you should ensure that all features have a similar scale, or else it will take much longer to converge. . Batch vs Stochastic Gradient Descent . The main problem with Batch Gradient Descent is that it uses the whole training set to compute the gradients at every step -&gt; very slow when training set is large . Stochastic Gradient Descent picks a random instance in the training set at every step and computes the gradients based only on that single instance -&gt; algorithm much faster because it has very little data to manipulate at every iteration. Possible to train on huge training sets, since only one instance in memory at each iteration . Randomness is good to escape from local optima, but bad because it means that the algorithm can never settle at the minimum -&gt; solution to this dilemma is to gradually reduce the learning rate . Learning Curves . Learning curves typical of a model that’s underfitting: Both curves have reached a plateau; they are close and fairly high. . If your model is underfitting the training data, adding more training examples will not help. You need to use a more complex model or come up with better features . If there is a gap between the curves. This means that the model performs significantly better on the training data than on the validation data, which is the hallmark of an overfitting model. If you used a much larger training set, however, the two curves would continue to get closer -&gt; feed more training data until the validation error reaches the training error . Bias/Variance Trade-off . Bias: Error due to wrong assumptions. A high-bias model is most likely to underfit the training data | Variance: Error due to model’s excessive sensitivity to small variations in the training data. Model with many degrees of freedom is likely to have high variance and thus overfit the training data | Irreducible error: due to the noiseness of the data itself. The only way to reduce this part of the error is to clean up the data | . Increasing a model’s complexity will typically increase its variance and reduce its bias. Conversely, reducing a model’s complexity increases its bias and reduces its variance. This is why it is called a trade-off. . Ridge Regression (Tikhonov regularization) - L2 . Keep the models weights as small as possible. It is important to scale the data before performing Ridge Regression, as it is sensitive to the scale of the input features. This is true of most regularized models. . Note that the regularization term should only be added to the cost function during training. Once the model is trained, you want to use the unregularized performance measure to evaluate the model’s performance. . It is quite common for the cost function used during training to be different from the performance measure used for testing. Apart from regularization, another reason they might be different is that a good training cost function should have optimization-friendly derivatives, while the performance measure used for testing should be as close as possible to the final objective. . Lasso Regression - L1 . Least Absolute Shrinkage and Selection Operator Regression. Tends to eliminate the weights of the least important features . Elastic Net . Regularization term is a simple mix of both Ridge and Lasso’s regularization terms . When should you use plain Linear Regression (i.e., without any regularization), Ridge, Lasso, or Elastic Net? It is almost always preferable to have at least a little bit of regularization, so generally you should avoid plain Linear Regression. Ridge is a good default, but if you suspect that only a few features are useful, you should prefer Lasso or Elastic Net because they tend to reduce the useless features’ weights down to zero. In general, Elastic Net is preferred over Lasso because Lasso may behave erratically when the number of features is greater than the number of training instances or when several features are strongly correlated . Early Stopping . Another way to regularize iterative learning algorithms. Stop training as soon as the validation error reaches a minimum. “Beautiful free lunch”, Geoffrey Hinton . Logistic Regression (Logit Regression) . Estimate the probability that an instance belongs to a particular class. Greater than 50% -&gt; positive class, else negative class (binary classifier) . Logistic Regression cost function = log loss . logit(p) = ln(p/(1-p)) -&gt; also called log-odds . Softmax Regression (Multinomial Logistic Regression) . Computes a score for each class, then estimates the probability of each class by applying the softmax function (normalized exponential) to the scores . Cross entropy -&gt; frequently used to measure how well a set of estimated class probabilities matches the target classes (when k=2 -&gt; equivalent to log loss) . CH5. Support Vector Machines . Soon… . CH6. Decision Trees . from sklearn.tree import export_graphviz export_graphviz ( tree_clf , out_file = image_path ( &quot;iris_tree.dot&quot; ), feature_names = iris . feature_names [ 2 :], class_names = iris . target_names , rounded = True , filled = True ) . One of the many qualities of Decision Trees is that they require very little data preparation. In fact, they don’t require feature scaling or centering at all. . Scikit-Learn uses the CART algorithm, which produces only binary trees : nonleaf nodes always have two children (i.e., questions only have yes/no answers). However, other algorithms such as ID3 can produce Decision Trees with nodes that have more than two children. . White/Black box models . Decision Trees are intuitive, and their decisions are easy to interpret. Such models are often called white box models. In contrast, as we will see, Random Forests or neural networks are generally considered black box models . The CART algorithm is a greedy algorithm: it greedily searches for an optimum split at the top level, then repeats the process at each subsequent level. It does not check whether or not the split will lead to the lowest possible impurity several levels down. A greedy algorithm often produces a solution that’s reasonably good but not guaranteed to be optimal. . Making predictions requires traversing the Decision Tree from the root to a leaf. Decision Trees generally are approximately balanced, so traversing the Decision Tree requires going through roughly O (log 2 ( m )) nodes. 3 Since each node only requires checking the value of one feature, the overall prediction complexity is O (log 2 ( m )), independent of the number of features. So predictions are very fast, even when dealing with large training sets. . Gini impurity tends to isolate the most frequent class in its own branch of the tree, while entropy tends to produce slightly more balanced trees . Nonparametric model, not because it does not have any parameters but because the number of parameters is not determined prior to training, so the model structure is free to stick closely to the data. In contrast, a parametric model, such as a linear model, has a predetermined number of parameters, so its degree of freedom is limited, reducing the risk of overfitting (but increasing the risk of underfitting). . Increasing min_* hyperparameters or reducing max_* hyperparameters will regularize the model. . Pruning . Standard statistical tests, such as the χ 2 test (chi-squared test), are used to estimate the probability that the improvement is purely the result of chance (which is called the null hypothesis ). If this probability, called the p-value , is higher than a given threshold (typically 5%, controlled by a hyperparameter), then the node is considered unnecessary and its children are deleted . Decision Trees love orthogonal decision boundaries (all splits are perpendicular to an axis), which makes them sensitive to training set rotation. . One way to limit this problem is to use Principal Component Analysis, which often results in a better orientation of the training data. . Problems . The main issue with Decision Trees is that they are very sensitive to small variations in the training data. . Random Forests can limit this instability by averaging predictions over many trees . CH7. Ensemble Learning and Random Forests . Wisdom of the crowd: aggregated answer is better than an expert’s answer. . A group of predictors is called an ensemble ; thus, this technique is called Ensemble Learning , and an Ensemble Learning algorithm is called an Ensemble method . Hard voting . Train a group of Decision Tree classifiers, each on a different random subset of the training set. To make predictions, you obtain the predictions of all the individual trees, then predict the class that gets the most votes . Very simple way to create an even better classifier is to aggregate the predictions of each classifier and predict the class that gets the most votes. This majority-vote classifier is called a hard voting classifier . Even if each classifier is a weak learner (meaning it does only slightly better than random guessing), the ensemble can still be a strong learner (achieving high accuracy), provided there are a sufficient number of weak learners and they are sufficiently diverse. . Independent classifiers . Ensemble methods work best when the predictors are as independent from one another as possible. One way to get diverse classifiers is to train them using very different algorithms. This increases the chance that they will make very different types of errors, improving the ensemble’s accuracy. . from sklearn.ensemble import VotingClassifier . Soft voting . If all classifiers are able to estimate class probabilities (i.e., they all have a predict_proba() method), then you can tell Scikit-Learn to predict the class with the highest class probability, averaged over all the individual classifiers. This is called soft voting. It often achieves higher performance than hard voting because it gives more weight to highly confident votes. All you need to do is replace voting=”hard” with voting=”soft” and ensure that all classifiers can estimate class probabilities . Generally, the net result is that the ensemble has a similar bias but a lower variance than a single predictor trained on the original training set. . Bagging and Pasting . Scikit-Learn offers a simple API for both bagging and pasting with the BaggingClassifier class (or BaggingRegressor for regression). . Bootstrapping . Bootstrapping introduces a bit more diversity in the subsets that each predictor is trained on, so bagging ends up with a slightly higher bias than pasting; but the extra diversity also means that the predictors end up being less correlated, so the ensemble’s variance is reduced. Overall, bagging often results in better models, which explains why it is generally preferred . Out-of-bag evaluation . In Scikit-Learn, you can set oob_score=True when creating a BaggingClassifier to request an automatic oob evaluation after training . Random Patches and Random Subspaces . Sampling both training instances and features is called the Random Patches method. Keeping all training instances (by setting bootstrap=False and max_samples=1.0) but sampling features (by setting bootstrap_features to True and/or max_features to a value smaller than 1.0) is called the Random Subspaces method. Sampling features results in even more predictor diversity, trading a bit more bias for a lower variance. . Instead of building a BaggingClassifier and passing it a DecisionTreeClassifier , you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees . Forest of such extremely random trees is called an Extremely Randomized Trees ensemble (Extra-Trees). This technique trades more bias for a lower variance. Much faster to train. . It is hard to tell in advance whether a RandomForestClassifier will perform better or worse than an ExtraTreesClassifier . Generally, the only way to know is to try both and compare them using cross-validation (tuning the hyperparameters using grid search). . Looking at how much the tree nodes that use that feature reduce impurity on average (across all trees in the forest) . Boosting . Boosting (originally called hypothesis boosting ) refers to any Ensemble method that can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor . AdaBoost . One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor underfitted. This results in new predictors focusing more and more on the hard cases. This is the technique used by AdaBoost . . This sequential learning technique has some similarities with Gradient Descent, except that instead of tweaking a single predictor’s parameters to minimize a cost function, AdaBoost adds predictors to the ensemble, gradually making it better. . There is one important drawback to this sequential learning technique: it cannot be parallelized (or only partially), since each predictor can only be trained after the previous predictor has been trained and evaluated. As a result, it does not scale as well as bagging or pasting. . Gradient Boosting . Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the residual errors made by the previous predictor. . The learning_rate hyperparameter scales the contribution of each tree. If you set it to a low value, such as 0.1 , you will need more trees in the ensemble to fit the training set, but the predictions will usually generalize better. This is a regularization technique called shrinkage . . If subsample=0.25 , then each tree is trained on 25% of the training instances, selected randomly. As you can probably guess by now, this technique trades a higher bias for a lower variance. It also speeds up training considerably. This is called Stochastic Gradient Boosting . . XGBoost . Extreme Gradient Boosting. This package was initially developed by Tianqi Chen as part of the Distributed (Deep) Machine Learning Community (DMLC), and it aims to be extremely fast, scalable, and portable. In fact, XGBoost is often an important component of the winning entries in ML competitions. . Stacking . Stacking (short for stacked generalization ). 18 It is based on a simple idea: instead of using trivial functions (such as hard voting) to aggregate the predictions of all predictors in an ensemble, why don’t we train a model to perform this aggregation? . The final predictor (called a blender , or a meta learner ) . To train the blender, a common approach is to use a hold-out set . It is actually possible to train several different blenders this way (e.g., one using Linear Regression, another using Random Forest Regression), to get a whole layer of blenders. The trick is to split the training set into three subsets: the first one is used to train the first layer, the second one is used to create the training set used to train the second layer (using predictions made by the predictors of the first layer), and the third one is used to create the training set to train the third layer (using predictions made by the predictors of the second layer). Once this is done, we can make a prediction for a new instance by going through each layer sequentially . Brew . Scikit-Learn does not support stacking directly, but it is not too hard to roll out your own implementation (see the following exercises). Alternatively, you can use an open source implementation such as brew . . CH8. Dimensionality Reduction . In some cases, reducing the dimensionality of the training data may filter out some noise and unnecessary details and thus result in higher performance, but in general it won’t; it will just speed up training . It is also extremely useful for data visualization . The Curse of Dimensionality . High-dimensional datasets are at risk of being very sparse: most training instances are likely to be far away from each other. This also means that a new instance will likely be far away from any training instance, making predictions much less reliable than in lower dimensions, since they will be based on much larger extrapolations -&gt; the more dimensions the training set has, the greater the risk of overfitting it . The number of training instances required to reach a given density grows exponentially with the number of dimensions . Projection . Project every training instance perpendicularly onto this subspace . Manifold . Many dimensionality reduction algorithms work by modeling the manifold on which the training instances lie; this is called Manifold Learning. It relies on the manifold assumption, also called the manifold hypothesis, which holds that most real-world high-dimensional datasets lie close to a much lower-dimensional manifold. This assumption is very often empirically observed . PCA . Identifies the hyperplane that lies closest to the data, and then it projects the data onto it . Principal Components . For each principal component, PCA finds a zero-centered unit vector pointing in the direction of the PC. Since two opposing unit vectors lie on the same axis, the direction of the unit vectors returned by PCA is not stable: if you perturb the training set slightly and run PCA again, the unit vectors may point in the opposite direction as the original vectors . PCA assumes that the dataset is centered around the origin (Scikit-Learn take care of this) . Explained Variance Ratio . Instead of arbitrarily choosing the number of dimensions to reduce down to, it is simpler to choose the number of dimensions that add up to a sufficiently large portion of the variance (e.g., 95%). Unless, of course, you are reducing dimensionality for data visualization—in that case you will want to reduce the dimensionality down to 2 or 3 . It is possible to compress and decompress a dataset (with loss). The mean squared distance between the original data and the reconstructed data is called the reconstruction error . Kernel PCA . Kernel trick -&gt; math technique that implicity maps instances into a very high-dimensional space (feature space). A linear decision boundary in the high-dimensional feature space corresponds to a complex nonlinear decision boundary in the original space . Locally Linear Embedding (LLE) . Powerful nonlinear dimensionality reduction (NLDR) technique. Does not rely on projections, like the previous algorithms do. . LLE works by first measuring how each training instance linearly relates to its closest neighbors, and then looking for a low-dimensional representation of the training set where these local relationships are best preserved . Scale poorly to very large datasets . Other Dimensionality Reductions Techniques . Random Projections: project the data to lower-dimensional space using a random linear projection | Multidimensional Scaling (MDS): try to preserve the distances between the instances | Isomap: creates a graph by connecting each instance to its nearest neighbors (try to preserve the geodesic distances between the instances) | t-Distributed Stochastic Neighbor Embedding (t-SNE): try to keep similar instances close and dissimilar instances apart. Mostly used for visualization -&gt; clusters of instances in high-dimensional space | Linear Discriminant Analysis (LDA): classification algorithm -&gt; learns the most discriminative axes between the classes -&gt; can be used to define a hyperplane to project the data | . CH9. Unsupervised Learning Techniques . “If intelligence was a cake, unsupervised learning would be the cake, supervised learning would be the icing on the cake, and reinforcement learning would be the cherry on the cake” - Yann LeCun . Clustering . Identifying similar instances and assigning them to clusters, or groups of similar instances . Customer Segmentation: i.e., recommender systems to suggest X that other users in the same cluster enjoyed | Data Analysis | Dimensionality Reduction: Once a dataset has been clustered, it is usually possible to measure how well an instance fits into a cluster (affinity). Each instance’s feature vector x can then be replaced with the vector of its cluster affinities. | Anomaly Detection / Outlier Detection: Any instance that has a low affinity to all the clusters is likely to be an anomaly. Useful in detecting defects in manufacturing, or for fraud detection | Semi-supervised Learning: If you only have a few labels, you could perform clustering and propagate the labels to all the instances in the same cluster. This technique can greatly increase the number of labels available for a subsequent supervised learning algorithm, and thus improve its performance | Search Engines | Segment an image: By clustering pixels according to their color, then replacing each pixel’s color with the mean color of its cluster, it is possible to considerably reduce the number of different colors in the image -&gt; used in many object detection and tracking systems -&gt; makes it easier to detect the contour of each object | . Algorithms: . Instances centered around a particular point -&gt; centroid | Continuous regions of densely packed instances | Hierarchical, clusters of clusters | . K-Means . Sometimes referred to as LLoyd-Forgy . Voronoi tessellation/diagram/decomposition/partition: is a partition of a plane into regions close to each of a given set of objects . Hard clustering: assigning each instance to a single cluster. Soft clustering: give each instance a score per cluster (can be the distance between the instance and the centroid, or the affinity score such as the Guassian Radial Basis Function) . Place the centroids randomly (pick k instances at random and use their locations as centroids) | Label the instances | Update the centroids | Label the instances | Update the centroids | Repeat until the centroids stop moving | The algorithm is guaranteed to converge in a finite a number of steps (usually quite small). K-Means is generally one of the fastest clustering algorithms . K-Means++ . Introduced a smarter initialization step that tends to select centroids that are distant from one another -&gt; makes the algorithm much less likely to converge to a suboptimal solution . Accelerated K-Means and mini-batch K-Means . Accelerated -&gt; exploits the triangle inequality . Mini-batches -&gt; speeds up the algorithm by a factor of 3 or 4 -&gt; makes it possible to cluster huge datasets that do not fit in memory (MiniBatchKMeans in Scikit-Learn) . If the dataset does not fit in memory, the simplest option is to use the memmap class. Alternatively, you can pass one mini-batch at a time to the partial_fit() method, but this will require much more work, since you will need to perform multiple initializations and select the best one yourself. . Finding the optimal number of clusters . Plotting the inertia as a function of the number of clusters k, the curve often contains an inflexion point called the “elbow” . A more precise approach (but also more computationally expensive) is to use the silhouette score, which is the mean silhouette coefficient over all the instances. An instance’s silhouette coefficient is equal to (b – a) / max(a, b), where a is the mean distance to the other instances in the same cluster (i.e., the mean intra-cluster distance) and b is the mean nearest-cluster distance (i.e., the mean distance to the instances of the next closest cluster, defined as the one that minimizes b, excluding the instance’s own cluster). The silhouette coefficient can vary between –1 and +1. A coefficient close to +1 means that the instance is well inside its own cluster and far from other clusters, while a coefficient close to 0 means that it is close to a cluster boundary, and finally a coefficient close to –1 means that the instance may have been assigned to the wrong cluster. . Silhouette diagram: more informative visualization -&gt; plot every instance’s silhouette coefficient, sorted by the cluster they are assigned to and by the value of the coefficient . Limits of K-Means . Necessary to run several times to avoid suboptimal solutions | You need to specify the number of clusters | Does not behave well when the clusters have varying sizes, different densities or nonspherical shapes | . It is important to scale the input features before you run K-Means, or the clusters may be very stretched and K-Means will perform poorly. Scaling the features does not guarantee that all the clusters will be nice and spherical, but it generally improves things . NOTE: remember to check the book again, there are some useful practical examples on clustering for preprocessing, semi-supervised learning (label propagation) . Active learning (uncertainty sampling) . The model is trained on the labeled instances gathered so far, and this model is used to make predictions on all the unlabeled instances. | The instances for which the model is most uncertain (i.e., when its estimated probability is lowest) are given to the expert to be labeled. | You iterate this process until the performance improvement stops being worth the labeling effort. | DBSCAN . Defines clusters as continuous regions of high density. Works well if all the clusters are dense enough and if they are well separated by low-density regions . It is robust to outliers, and it has just two hyperparameters (eps and min_samples) . Other Clustering Algorithms . Agglomerative clustering | BIRCH: Balanced Iterative Reducing and Clustering using Hierarchies -&gt; designed specifically for very large datasets | Mean-Shift: computational complexity is O(m^2), not suited for large datasets | Affinity propagation: same problem as mean-shift | Spectral clustering: does not scale very well to large numbers of instances and it does not behave well when the clusters have very different sizes | . Gaussian Mixtures . A Gaussian mixture model (GMM) is a probabilistic model that assumes that the instances were generated from a mixture of several Gaussian distributions whose parameters are unknown. . GMM is a generative model -&gt; you can sample new instances from it . Anomaly Detection using Gaussian Mixtures . Using a GMM for anomaly detection is quite simple: any instance located in a low-density region can be considered an anomaly. You must define what density threshold you want to use. . A closely related task is novelty detection: it differs from anomaly detection in that the algorithm is assumed to be trained on a “clean” dataset, uncontaminated by outliers, whereas anomaly detection oes not make this assumption. Outlier detection is often used to clean up a dataset. . Selecting the Number of Clusters . Find the model that minimizes a theoretical information criterion -&gt; Bayesian information criterion (BIC) or the Akaike information criterion (AIC) . Bayesian Gaussian Mixture Models . Rather than manually searching for the optimal number of clusters, you can use the BayesianGaussianMixture class, which is capable of giving weights equal (or close) to zero to unnecessary clusters. Set the number of clusters n_components to a value that you have good reason to believe is greater than the optimal number of clusters (this assumes some minimal knowledge about the problem at hand), and the algorithm will eliminate the unnecessary clusters automatically. . GMM work great on clusters with ellipsoidal shapes, but if you try to fit a dataset with different shapes, you may have bad surprises . Other Algorithms for Anomaly and Novelty Detection . PCA (inverse_transform() method): If you compare the reconstruction error of a normal instance with the reconstruction error of an anomaly, the latter will usually be much larger. This is a simple and often quite efficient anomaly detection approach | Fast-MCD (minimum covariance determinant): Implemented by the EllipticEnvelope class, this algorithm is useful for outlier detection, in particular to clean up a dataset | Isolation Forest: efficient in high-dimensional datasets. Anomalies are usually far from other instances, so on average they tend to get isolated in fewer steps than normal instances | Local Outlier Factor (LOF): compares the density of instances around a given instance to the density around its neighbors | One-class SVM: better suited for novelty detection. Works great, especially with high-dimensional datasets, but like all SVMs it does not scale to large datasets | . Part II, Neural Networks and Deep Learning . CH10. Introduction to Artificial Neural Networks with Keras . ANNs are the very core of Deep Learning -&gt; versatile, powerful and scalable . Renewed interest in ANNs: . Huge quantity of data -&gt; ANNs frequently outperform other ML techniques (large and complex problems) | Increase in computing power -&gt; GPU cards and cloud computing | Tweaks to the training algorithms | Reach fairly close to global optimum | Virtuous circle of funding and progress | . Perceptron . Simple ANN architecture. Based on threshold logic unit (TLU), or linear threshold unit (LTU). . A Perceptron is simply composed of a single layer of TLUs, with each TLU connected to all the inputs. Fully conected layer / dense layer: when all the neurons in a layer are connected to every neuron in the previous layer. . Hebb’s rule / Hebbian learning: “Cells that fire together, wire together”, the connection weight between two neurons tends to increase when they fire simultaneously . The Perceptron learning algorithm strongly resembles Stochastic Gradient Descent. Contrary to Logistic Regression classifiers, Perceptrons do not output a class probability; rather, they make predictions based on a hard threshold . The Multilayer Perceptron and Backpropagation . Perceptrons are incapable of solving some trivial problems (e.g., XOR) -&gt; true of any linear classification model. This can be solved by stacking multiple Perceptrons -&gt; Multilayer Perceptron (MLP) . An MLP is composed of one (passthrough) input layer, one or more layers of TLUs, called hidden layers, and one final layer of TLUs called the output layer. The layers close to the input layer are usually called the lower layers, and the ones close to the outputs are usually called the upper layers. Every layer except the output layer includes a bias neuron and is fully connected to the next layer . Feedforward neural network (FNN): signal flows only in one direction (from the inputs to the outputs) . When an ANN contains a deep stack of hidden layers -&gt; Deep neural network (DNN) . Backpropagation . Training algorithm. It is Gradient Descent using and efficient technique for computing the gradients automatically: in just two passes through the network (one forward, one backward), the backpropagation algorithm is able to compute the gradient of the network’s error with regard to every single model parameter. It can find out how each connection weight and each bias term should be tweaked in order to reduce the error. Once it has these gradients, it just performs a regular Gradient Descent step, and the whole process is repeated until the network converges to the solution. . Automatically computing gradients is called automatic differentiation, or autodiff. There are various autodiff techniques, with different pros and cons. The one used by backpropagation is called reverse-mode autodiff. It is fast and precise, and is well suited when the function to differentiate has many variables (e.g., connection weights) and few outputs (e.g., one loss). . For each training instance, the backpropagation algorithm first makes a prediction (forward pass) and measures the error, then goes through each layer in reverse to measure the error contribution from each connection (reverse pass), and finally tweaks the connection weights to reduce the error (Gradient Descent step) . It is important to initialize all the hidden layers’ connection weights randomly, or else training will fail. For example, if you initialize all weights and biases to zero, then all neurons in a given layer will be perfectly identical, and thus backpropagation will affect them in exactly the same way, so they will remain identical . Activation functions . You need to have some nonlinearity between layers to solve very complex problems . Examples: . Logistic (sigmoid) function | Hyperbolic tangent (tanh) | Rectified Linear Unit (ReLU) -&gt; fast to compute, has become the default | . Regression MLP architecture . Hyperparameter - Typical value . input neurons - One per input feature (e.g., 28 x 28 = 784 for MNIST) . hidden layers - Depends on the problem, but typically 1 to 5 . neurons per hidden layer - Depends on the problem, but typically 10 to 100 . output neurons - 1 per prediction dimension . Hidden activation - ReLU (or SELU, see Chapter 11) . Output activation - None, or ReLU/softplus (if positive outputs) or logistic/tanh (if bounded outputs) . Loss function - MSE or MAE/Huber (if outliers) . Classification MLP architecture . Hyperparameter - Binary classification - Multilabel binary classification - Multiclass classification . Input and hidden layers - Same as regression - Same as regression - Same as regression . output neurons - 1 - 1 per label - 1 per class . Output layer activation - Logistic - Logistic - Softmax . Loss function - Cross entropy - Cross entropy - Cross entropy . Implementing MLPs with Keras . Tensorflow 2 is arguably just as simple as PyTorch, as it has adopted Keras as its official high-level API and its developers have greatly simplified and cleaned up the rest of the API . Since we are going to train the neural network using Gradient Descent, we must scale the input features . Creating a Sequential model . You can pass a list of layers when creating the Sequential model: . model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dense(300, activation=&quot;relu&quot;), keras.layers.Dense(100, activation=&quot;relu&quot;), keras.layers.Dense(10, activation=&quot;softmax&quot;) ]) . The model’s summary() method displays all the model’s layers, including each layer’s name (which is automatically generated unless you set it when creating the layer), its output shape (None means the batch size can be anything), and its number of parameters. The summary ends with the total number of parameters, including trainable and non-trainable parameters . Dense layers often have a lot of parameters. This gives the model quite a lot of flexibility to fit the training data, but it also means that the model runs the risk of overfitting, especially when you do not have a lot of training data . Compiling the model . After a model is created, you must call its compile() method to specify the loss function and the optimizer to use. Optionally, you can specify a list of extra metrics to compute during training and evaluation: . model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&quot;sgd&quot;, metrics=[&quot;accuracy&quot;]) . Training and evaluating the model . Now the model is ready to be trained. For this we simply need to call its fit() method: . &gt;&gt;&gt; history = model.fit(X_train, y_train, epochs=30, ... validation_data=(X_valid, y_valid)) . If the training set was very skewed, with some classes being overrepresented and others underrepresented, it would be useful to set the class_weight argument when calling the fit() method, which would give a larger weight to underrepresented classes and a lower weight to overrepresented classes. These weights would be used by Keras when computing the loss . The fit() method returns a History object containing the training parameters (history.params), the list of epochs it went through (history.epoch), and most importantly a dictionary (history.history) containing the loss and extra metrics it measured at the end of each epoch on the training set and on the validation set (if any) -&gt; use this dictionary to create a pandas DataFrame and call its plot() method to get the learning curves . When plotting the training curve, it should be shifted by half an epoch to the left . If you are not satisfied with the performance of your model, you should go back and tune the hyperparameters. The first one to check is the learning rate. If that doesn’t help, try another optimizer (and always retune the learning rate after changing any hyperparameter). If the performance is still not great, then try tuning model hyperparameters such as the number of layers, the number of neurons per layer, and the types of activation functions to use for each hidden layer. You can also try tuning other hyperparameters, such as the batch size (it can be set in the fit() method using the batch_size argument, which defaults to 32). . Building Complex Models Using the Functional API . Wide &amp; Deep neural network: nonsequential, connects all or part of the inputs directly to the output layer. Makes it possible for the NN to learn both deep patterns (using the deep path) and simple rules (through the short path). Regular MLP forces all the data to flow through the full stack of layers -&gt; simple patterns may end up being distorted by the sequence of transformations . input_ = keras.layers.Input(shape=X_train.shape[1:]) hidden1 = keras.layers.Dense(30, activation=&quot;relu&quot;)(input_) hidden2 = keras.layers.Dense(30, activation=&quot;relu&quot;)(hidden1) concat = keras.layers.Concatenate()([input_, hidden2]) output = keras.layers.Dense(1)(concat) model = keras.Model(inputs=[input_], outputs=[output]) . Using the Subclassing API to Build Dynamic Models . Sequential and Functional API are declarative: . model can easily be saved, clone, shared | structure can be displayed and analyzed | framework can infer shapes and check types (caught errors early) | easy to debug -&gt; static graph of layers | problem: it’s static, so dynamic behaviors like loops, varying shapes and conditional branching are not easy | . Subclassing API -&gt; imperative programming style . Simply subclass the Model class, crete the layers you need in the constructor, and use them to perform the computations you want in the call() method. . Great API for researches experimenting with new ideas . Cons: . model’s architecture is hidden within the call() method -&gt; cannot inspect, save, clone | Keras cannot check types and shapes ahead of time | . Unles you really need that extra flexibility, you should probably stick to the Sequential/Functional API . Saving and Restoring a Model . model = keras.layers.Sequential([...]) model.compile([...]) model.fit([...]) model.save(&quot;my_keras_model.h5&quot;) # loading the model model = keras.models.load_model(&quot;my_keras_model.h5&quot;) . Using Callbacks . The fit() method accepts a callbacks argument that lets you specify a list of objects that Keras will call at the start and end of training, at the start and end of each epoch, and even before and after processing each batch. For example, the ModelCheckpoint callback saves checkpoints of your model at regular intervals during training, by default at the end of each epoch: . [...] # build and compile the model checkpoint_cb = keras.callbacks.ModelCheckpoint(&quot;my_keras_model.h5&quot;) history = model.fit(X_train, y_train, epochs=10, callbacks=[checkpoint_cb]) . Moreover, if you use a validation set during training, you can set save_best_only=True when creating the ModelCheckpoint. In this case, it will only save your model when its performance on the validation set is the best so far. . early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True) history = model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[checkpoint_cb, early_stopping_cb]) . Using TensorBoard for Visualization . Interactive visualization tool: . view learning curves during training | compare learning curves between multiple runs | visualize the computation graph | analyze training statistics | view images generated by your model | visualize complex multidimensional data projected down to 3D and automatically clustered | . To use it, modify your program so that it outputs the data you want to visualize to special binary log files called event files. Each binary data record is called a summary . import os root_logdir = os.path.join(os.curdir, &quot;my_logs&quot;) def get_run_logdir(): import time run_id = time.strftime(&quot;run_%Y_%m_%d-%H_%M_%S&quot;) return os.path.join(root_logdir, run_id) run_logdir = get_run_logdir() # e.g., &#39;./my_logs/run_2019_06_07-15_15_22&#39; . The good news is that Keras provides a nice TensorBoard() callback: . [...] # Build and compile your model tensorboard_cb = keras.callbacks.TensorBoard(run_logdir) history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid), callbacks=[tensorboard_cb]) . You need to start the TensorBoard server: . $ tensorboard --logdir=./my_logs --port=6006 TensorBoard 2.0.0 at http://mycomputer.local:6006/ (Press CTRL+C to quit) . To use directly withing Jupyter . %load_ext tensorboard %tensorboard --logdir=./my_logs --port=6006 . Fine-Tuning Neural Network Hyperparameters . NN are flexible. Drawback: many hyperparameters to tweak . You can wrap the Keras models in objects that mimic regular Scikit-Learn algorithms -&gt; then use GridSearchCV or RandomizedSearchCV . from scipy.stats import reciprocal from sklearn.model_selection import RandomizedSearchCV param_distribs = { &quot;n_hidden&quot;: [0, 1, 2, 3], &quot;n_neurons&quot;: np.arange(1, 100), &quot;learning_rate&quot;: reciprocal(3e-4, 3e-2), } rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3) rnd_search_cv.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[keras.callbacks.EarlyStopping(patience=10)]) . There are many techniques to explore a search space much more efficiently than randomly -&gt; when a region of the space turns out to be good, it should be explored more: . Hyperopt: popular lib for optimizing over all sorts of complex search spaces | Hyperas, kopt, or Talos: libs for optimizing hyperparameters for Keras models | Keras Tuner: lib by Google for Keras models | Skicit-Optimize (skopt) | Spearmint: bayesian | Hyperband | Sklearn-Deep: evolutionary algorithms | . Hyperparameter tuning is still an active area of research, and evolutionary algorithms are making a comeback . Number of Hidden Layers . For complex problems, deep networks have a much higher parameter efficiency than shallow ones -&gt; they can model complex functions using exponentially fewer neurons than shallow nets, allowing them to reach much better performance with the same amount of training data. Ramp up the number of hidden layers until you start overfitting the training set . Transfer Learning: Instead of randomly initializing the weights and biases of the first few layers of the new neural network, you can initialize them to the values of the weights and biases of the lower layers of the first network. This way the network will not have to learn from scratch all the low-level structures that occur in most pictures; it will only have to learn the higher-level structures . Number of Neurons per Hidden Layer . The number of neurons in the input and output layers is determined by the type of input and output your task requires . Hidden layers: it used to be common to size them to form a pyramid, with fewer neurons at each layer -&gt; many low-level features can coalesce into far fewer high-level features -&gt; this practice has been largely abandoned -&gt; Using the same number of neurons in all hidden layers performs just as well in most cases or even better; plus, there is only one hyperparameter to tune, insted of one per layer . Depending on the dataset, first hidden layer bigger can be good . Pick a model with more layers and neurons than you actually need, then use early stopping or other regularization techniques to prevent overfitting -&gt; “Stretch pants” . Increasing the number of layers » increase the number of neurons per layer . Learning Rate, Batch Size, and Other Hyperparameters . Plot the loss as a function of the learning rate (using a log scale for the learning rate), you should see it dropping at first. But after a while, the learning rate will be too large, so the loss will shoot back up: the optimal learning rate will be a bit lower than the point at which the loss starts to climb (typically about 10 times lower than the turning point) . Benefit of using large batch sizes -&gt; GPUs can process them efficiently -&gt; use the largest batch size that can fit in GPU RAM . Try to use a large batch size, using learning rate warmup. If training is unstable or bad performance -&gt; try using a small batch size instead . ReLU activation function is a good default . The optimal learning rate depends on the other hyperparameters—especially the batch size—so if you modify any hyperparameter, make sure to update the learning rate as well. . CH11. Training Deep Neural Networks . The Vanishing/Exploding Gradients Problems . Vanishing gradients problem: gradients often geet smaller as the algorithm progresses down to the lower layers -&gt; Gradient Descent update leaves the lower layers’ connection weights virtually unchanged, and training never converges to a good solution . Exploding gradients problem: gradients can grow bigger until layers get insanely large weight updates and the algorithm diverges . More generally, DNNs suffer from unstable gradients, different layers may learn at widely different speeds . Glorot and He Initialization . Using Glorot initialization can speed up training considerably . ReLU actv fn and its variants, sometimes called He initialization . SELU actv fn should be used with LeCun initialization (with normal distribution) . Nonsaturating Activation Functions . Dying ReLUs: during training some neurons stops outputting anything other than 0. | leaky ReLU: small slope ensures that leaky ReLUs never die -&gt; always outperformed the strict ReLU actv fn | randomized leaky ReLU (RReLU): perform well, seemed to act as regularizer | parametric leaky ReLU (PReLU): strongly outperformed ReLU on large image datasets, overfit on smaller datasets | exponential linear unit (ELU): outperformed ReLU, faster convergence rate, despite being slower to compute | Scaled ELU (SELU): self-normalize the network (mean=0, std=1) -&gt; solves vanishing/exploding gradients. Significantly outperforms other actv fn, but need to be configured correcly (some assumptions for self-normalization). | . In general: SELU &gt; ELU &gt; leaky ReLU &gt; ReLU &gt; tanh &gt; logistic . Batch Normalization . Also help solve vanishing/exploding gradients problems. . Add an operation just before or after the actv fn of each hidden layer: zero-centers and normalizes each input, then scales and shifts the result using two new parameter vectors per layer: one for scaling, other for shifting -&gt; many cases if the BN layer as the very first of the NN, you do not need to standardize your training set . BN also acts like a regularizer, reducing the need for other regularization techniques . model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.BatchNormalization(), keras.layers.Dense(300, activation=&quot;elu&quot;, kernel_initializer=&quot;he_normal&quot;), keras.layers.BatchNormalization(), keras.layers.Dense(100, activation=&quot;elu&quot;, kernel_initializer=&quot;he_normal&quot;), keras.layers.BatchNormalization(), keras.layers.Dense(10, activation=&quot;softmax&quot;) ]) . BatchNormalization -&gt; one of the most-used layers in DNNs, often omitted in the diagrams, it is assumed BN is added after every layer . Gradient Clipping . Mitigate the exploding gradients problem. Clip the gradients during backpropagation, never exceed some threshold. . Most often used in RNNs (BN is trickier to use here) . Reusing Pretrained Layers . Transfer Learning: Not a good idea to train a very large DNN from scratch: find an existing NN that accomplishes a similar task. Speed up training, require significantly less training data . Transfer learning will work best when the inputs have similar low-level features (resize inputs to the size expected by the original model). The output layer should be replaced according to the new task . More similar tasks = more layers you want to reuse (starting with the lower layers) . Transfer Learning with Keras . Cloning a model with their weights . model_A_clone = keras.models.clone_model(model_A) model_A_clone.set_weights(model_A.get_weights()) . Freezing layers up until the last . for layer in model_B_on_A.layers[:-1]: layer.trainable = False # You must always compile the model after freezing/unfreezing layers model_B_on_A.compile(loss=&quot;binary_crossentropy&quot;, optimizer=&quot;sgd&quot;, metrics=[&quot;accuracy&quot;]) . Transfer Learning does not work very well with small dense networks. Works best with deep CNN -&gt; tend to learn feature detectors that are much more general . Unsupervised Pretraining . Often cheap to gather unlabeled training examples, but expensive to label them. You can try to use the unlabeled data to train an unsupervised model, such as an autoencoder or a generative adversarial network (GAN). Then reuse the lower layers of the autoencoder/GAN’s discriminator, add the output layer for your task and fine-tune the final network using supervised learning . Before -&gt; restricted Boltzmann machines (RBMs) for unsupervised learning . Self-supervised learning is when you automatically generate the labels from the data itself, then you train a model on the resulting “labeled” dataset using supervised learning techniques. Since this approach requires no human labeling whatsoever, it is best classified as a form of unsupervised learning . Faster Optimizers . Ways to speed up training: . initialization strategy for the weights | activation function | batch normalization | reusing parts of a pretrained network | faster optimizers than regular gradient descent | . Momentum Optimization . with momentum the system may oscillate before stabilizing -&gt; it’s good to have a bit of friction in the system | momentum value = 0.9 -&gt; usually works well in practice | . Nesterov Accelerated Gradient (NAG) . NAG ends up being significantly faster than regular momentum optimization | less oscillations and converges faster | nesterov=True | . AdaGrad . adaptive learning rate | efficient for simpler tasks such as Linear Regression | should NOT be used to train DNNs | . RMSProp . better than AdaGrad on more complex problems | . Adam and Nadam Optimization . Adam: adaptive moment estimation | requires less tuning of the learning rate | . Variations: . AdaMax: can be more stable than Adam in some datasets, try if experiencing problems with Adam | Nadam: Adam + Nesterov -&gt; often converge slightly faster than Adam | . The optimizers discussed rely on First-order partial derivatives (Jacobians). Second-order partial derivatives (Hessians) exists in literature, but are just too slow to compute (when they fit in memory!) . Training Sparse Models: to achieve fast model at runtime with less memory. Get rid of tiny weights. Apply strong L1 regularization during training. If still insufficient -&gt; Tensorflow Model Optimization Toolkit (TF-MOT) . Learning Rate Scheduling . Learning schedules -&gt; vary the lr during training . Power scheduling: lr drops at each step; first drops quickly, then more and more slowly | Exponential scheduling: slashs the lr by a factor of 10 every s steps | Piecewise constant scheduling: requires fiddling with the sequence of steps | Performance scheduling: measure validation error, reduce the lr when the error stops dropping | 1cycle scheduling: increases then decreases and plays with momentum -&gt; paper showing speed up in training and better performance in fewer epochs | . Avoiding Overfitting Through Regularization . early stopping is one of the best regularization techniques | batch normalization is very good too | . L1 and L2 Regularization . L2 -&gt; constrain NN’s weights | L1 -&gt; if you want a sparse model (many weights = 0) | . functools.partial() -&gt; create a thin wrapper for any callable, with some default arguments . from functools import partial RegularizedDense = partial(keras.layers.Dense, activation=&quot;elu&quot;, kernel_initializer=&quot;he_normal&quot;, kernel_regularizer=keras.regularizers.l2(0.01)) model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), RegularizedDense(300), RegularizedDense(100), RegularizedDense(10, activation=&quot;softmax&quot;, kernel_initializer=&quot;glorot_uniform&quot;) ]) . Dropout . One of the most popular regularization techniques for DNNs . at every training step, every neuron (only exception = output neurons) has a probability p (10-50%) of being temporarily ignored (dropped out) -&gt; may be active in the next step | . in practice, usually apply dropout to the neurons in the top one to three layers (excluding output layer) . dropout is only active during training -&gt; comparing training x validation loss can be misleading -&gt; make sure to evaluate the training loss without dropout (after training) . | many state of the art only use dropout after the last hidden layer . | . Monte Carlo (MC) Dropout . dropout networks have a profound connection with approximate Bayesian inference -&gt; solid math justification . | MC Dropout -&gt; boost the performance of any trained dropout model without having to retrain it or even modify it at all . | . y_probas = np.stack([model(X_test_scaled, training=True) for sample in range(100)]) y_proba = y_probas.mean(axis=0) . Averaging over multiple predictions with dropout on gives us a Monte Carlo estimate that is generally more reliable than the result of a single prediction with dropout off . Default DNN configuration . Hyperparameter - Default value . Kernel initializer - He initialization . Activation function - ELU . Normalization - None if shallow; Batch Norm if deep . Regularization - Early stopping (+ℓ2 reg. if needed) . Optimizer - Momentum optimization (or RMSProp or Nadam) . Learning rate schedule - 1cycle . DNN configuration for a self-normalizing net . Hyperparameter - Default value . Kernel initializer - LeCun initialization . Activation function - SELU . Normalization - None (self-normalization) . Regularization - Alpha dropout if needed . Optimizer - Momentum optimization (or RMSProp or Nadam) . Learning rate schedule - 1cycle . TIP: Refer back to the summary at the end of Chapter 11! . CH12. Custom Models and Training with TensorFlow . CH13. Loading and Preprocessing Data with TensorFlow . CH14. Deep Computer Vision Using Convolutional Neural Networks . CH15. Processing Sequences Using RNNs and CNNs . Recurrent Neurons and Layers . RNN -&gt; much like a feedforward NN, except it also has connections pointing backward. . Unrolling the network through time . Each neuron has two sets of weights: one for the inputs and other for the outputs of the previous time step . Memory Cells . Memory: the output of a recurrent neuron at time step t is a function of all the inputs from previous steps | Memory cell (or just cell): part of a NN that preserves some state across time steps. Capable of learning short patterns (about 10 steps long depending on the task) | . Input and Output Sequences . Sequence-to-sequence network: RNN that takes a sequence of inputs and produce a sequence of outputs. Useful for predicting time series | Sequence-to-vector network: feed the network a sequence of inputs and ignore all outputs except for the last one | Vector-to-sequence: feed the same input vector over and over again at each time step and let it output a sequence | Encoder (sequence-to-vector)-Decoder (vector-to-sequence): i.e., translation: feed the network a sentence in one language, the encoder convert into a single vector representation, and then the decoder decode the vector into a sentence in another language | . Training RNNs . Backpropagation through time (BPTT): forward pass throught the unrolled network, then the output sequence is evaluated using a cost function. The gradients of that cost function are then propagated backward through the unrolled network. Finally the model parameters are updated using the gradients computed during BPTT. | . The gradients flow backward through all the outputs used by the cost function, not just the final output. Since the same parameters W and b are used at each time step, backpropagation will do the right thing and sum over all time steps . Forecasting a Time Series . Time series: the input features are generally represented as 3D arrays of shape [batch size, time steps, dimensionallity], where dimensionallity is 1 for univariate time series and more for multivariate time series . Baseline Metrics . Naive forecasting: predict the last value in each series | Fully connected network | . Implementing a Simple RNN . model = keras.models.Sequential([ keras.layers.SimpleRNN(1, input_shape=[None, 1]) ]) . Trend and Seasonality: when using RNNs, it is generally not necessary to remove trend/seasonality before fitting, but it may improve performance in some cases, since the model will not have to learn the trend and the seasonality . Deep RNNs . model = keras.models.Sequential([ keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]), keras.layers.SimpleRNN(20), keras.layers.Dense(1) ]) . Forecasting Several Time Steps Ahead . First option, use the model already trained, make it predict the next value, then add that value to the inputs, and use the model again to predict the following value… Errors might accumulate | Second option, train an RNN to predict all 10 next values at once | . It may be surprising that the targets will contain values that appear in the inputs (there is a lot of overlap between X_train and Y_train). Isn’t that cheating? Fortunately, not at all: at each time step, the model only knows about past time steps, so it cannot look ahead. It is said to be a causal model. . model = keras.models.Sequential([ keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]), keras.layers.SimpleRNN(20, return_sequences=True), keras.layers.TimeDistributed(keras.layers.Dense(10)) ]) . Forecasting: often useful to have some error bars along with your predictions. Add an MC Dropout layer within each memory cell, dropping part of the inputs and hidden states. After training, to forecast a new time series, use the model many times and compute the mean and stdev of the predictions at each time step . Handling Long Sequences . Fighting the Unstable Gradients Problem . Good parameter initialization | Faster optimizers | Dropout | Saturating activation function: hyperbolic tangent | . Batch Normalization cannot be used as efficiently with RNNs -&gt; another form of normalization often works better: Layer Normalization -&gt; similar no BN, but instead of normalizing across the batch dimension, it normalizes across the features dimension . Tackling the Short-Term Memory Problem . Due to the transformations that the data goes through when traversing an RNN, some information is lost at each time step. After a while, the RNN’s state contains virtually no trace of the first inputs . LSTM (Long Short-Term Memory) cells . LSTM cell looks exactly like a regular cell, except that its state is split into two vectors: h(t) and c(t) (“c” stands for “cell”). You can think of h(t) as the short-term state and c(t) as the long-term state . The key idea is that the network can learn what to store in the long-term state, what to throw away, and what to read from it . LSTM cell can learn to recognize an important input (that’s the role of the input gate), store it in the long-term state, preserve it for as long as it is needed (that’s the role of the forget gate), and extract it whenever it is needed . GRU (Gated Recurrent Unit) cells . Simplified version of the LSTM cell that performs just as well . LSTM and GRU cells are one of the main reasons behind the success of RNNs. Yet while they can tackle much longer sequences than simple RNNs, they still have a fairly limited short-term memory, and they have a hard time learning long-term patterns in sequences of 100 time steps or more, such as audio samples, long time series, or long sentences. One way to solve this is to shorten the input sequences, for example using 1D convolutional layers . Using 1D convolutional layers to process sequences . A 1D convolutional layer slides several kernels across a sequence, producing a 1D feature map per kernel. Each kernel will learn to detect a single very short sequential pattern (no longer than the kernel size) . By shortening the sequences, the convolutional layer may help the GRU layers detect longer patterns . It is actually possible to use only 1D convolutional layers and drop the recurrent layers entirely . WaveNet . WaveNet: Stacked 1D convolutional layers, doubling the dilation rate (how spread apart each neuron’s inputs are) at every layer . Lower layers learn short-term patterns, while the higher layers learn long-term patterns. Thanks to the doubling dilation rate, the network can process extremely large sequences very efficiently . model = keras.models.Sequential() model.add(keras.layers.InputLayer(input_shape=[None, 1])) for rate in (1, 2, 4, 8) * 2: model.add(keras.layers.Conv1D(filters=20, kernel_size=2, padding=&quot;causal&quot;, activation=&quot;relu&quot;, dilation_rate=rate)) model.add(keras.layers.Conv1D(filters=10, kernel_size=1)) model.compile(loss=&quot;mse&quot;, optimizer=&quot;adam&quot;, metrics=[last_time_step_mse]) history = model.fit(X_train, Y_train, epochs=20, validation_data=(X_valid, Y_valid)) . CH16. Natural Language Processing with RNNs and Attention . character RNN: predict the next character in a sentence | stateless RNN: learns on random portions of text at each iteration, without any information on the rest of the text | stateful RNN: preserves the hidden state between training iterations and continues reading where it left off, allowing it to learn longer patterns | . Char-RNN: “The Unreasonable Effectiveness of Recurrent Neural Networks”, Andrej Karpathy . Character RNN . tokenizer = keras.preprocessing.text.Tokenizer(char_level=True) tokenizer.fit_on_texts([shakespeare_text]) . How to Split a Sequential Dataset . Assuming the time series is stationary -&gt; split across time . truncated backpropagation through time: RNN is unrolled over the length of every short substring of the whole text (window() method) | . n_steps = 100 -&gt; RNN will only be able to learn patterns shorter or equal to n_steps (100 in this case). Higher n_steps -&gt; harder to train! . Stateful RNN . shift = n_steps (instead of 1 like stateless RNN) when calling window() | we must obviously not call shuffle() method | harder to do batching | . After the stateful model is trained, it will only be possible to use it to make predictions for batches of the same size as were used during training. To avoid this restriction, create an identical stateless model, and copy the stateful model’s weights to this model. . Sentiment Analysis . Google’s SentencePiece: unsupervised learning technique to tokenize/detokenize text at the subword level in a language-independent way, treating spaces like other characters | Byte pair encoding | TF.Text -&gt; WordPiece | . Reusing Pretrained Embeddings . Tensorflow Hub project: model components called modules. Browse the TF Hub repository -&gt; copy the code example into your project -&gt; module will be downloaded, along with its pretrained weights, and included in your model . Warning: Not all TF Hub modules support TensorFlow 2 -&gt; check before . An Encoder-Decoder Network for Neural Machine Translation (NMT) . import tensorflow_addons as tfa encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32) decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32) sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32) embeddings = keras.layers.Embedding(vocab_size, embed_size) encoder_embeddings = embeddings(encoder_inputs) decoder_embeddings = embeddings(decoder_inputs) encoder = keras.layers.LSTM(512, return_state=True) encoder_outputs, state_h, state_c = encoder(encoder_embeddings) encoder_state = [state_h, state_c] sampler = tfa.seq2seq.sampler.TrainingSampler() decoder_cell = keras.layers.LSTMCell(512) output_layer = keras.layers.Dense(vocab_size) decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler, output_layer=output_layer) final_outputs, final_state, final_sequence_lengths = decoder( decoder_embeddings, initial_state=encoder_state, sequence_length=sequence_lengths) Y_proba = tf.nn.softmax(final_outputs.rnn_output) model = keras.Model(inputs=[encoder_inputs, decoder_inputs, sequence_lengths], outputs=[Y_proba]) . Bidirectional RNNs . Normally -&gt; only looks at past and present inputs before generating its outputs -&gt; it’s “causal” (cannot look into the future) -&gt; makes sense for forecasting time series . For many NLP tasks, often is preferable to look ahead at the next words -&gt; bidirectional recurrent layer (keras.layers.Bidirectional) . Beam Search . Keeps track of a short list of the k most promising sentences, and at each decoder step it tries to extend them by one word, keeping only the k most likely sentences. k = beam width . Attention Mechanisms . Allow the decoder to focus on the appropriate words (as encoded by the encoder) at each time step -&gt; the path from an input word to its translation is now much shorter, so the short-term memory limitations of RNNs have much less impact . Alignment model / attention layer: small neural network trained jointly with the rest of the Encoder-Decoder model . Bahdanau attention: concatenative/additive attention | Luong attention: multiplicative attention | . Visual Attention . Generate image captions using visual atention: a CNN processes the image and outputs some feature maps, then a decoded RNN with an attention mechanism generates the caption, one word at a time . Explainability: Attention mechanisms make it easier to understand what led the model to produce its output -&gt; especially useful when the model makes a mistake (check what the model focused on). Explainability may be a legal requirement in some applications, i.e., system deciding whether or not it should grant a loan . “Attention mechanisms are so powerful that you can actually build state-of-the-art models using only attention mechanisms” . Attention is All You Need: The Transformer Architecture . Google’s research -&gt; Transformer. Improved state of the art NMT without using recurrent or convolutional layers, just attention mechanisms. Also faster to train and easier to parallelize. . CH17. Representation Learning and Generative Learning Using Autoencoders and GANs . CH18. Reinforcement Learning . CH19. Training and Deploying TensorFlow Models at Scale .",
            "url": "https://millengustavo.github.io/blog/book/machine%20learning/data%20science/2019/12/24/handson-ml.html",
            "relUrl": "/book/machine%20learning/data%20science/2019/12/24/handson-ml.html",
            "date": " • Dec 24, 2019"
        }
        
    
  
    
        ,"post17": {
            "title": "DataSUS death records and Streamlit",
            "content": "In Brazil, more than 70% of the population depends only on the medical assistance provided by the government. The Brazilian public healthcare system is called SUS (Sistema Único de Saúde). . There is a public SUS data repository available online (DataSUS). Although the data is not always clean and complete, we can derive many insights from DataSUS. . In this post we are going to build and deploy a Streamlit application inspired on the Uber pickups example, but using DataSUS death records (2006-2017) and geographic coordinates from health facilities. . The code for this application is available here. . Downloading the data from DataSUS . SIM . From the DataSUS website we have the definition of SIM: . The Mortality Information System (SIM) was created by DATASUS for the regular collection of mortality data in the country. From the creation of the SIM it was possible to comprehensively capture mortality data to subsidize the various management spheres in public health. Based on this information it is possible to perform situation analysis, planning and evaluation of actions and programs in the area. . Let’s download the SIM data for the São Paulo state. The prefixes of the files are “DOSP”. . Downloading the data from the ftp . from ftplib import FTP ftp = FTP(&quot;ftp.datasus.gov.br&quot;) ftp.login() ftp.cwd(&quot;dissemin/publicos/SIM/CID10/DORES/&quot;) all_files = ftp.nlst(&quot;.&quot;) state_prefix = &quot;DOSP&quot; # We sort the list and keep only the last 12 records # This is because they share the same layout of the current data (2006-2017) files = sorted([file for file in all_files if state_prefix in file])[-12:] for file in files: print(&quot;Downloading {}...&quot;.format(file)) with open(file, &quot;wb&quot;) as fp: ftp.retrbinary(&quot;RETR {}&quot;.format(file), fp.write) . Renaming the files which the extension is capitalized . import os files = [file for file in os.listdir() if &quot;DOSP&quot; in file and &quot;.DBC&quot; in file] for file in files: os.rename(file, file[:-4] + &quot;.dbc&quot;) . Converting from .dbc to .csv . As you may have noticed, the files are in a .dbc format. This is a proprietary format of the SUS Department of Informatics (DATASUS). . A kind developer provided a tool to convert files from .dbc to .csv. To use this tool we will need to have git and docker installed. . Build the docker image . git clone https://github.com/greatjapa/dbc2csv.git cd dbc2csv docker build -t dbc2csv . . Convert the files . Navigate to the folder where you download the .dbc files. Copy the full path to the directory, you can get this path by running: pwd . | Run: docker run -it -v &lt;full_path_to_the_directory&gt;:/usr/src/app/data dbc2csv make . | A /csv folder will be populated with the converted files. | . CNES . From their website: . The National Register of Health Facilities (CNES) is a public document and official information system for registering information about all health facilities in the country, regardless of their legal nature or integration with the Unified Health System (SUS). . The process to download the data is simpler this time, they are already in a .zip file you can download from this link: . ftp://ftp.datasus.gov.br/cnes/BASE_DE_DADOS_CNES_201910.ZIP . We are going to use only one .csv file from this data: tbEstabelecimento201910.csv . Processing the data . Reading the facilities table with pandas . To be efficient, we will pass only the columns that matter to our application. . import pandas as pd cnes = pd.read_csv( &quot;tbEstabelecimento201910.csv&quot;, sep=&quot;;&quot;, usecols=[ &quot;CO_CNES&quot;, &quot;CO_CEP&quot;, &quot;NO_FANTASIA&quot;, &quot;NO_LOGRADOURO&quot;, &quot;CO_ESTADO_GESTOR&quot;, &quot;NU_LATITUDE&quot;, &quot;NU_LONGITUDE&quot;, ], ) . Filtering the data for the São Paulo state. . From the dictionary available on the DataSUS website we know that ‘35’ is the code for São Paulo. For this application we are only going to keep this data . cnes = cnes[cnes[&quot;CO_ESTADO_GESTOR&quot;]==35] . Merging with the death records . My converted .csv SIM files are in the path ../data/SIM/csv/, make sure you modify the path accordingly . files = sorted(os.listdir(&quot;../data/SIM/csv/&quot;)) dfs = [ pd.read_csv( &quot;../data/SIM/csv/&quot; + file, usecols=[&quot;NUMERODO&quot;, &quot;DTOBITO&quot;, &quot;HORAOBITO&quot;, &quot;CODESTAB&quot;], ) for file in files ] df = pd.concat(dfs) # We will drop the null CODESTABs (data without CNES code) df = df.dropna() . Before proceeding to fill the missing coordinates, join the CODESTAB with the CO_CNES, so we have fewer facilities to fill. . cnes = cnes.rename(columns={&quot;CO_CNES&quot;: &quot;CODESTAB&quot;}) merged = df.merge(cnes, on=&quot;CODESTAB&quot;) # Since we merged with the death records file, we have many duplicates, # let&#39;s drop it to see which facilities have coordinates missing unique_merged = merged[ [&quot;CODESTAB&quot;, &quot;CO_CEP&quot;, &quot;NU_LATITUDE&quot;, &quot;NU_LONGITUDE&quot;] ].drop_duplicates() # Filtering the data for only the records where the coordinates are missing missing_coords = unique_merged[unique_merged[&quot;NU_LATITUDE&quot;].isnull()] # The CEP was automatically converted to int and we have lost the first zero digit. # This line converts to string and pad with zero so we have a valid CEP missing_coords[&quot;CO_CEP&quot;] = ( missing_coords[&quot;CO_CEP&quot;].astype(str).apply(lambda x: x.zfill(8)) . We have 697 CEPs without coordinates, let’s try to fill them up. . Enriching the data from DataSUS with latitude and longitude (cep_to_coords) . The data we downloaded from DataSUS is not complete. Geographic coordinates of various health facilities are missing. While latitude and longitude are not present in all cases, we do have the Brazilian zip code (CEP) for some. . A quick search on Google for converting from CEP to latitude and longitude has shown that we had some scripts that mixed R and Python to achieve this task. . Investigating the scripts further, it became clear that it was simple and valuable to implement this in Python. So, I removed the dependency of R to achieve the same result with just Python https://github.com/millengustavo/cep_to_coords. . Install geocode from source . git clone https://github.com/millengustavo/cep_to_coords.git cd cep_to_coords git checkout master pip install -e . . The package usage is simple. Call the cep_to_coords function with a valid CEP string, it will search the correios API for an address, concatenate it with the city and country and hit an API to get the coordinates. . If you find it useful, please leave a star on Github. The project is still in its infancy, so it is a great opportunity to contribute to your first open source project adding features or refactoring the code! . Fill the coordinates . from cep_to_coords.geocode import cep_to_coords cep_column = &quot;CO_CEP&quot; unique_ceps = missing_coords[cep_column].unique() # cep_to_coords returns a [lat, lon] list if it finds the coordinates # else it returns [NaN, NaN] missing_coords[&quot;lat&quot;] = float(&quot;nan&quot;) missing_coords[&quot;lon&quot;] = float(&quot;nan&quot;) for ind, elem in enumerate(unique_ceps): try: coords = cep_to_coords(elem) missing_coords.loc[ind, &quot;lat&quot;] = coords[0] missing_coords.loc[ind, &quot;lon&quot;] = coords[1] except Exception as e: print(elem, coords, e) print(&quot;{}%...&quot;.format(ind * 100 / len(unique_ceps))) . Using the cep_to_coords function we were able to fill 78% of the missing coordinates! . Compiling the final CEP table . To complete the data preparation, we need to take our filled coordinates and replace the NaNs on the death records table. . unique_merged[&quot;CO_CEP&quot;] = ( unique_merged[&quot;CO_CEP&quot;].astype(str).apply(lambda x: x.zfill(8)) ) # unfortunately we didn&#39;t fill all coordinates, let&#39;s drop them missing_coords = missing_coords.drop(columns=[&quot;NU_LATITUDE&quot;, &quot;NU_LONGITUDE&quot;]).dropna() # joining the datasets full_table = unique_merged.merge(missing_coords, on=&quot;CO_CEP&quot;, how=&quot;left&quot;) # filling the missing data full_table[&quot;lat&quot;] = full_table.apply( lambda x: x[&quot;lat&quot;] if pd.isnull(x[&quot;NU_LATITUDE&quot;]) else x[&quot;NU_LATITUDE&quot;], axis=1 ) full_table[&quot;lon&quot;] = full_table.apply( lambda x: x[&quot;lon&quot;] if pd.isnull(x[&quot;NU_LONGITUDE&quot;]) else x[&quot;NU_LONGITUDE&quot;], axis=1 ) # compiling the CEP final table full_table = ( full_table.drop(columns=[&quot;NU_LATITUDE&quot;, &quot;NU_LONGITUDE&quot;, &quot;CODESTAB_y&quot;]) .dropna() .rename(columns={&quot;CODESTAB_x&quot;: &quot;CODESTAB&quot;}) .reset_index(drop=True) ) . Merging the facilities back to the death records dataframe and cleaning the data . df_enriched = df.merge(full_table, on=&quot;CODESTAB&quot;) df_enriched[&quot;HORAOBITO&quot;] = pd.to_numeric( df_enriched[&quot;HORAOBITO&quot;], downcast=&quot;integer&quot;, errors=&quot;coerce&quot; ) df_enriched = df_enriched.dropna() df_enriched[&quot;DTOBITO&quot;] = df_enriched[&quot;DTOBITO&quot;].astype(str).apply(lambda x: x.zfill(8)) df_enriched[&quot;HORAOBITO&quot;] = ( df_enriched[&quot;HORAOBITO&quot;].astype(int).astype(str).apply(lambda x: x.zfill(4)) ) # Creating a timestamp column with both date and hour of death df_enriched[&quot;DATA&quot;] = df_enriched[&quot;DTOBITO&quot;] + &quot; &quot; + df_enriched[&quot;HORAOBITO&quot;] df_enriched[&quot;DATA&quot;] = pd.to_datetime( df_enriched[&quot;DATA&quot;], format=&quot;%d%m%Y %H%M&quot;, errors=&quot;coerce&quot; ) df_enriched = df_enriched.dropna() df_enriched[&quot;NUMERODO&quot;] = df_enriched[&quot;NUMERODO&quot;].astype(str) df_enriched[&quot;lat&quot;] = ( df_enriched[&quot;lat&quot;].astype(str).str.replace(&quot;,&quot;, &quot;.&quot;, regex=False).astype(float) ) df_enriched[&quot;lon&quot;] = ( df_enriched[&quot;lon&quot;].astype(str).str.replace(&quot;,&quot;, &quot;.&quot;, regex=False).astype(float) ) . Saving to a .parquet file . df_enriched.to_parquet(&quot;../data/clean/dataset.parquet.gzip&quot;, compression=&quot;gzip&quot;, index=False) . Creating the app using the Uber pickups example . Streamlit according to the website is . “The fastest way to build custom ML tools”. . It is indeed a bold statement, but what sold me on it was the sentence on the subtitle: . “So you can stop spending time on frontend development and get back to what you do best.”. . For this experiment, we are going to spend even less time on frontend development by using an example gently posted by the Streamlit team https://github.com/streamlit/demo-uber-nyc-pickups. The demo presents the Uber pickups on New York City by hour. Our goal here is to replace pickups with deaths registered on SIM and New York City with the state of São Paulo. . There are only a few things we need to change in the code to adapt the application to our use. . Clone the repository . git clone https://github.com/streamlit/demo-uber-nyc-pickups.git cd demo-uber-nyc-pickups . Open app.py on your favorite text editor and change the following lines (commented here) . # OLD -&gt; DATE_TIME = &quot;date/time&quot; # NEW -&gt; DATE_TIME = &quot;data&quot; # OLD -&gt; data = pd.read_csv(DATA_URL, nrows=nrows) # NEW -&gt; data = pd.read_parquet(&quot;../data/clean/dataset.parquet.gzip&quot;) . For cosmetic purposes you may also change the title and other specific references . Install streamlit . pip install streamlit . Run the app . streamlit run app.py . . Voilà! This command will automatically open the app on your browser (port 8051 by default). . Conclusion . This is a very simple project that shows some amazing libraries that are being developed lately for Machine Learning applications. Although we didn’t used any complex techniques here, we covered an interesting part of what a data scientist do. Data ingestion, cleaning, enriching and finally visualization. . I hope you learned something from this and I encourage you to play around with Streamlit, it’s definitely amazing! . .",
            "url": "https://millengustavo.github.io/blog/data%20science/machine%20learning/health/data%20driven/2019/12/17/demo-datasus-streamlit.html",
            "relUrl": "/data%20science/machine%20learning/health/data%20driven/2019/12/17/demo-datasus-streamlit.html",
            "date": " • Dec 17, 2019"
        }
        
    
  
    
        ,"post18": {
            "title": "cep-to-coords: Pacote Python para converter CEP em coordenadas geográficas (latitude e longitude)",
            "content": "Pacote Python para transformar CEP em latitude e longitude . https://github.com/millengustavo/cep-to-coords . Como usar: . Instale: | git clone https://github.com/millengustavo/cep-to-coords.git cd cep-to-coords git checkout master pip install -e . . Converta um CEP para latitude e longitude: | from cep_to_coords.convert import cep_to_coords coordenadas = cep_to_coords(&#39;22070-900&#39;) print(coordenadas) # {&#39;latitude&#39;: -22.9864082, &#39;longitude&#39;: -43.189592} . Opcional - Usando o https://cepaberto.com/ . Para usar esse conversor você precisa se cadastrar no site e exportar seu token como variável de ambiente CEP_ABERTO_TOKEN . export CEP_ABERTO_TOKEN=&#39;seu_token&#39; . from cep_to_coords.convert import cep_to_coords from cep_to_coords.strategies import CEPAbertoConverter coordenadas = cep_to_coords(&#39;22070-900&#39;, factory=CEPAbertoConverter) print(coordenadas) # {&#39;latitude&#39;: -22.9864082, &#39;longitude&#39;: -43.189592} . Testando . python -m unittest discover .",
            "url": "https://millengustavo.github.io/blog/data%20science/package/python/2019/12/16/cep-to-coords.html",
            "relUrl": "/data%20science/package/python/2019/12/16/cep-to-coords.html",
            "date": " • Dec 16, 2019"
        }
        
    
  
    
        ,"post19": {
            "title": "Innovation and Operations",
            "content": "I was at Google’s meetup a few weeks ago about DevOps and SRE (Site Reliability Engineering). . . While these themes are not part of the essence of Data Science such as statistics, math, and programming, it is possible to draw relevant insights to technology in general. . In the term DevOps, there are morphologically two sectors: . Developers looking to implement new features quickly, which can mean innovation. | Operations, which ensure customers will be served consistently. | . While some (dev) want to create new features as quick as possible, others (ops) like what is there working, and have an aversion to uncertainty. . . Both views are extremely valid, and neither is more correct than the other. It is like a balance. For sectors to generate value consistently, the balance should not weigh heavily on either side. . Error Budget . Most technology service provision agreements have an SLA. An acceptable maximum delay for delivery of the service or product is set. . . This concept, in addition to guiding the quality of operations, provides an interesting insight into freedom of innovation. . Imagine that the service is being delivered at a consistency far above that agreed upon. For operations this is the perfect scenario, nothing fails, we have no errors. . But this view is not 100% correct if you think about business in the medium and long term. If there is room to make more mistakes, the business must be free to innovate. This is where developers come in and can spend their error budget on new products or enhancements. . . The error budget provides a metric that determines how unreliable the service is allowed to be. This metric removes the politics from negotiations when deciding how much risk to allow. . The main benefit of an error budget is that it provides a common incentive that allows both product development and SRE to focus on finding the right balance between innovation and reliability. . Key Insights from Google . Managing service reliability is largely about managing risk, and managing risk can be costly. | 100% is probably never the right reliability target: not only is it impossible to achieve, it’s typically more reliability than a service’s users want or notice. Match the profile of the service to the risk the business is willing to take. | An error budget aligns incentives and emphasizes joint ownership between SRE and product development. Error budgets make it easier to decide the rate of releases and to effectively defuse discussions about outages with stakeholders, and allows multiple teams to reach the same conclusion about production risk without rancor. | . References . SRE Book by Google - https://landing.google.com/sre/sre-book/chapters/embracing-risk/ | .",
            "url": "https://millengustavo.github.io/blog/sre/innovation/devops/development/operations/2019/09/15/innovation.html",
            "relUrl": "/sre/innovation/devops/development/operations/2019/09/15/innovation.html",
            "date": " • Sep 15, 2019"
        }
        
    
  
    
        ,"post20": {
            "title": "Storytelling with Data",
            "content": "Gustavo is a data scientist that is trying to grow a data driven culture in the company he works for. . Many hours of study of scientific method, statistics and programming have made him technically competent in his role. . For him, analyzing charts coming from his usual tools is trivial. However, Gustavo works with other amazing people in different roles from his. His goal of making the company data driven depends on their engagement. . To fulfill his goal, our hero embarks on an arduous journey to make his visualizations more assertive and explain his points simply and elegantly. . Make no mistake, often summarizing an idea to the essentials is a much more complex task than expected. . The above seems like the synopsis of a book, doesn’t it? That was the way I found it most interesting to engage the reader inspired by the chapter “The magic of story” from the book Storytelling with Data. . This post is my summary of interesting points from the book. As a three-act structure for plays, we had the setup and conflict already laid down. The following is my attempt of resolution. . Exploratory vs Explanatory . After undertaking an entire analysis, it can be tempting to want to show your audience everything, as evidence of all of the work you did and the robustness of the analysis (Exploratory). Resist this urge. Concentrate on the information your audience needs to know (Explanatory). . Being concise is often more challenging than being verbose. If you know exactly what it is you want to communicate, you can make it fit the time slot you’re given. . Understand the context . Build a clear understanding of who you are communicating to, what you need them to know or do, how you will communicate to them, and what data you have to back up your case. Employ concepts like the 3‐minute story, the Big Idea, and storyboarding to articulate your story and plan the desired content and flow. . The Big Idea . Must articulate your unique point of view; | Must convey what’s at stake; | Must be a complete sentence. | Choosing an effective visual . . Simple text: When you have a number or two that you want to communicate, think about using the numbers themselves. When you have more data that you want to show, generally a table or graph is the way to go. | Tables and Graphs: While tables interact with our verbal system, graphs interact with our visual system, which is faster at processing information. | Scatterplots can be useful for showing the relationship between two things. | Line graphs are most commonly used to plot continuous data. | Slopegraphs can be useful when you have two time periods or points of comparison and want to quickly show relative increases and decreases or differences across various categories between the two data points. | Bar charts are easy for our eyes to read. Our eyes compare the endpoints of the bars, so it is easy to see quickly which category is the biggest, which is the smallest, and also the incremental difference between categories. Note that, because of how our eyes compare the relative end points of the bars, it is important that bar charts always have a zero baseline (where the x‐axis crosses the y‐axis at zero), otherwise you get a false visual comparison. | Stacked vertical bar chart are meant to allow you to compare totals across categories and also see the subcomponent pieces within a given category. | Horizontal bar chart are the go-to graph for categorical data. They are especially useful if your category names are long, as the text is written from left to right, making your graph legible for your audience. | Avoid: pie charts, donut charts, 3D and secondary y-axes. | . Focus your audience’s attention . If we use preattentive attributes strategically, they can help us enable our audience to see what we want them to see before they even know they’re seeing it! | Don’t let your design choices be happenstance; rather, they should be the result of explicit decisions. | The use of color should always be an intentional decision. Never let your tool make this important decision for you! | If something is important, try not to make your audience wade through other stuff to get to it. Eliminate this work by putting the important thing at the top. | . Think like a designer . Antoine de Saint‐Exupery famously said, “You know you’ve achieved perfection, not when you have nothing more to add, but when you have nothing to take away” . Ask yourself: would eliminating this change anything? No? Take it out! Resist the temptation to keep things because they are cute or because you worked hard to create them; if they don’t support the message, they don’t serve the purpose of communication. | Push necessary, but non‐message‐impacting items to the background. Use your knowledge of preattentive attributes to deemphasize. Light grey works well for this. | Make it legible; Keep it clean; Use straightforward language; Remove unnecessary complexity. | Assume that every chart needs a title and every axis needs a title (exceptions to this rule will be extremely rare). The absence of these titles—no matter how clear you think it may be from context—causes your audience to stop and question what they are looking at. Instead, label explicitly so they can use their brainpower to understand the information, rather than spend it trying to figure out how to read the visual. | In data visualization—and communicating with data in general—spending time to make our designs aesthetically pleasing can mean our audience will have more patience with our visuals, increasing our chance of success for getting our message across. | . Lessons in Storytelling . A good story grabs your attention and takes you on a journey, evoking an emotional response. In the middle of it, you find yourself not wanting to turn away or put it down. After finishing it—a day, a week, or even a month later—you could easily describe it to a friend. | Aristotle introduced a basic but profound idea: that story has a clear beginning, middle, and end. He proposed a three‐act structure for plays. This concept has been refined over time and is commonly referred to as the setup, conflict, and resolution. | . What exactly is story? At a fundamental level, a story expresses how and why life changes. Stories start with balance. Then something happens—an event that throws things out of balance. McKee describes this as “subjective expectation meets cruel reality.” . Telling a compelling story is harder than conventional rhetoric. But delving into your creative recesses is worth it because a story allows you to engage your audience on an entirely new level. | End with a call to action: make it totally clear to your audience what you want them to do with the new understanding or knowledge that you’ve imparted to them. One classic way to end a story is to tie it back to the beginning | . In his book, Beyond Bullet Points, Cliff Atkinson outlines the following questions to consider and address when it comes to setting up the story: . The setting: When and where does the story take place? | The main character: Who is driving the action? (This should be framed in terms of your audience!) | The imbalance: Why is it necessary, what has changed? | The balance: What do you want to see happen? | The solution: How will you bring about the changes? | Final thoughts . The default settings of our graphing application are typically far from ideal. Our tools do not know the story we aim to tell. Combine these two things and you run the risk of losing a great deal of potential value—including the opportunity to drive action and effect change—if adequate time isn’t spent on this final step in the analytical process: the communication step. . As a Python programmer I found this amazing Github repository with code to plot the same graphs the author presents in her book using the library Matplotlib: https://github.com/empathy87/storytelling-with-data . I intentionally did not present any graphs here to encourage you to read the book and navigate through the examples that are beautifully laid out by the author. . “Nirvana in communicating with data is reached when the effective visuals are combined with a powerful narrative.” . Reference: . Knaflic, Cole Nussbaumer. Storytelling with data: A data visualization guide for business professionals. John Wiley &amp; Sons, 2015. . Header: Photo by Nong Vang on Unsplash . Body: Photo by Benjamin Smith on Unsplash .",
            "url": "https://millengustavo.github.io/blog/data%20science/storytelling/data%20driven/book/2019/08/24/storytelling.html",
            "relUrl": "/data%20science/storytelling/data%20driven/book/2019/08/24/storytelling.html",
            "date": " • Aug 24, 2019"
        }
        
    
  
    
        ,"post21": {
            "title": "Data Science Workflow",
            "content": "I don’t come from a software engineering background. Most data scientists I know, also don’t. . We can argue that some of our work will never be executed again and we shouldn’t waste time organizing it. The big pletora of tools introduced every day may overwhelm even the workaholics. This may lead to analysis paralysis and, for me, this is worse than doing things in an unorganized manner. . A method to the madness . I’ve been testing different methods to better organize my data science work. The main objectives for me are: . Ensure reproducibility of my work | Be modular in order to be maintanable and extensible | Generate documentation without hassle | . Disclaimer . This is a work in progress and may be updated with time. . OS . Unix based operating systems, so is either Mac or some Linux distribution. Ubuntu works without much hassle, the latest 19.04 with the Gnome patches is working well so far. . Python . Python 3, for Data Science you can save a lot of time by installing Anaconda. . Python virtual environment . Using conda you can create a python 3.7 environment called env_name by running: . conda create --name env_name python=3.7 . Creating a new project . I started using Kedro lately to structure my projects. . You can use others like the great Cookiecutter Data Science. . Open the terminal and navigate to the directory in which you want to work. Activate your environment created on the previous step by running: . conda activate env_name . Install kedro: . pip install kedro . Create a new project: . kedro new . A new folder will be created in &lt;current_dir&gt;/&lt;repo_name&gt;/ . Navigate to this folder cd &lt;repo_name&gt; and move the credentials.yml file to the local configuration: . mv ./conf/base/credentials.yml ./conf/local/ . Data . The workflow starts to become more specific now. If you have the data as file(s), copy it to the data folder in the correct subfolder. If you are querying from a database, read Kedro documentation for guidelines. . Reference all datasets for the project in conf/base/catalog.yml . Exploration . Use the notebooks folder to explore and draft experimental code in Jupyter notebooks. Once you have a good understanding of the data and have written your functions to process it, copy them to src/&lt;project_name&gt;/nodes/ as .py file(s). . Nodes . Create the data transformation steps as pure Python functions when possible. With every step coded as functions in the nodes folder, it’s time to create the pipeline. . Pipeline . The data pipeline code goes in src/&lt;project_name&gt;/pipeline.py. You need to specify the inputs and outputs of the functions. Make use of the conf/base/catalog.yml to document the datasets. . Finally, choose if you want to run the pipeline in sequence or in parallel (--parallel on the command below). . kedro run . Documentation . Include docstrings to explain your Python functions, so you can take advantage of auto-generated Sphinx documentation. . Build the project documentation: . kedro build-docs . Other details . I simplified the process here, I recommend that you run both examples on the Kedro documentation page to get used to the workflow. . Use Git to version control your codebase. . Ideally, you should also write unit tests and add CI configuration to your repository. . Conclusion . Quoting the main features of Kedro that won me over: . Kedro is a workflow development tool that helps you build data pipelines that are robust, scalable, deployable, reproducible and versioned. We provide a standard approach so that you can: . spend more time building your data pipeline, | worry less about how to write production-ready code, | standardise the way that your team collaborates across your project, | work more efficiently. | . If you are starting with data science, don’t over organize things. Use simple tools to understand the concepts and play with models/visualization. Have fun playing around, and you are more likely to want to advance on complex concepts. .",
            "url": "https://millengustavo.github.io/blog/data%20science/machine%20learning/kedro/2019/06/07/data-science-workflow.html",
            "relUrl": "/data%20science/machine%20learning/kedro/2019/06/07/data-science-workflow.html",
            "date": " • Jun 7, 2019"
        }
        
    
  
    
        ,"post22": {
            "title": "Measure What Matters",
            "content": "“A management methodology that helps to ensure that the company focuses efforts on the same important issues throughout the organization” . Ideas are easy. Execution is everything | OKRS. Short for Objectives and Key Results | . Objective . WHAT. Significant, concrete, action oriented, and (ideally) inspirational . Key Results . HOW. Specific and time-bound, aggressive yet realistic. Measure and verifiable. . The Father of OKRs . . Intel Free Press [CC BY-SA 2.0 (https://creativecommons.org/licenses/by-sa/2.0)] . Former Intel CEO Andy Grove in 2003 with a 1978 photo of him with Intel co-founders Robert Noyce and Gordon Moore. . Each objective should be tied to five or fewer key results | Set goals from the bottom up | No dictating | Stay flexible | Dare to fail | Be patient; be resolute | . OKRs Superpowers . . Focus and Commit to Priorities . Successful organizations focus on the handful of initiatives that can make a real difference, deferring less urgent ones | “must be able to measure… performance and results against the goal.” Peter Drucker | If you’re certain you’re going to nail it, you’re probably not pushing hard enough | The best OKR cadence is the one that fits the context and culture of your business | Completion of all key results must result in attainment of the objective. If not, it’s not an OKR | The one thing an OKR system should provide par excellence is focus. This can only happen if we keep the number of objectives small | . Align and Connect for Teamwork . Public goals are more likely to be attained than goals held in private | Meritocracy flourishes in sunlight | Companies with highly aligned employees are more than twice as likely to be top performers | People who choose their destination will own a deeper awareness of what it takes to get there | MyFitnessPal and Under Armor: “North star alignment”: “When our customers succeed at reaching their health and fitness goals, we succeed as a company”. | . Track for Accountability . OKRs are living, breathing organisms | The single greatest motivator is ‘making progress in one’s work.’ The days that people make progress are the days they feel most motivated and engaged | The point of a real-time dashboard is to quantify progress against a target and flag what needs attention | Where OKRs scores pinpoint what went right or wrong in the work, and how the team might improve, self-assessments drive a superior goal-setting process for the next quarter | There are no judgments, only learning | The Gates Foundation: making goals concrete | . Stretch for Amazing . “The biggest risk of all is not taking one” – Mellody Hobson | BHAG: “Big Hairy Audacious Goals” – Jim Collins | Google divides its OKRs into two categories, committed goals and aspirational (or “stretch”) goals | There is no one magic number for the “right” stretch. If you seek to achieve greatness, stretching for amazing is a great place to start | Google: “The Gospel of 10x”. Gmail: 1 GB of storage when web-based email systems offered 2 to 4 MB | . Continuous Performance Management: OKRs and CFRs . CFRs. Conversations, Feedback, Recognition . Conversations . An authentic, richly textured exchange between manager and contributor, aimed at driving performance . Feedback . Bidirectional or networked communication among peers to evaluate progress and guide future improvement . Recognition . Expressions of appreciation to deserving individuals for contributions of all sizes . Culture . Structure and clarity: are goals, roles, and execution plans on our team clear? | Psychological safety: can we take risks on this team without feeling insecure or embarrassed? | Meaning of work: are we working on something that is personally important for each of us? | Dependability: can we count on each other to do high-quality work on time? | Impact of work: do we fundamentally believe that the work we’re doing matters? | . Reference . Doerr, John. Measure what matters: How Google, Bono, and the Gates Foundation rock the world with OKRs. Penguin, 2018. .",
            "url": "https://millengustavo.github.io/blog/management%20methodology/project%20management/objectives%20and%20key%20results/2019/05/12/okr.html",
            "relUrl": "/management%20methodology/project%20management/objectives%20and%20key%20results/2019/05/12/okr.html",
            "date": " • May 12, 2019"
        }
        
    
  
    
        ,"post23": {
            "title": "II Artificial Intelligence in Healthcare Symposium and Datathon",
            "content": "From April 25-27th 2019 Hospital Israelita Albert Einstein (HIAE), the PROADI-SUS project and MIT organized its Artificial Intelligence in Healthcare Symposium and Datathon. . In the first day, we attended a Symposium including talks and round tables with experts in the field of Artificial Intelligence, Big Data and Healthcare. I took some notes of things I found interesting and will be posting here for further consultation. . Symposium . Richard Delaney - Vital Strategies . Intersection Between High-dimensionality Data and Established Public Health Databases . High Dimensional Data: Many factors to control for, harder to do hypothesis testing. One alternative is to leverage machine learning to do more pattern recognition | Don’t always come with pre conceived ideas, but try to find patterns freely | Find probabilities of actions to know where to deliver Public health service | . | Creating conditions to success: Start with a project: avoid “We prepare and then we act”, we are never prepared enough! Start easy, purposeful, datathon events follow this idea! | After the first step, discuss systemic changes | | . Nick Guldemond - The Institute of Health Policy &amp; Management Erasmus University . Digital Technologies for Public and Private Primary Care . Design Thinking for Health Data Science Projects Patient Journey | Clinical Pathway | . | Data new vs old Wikipedia Findable | Accessible | Interoperable | Reusable | . | If we find a good solution, how do we scale? To another region, etc… | . Jacson Venâncio Barros - President of the Brazilian Association of CIO’s in Health . Technologies for Supporting Public Health Management and Governance Problem: Startups are disconnected from end users . 70% of hospitals don’t have eletronic medical records | Lack of standards in vocabulary (taxonomy) | Asymmetry of information: each specialty has its own jargon | Lack of trust among health sectors Challenge: Identify in this sea of ​​data, which is the drop that interests us | . Round Table: Challenges of Big Data and AI in Population Health - Gisele Bohn, Richard Delaney, Nick Guldemond, Jacson Barros, Fatima Marinho . Topics: . Aggregate behavioral data to clinical data | Think small first: break problems into small questions that big data can help | Predict events that are repeated in certain population groups | Integration with the real world | How social networks influence behaviour | . Open Session: Data Access, Privacy and Security - Andrea Suman, Silvio Pereira, Anderson Soares, Marcelo Felix, Rogéria Leoni Cruz . Restrictive law? | Big Data x Data Governance | No safe culture, territoriality, data sharing (would it restrict the access for research?) | Loss of potential of using machine learning | . Leonardo Rolim Ferraz . Data Driven ICU - Einstein Initiative . Lucas Bulgarelli - MIT/HIAE . Big Data 360 . MIMIC | De-identification | Leveraging electronic health records for clinical research | Sharing - Physionet | . Ary Serpa Neto - HIAE . Innovative Models of Clinical Trials Using Large Databases . Christopher Cosgriff - MIT . Deep Learning: A Brief Overview for Clinicians . Round Table: Challenges of Big Data and AI in the Intensive Care Unit - Leo Anthony Celi, Alistair Johnson, Leonardo Rolim Ferraz, Ary Serpa Neto, Lucas Bulgarelli . The Book of Why | . Leo Anthony Celi - MIT . MIT Experience in Data Analytics applied to Intensive Care Units . MIMIC (Medical Information Mart for Intensive Care) | Book: Secondary Analysis of Electronic Health Records | Opportunities for AI in Healthcare Classification | Prediction | Optmization (Precision Medicine) | . | . Alistair Johnson - MIT . Keynote: MIMIC Across Modalities: X-rays and Beyond . Chest X-rays are ubiquitous, radiologists are not | Perfect algorithms on imperfect reports | . Matthieu Komorowski - Imperial College London . Keynote: Reinforcement Learning Approaches to Decision Support in Sepsis . Open Session: Opportunities for Innovation in Healthcare - Pedro Marton Pereira, Gustavo Landsberg, Gisele Bohn, André Bem . I forgot to take notes. . . Datathon . . Cauê Bueno, Eduardo Casaroto, Fernando Ramos, Gustavo Silveira, Jacqueline Silva, Matheus Silva, Marcelo Fiorelli, Sonia Altavila, Wellington Araújo . Pulse Pressure: a new outcome predictor in the Intensive Care Unit? . Brief literature review and motivation for study . Mean Arterial Pressure (MAP) is the main parameter used to define hemodynamic condition in critically ill patients and levels below 65mmHg are related to poor outcomes especially in septic patients | Pulse Pressure (PP) is related to pressure and stroke volume. | Pulse Pressure is a neglected hemodynamic parameter at bedside. | . Aim of Study . Evaluate if Pulse Pressure is a reliable predictor of 28-days mortality compared to Mean Arterial Pressure in critically ill patients . Data Source . Inclusion criteria: First ICU admissions (MIMIC-3) | . | Exclusion criteria Patients readmitted at ICU in the same hospitalization | Age &lt; 18 years | . | . Statistical analysis . Description of the sample: mean and standard deviation | bars plots and boxplots | . | Logistic regression model Outcome: 28-days mortality (from ICU admission) | Model 1: mean arterial pressure | Model 2: pulse pressure | . | Results . Characteristis of the Sample . . SD: standard deviation *Values taken when the minimum systolic blood pressure where observed in the first 24h of ICU admission . MAP vs PP . . . Logistic Regression . Outcome = 28-days mortality . Model 1: predictor = mean arterial pressure OR = 0.9694 95% IC = 0.9672 - 0.9717 . Model 2: predictor = pulse pressure OR = 0.9960 95% IC = 0.9942 - 0.9977 . Discussion and Next Steps . MAP assessment alone might not be useful to predict outcome to all ICU patients | Preliminary result: PP assessment looks to be a better outcome predictor for “higher PP” patients | Promising results: min PP&gt;70 - No min MAP difference between outcomes | . . Next Steps: . Group analysis: who are those patients? Are they elderly people? | Check ICU interventions (mechanical ventilation, vasopressors etc)? | Should we develop a specific guideline for those patients? | .",
            "url": "https://millengustavo.github.io/blog/health%20analytics/datathon/data%20science/machine%20learning/2019/04/28/datathon.html",
            "relUrl": "/health%20analytics/datathon/data%20science/machine%20learning/2019/04/28/datathon.html",
            "date": " • Apr 28, 2019"
        }
        
    
  
    
        ,"post24": {
            "title": "Digital blindness: Learning to look at the digital conversation between two major bank brands and their audience using Python",
            "content": "New “digital” banks are rising in popularity in Brazil. It is hard to pinpoint the exact reasons for this, we may gain some insight on the subject by investigating data. . In this article we are going to dive deeper on tweets from two Brazilian banks: Itaú and Nubank. We will use Python to help us on this mission. . Cleaning the data . The Jupyter Notebook with the code is available here. . We start our journey with two datasets in JSON format. The data consists of tweets from both banks between 2014 and 2019 (February). . Let’s import one of the datasets and manipulate it with Pandas. . import pandas as pd import numpy as np import matplotlib.pyplot as plt from wordcloud import WordCloud, STOPWORDS df_nubank = pd.read_json(‘nubank_tweets.json’) df_nubank.head() . . We can see that these are tweets from the bank itself, some are replies to other users. We have some interesting columns: ‘created_at’, ‘favorite_count’ and ‘retweet_count’. . Let’s sort the tweets by ‘favorite_count’ and keep tweets created after January 2018. We’ll need to clean the text, but first we duplicate it and save to a new ‘original_tweet’ column to help us later. . df_nubank = df_nubank.sort_values(by=[‘favorite_count’],ascending=False).copy() df_nubank = df_nubank.reset_index(drop=True) df_nubank = df_nubank[df_nubank[‘created_at’] &gt; ‘2018–01–01’] df_nubank[‘original_tweet’] = df_nubank[‘text’] df_nubank.head() . . Every NLP (Natural Language Processing) pipeline starts with cleaning the text. Let’s do it now! . # Lower case df_nubank[‘text’] = df_nubank[‘text’].apply(lambda x: “ “.join(x.lower() for x in x.split())) # Removing usernames df_nubank[‘text’] = df_nubank[‘text’].str.replace(‘@[^ s]+’,””) # Removing urls df_nubank[‘text’] = df_nubank[‘text’].str.replace(‘https?: / /.*[ r n]*’,’’) # Removing punctuation df_nubank[‘text’] = df_nubank[‘text’].str.replace(‘[^ w s]’,’’) # Removing stopwords from nltk.corpus import stopwords stop = stopwords.words(‘portuguese’) df_nubank[‘text’] = df_nubank[‘text’].apply(lambda x: “ “.join(x for x in x.split() if x not in stop)) # Removing common brazilian names -&gt; you will need a .txt with these, it is easy to find it online names = pd.read_csv(‘nomes.txt’, encoding=’latin’, header=None) name_list = (names[0].str.lower()).tolist() df_nubank[‘text’] = df_nubank[‘text’].apply(lambda x: “ “.join(x for x in x.split() if x not in name_list)) # Removing numbers df_nubank[‘text’] = df_nubank[‘text’].str.replace(‘ d+’,’’) # Removing small words -&gt; e.g.: é, tá, lá, pra, etc df_nubank[‘text’] = df_nubank[‘text’].str.replace(r’ b( w{1,3}) b’, ‘’) # Normalizing it df_nubank.text = df_nubank.text.str.normalize(‘NFKD’) .str.encode(‘ascii’, errors=’ignore’) .str.decode(‘utf-8’) # Removing empty rows df_nubank[‘text’].replace(‘’, np.nan, inplace=True) df_nubank.dropna(subset=[‘text’], inplace=True) # Let’s keep only the first 10k tweets from our dataframe df_nubank = df_nubank[:10000] df_nubank.head() . . By now you can see how different our ‘text’ column looks in comparison with the ‘original_tweet’. . Let’s proceed to our first visualization: Word Clouds! We are going to use the wordcloud library. . text = “ “.join(review for review in df_nubank.text) %matplotlib inline stopwords = set(STOPWORDS) wordcloud = WordCloud(width = 3000, height = 2000, background_color = ‘black’, stopwords = stopwords).generate(text) fig = plt.figure(figsize = (40, 30), facecolor = ‘k’, edgecolor = ‘k’) plt.imshow(wordcloud, interpolation = ‘bilinear’) plt.axis(‘off’) plt.tight_layout(pad=0) plt.show() . . Awesome right?! Let’s do the same for Itaú and proceed to the scattertext! . df_itau = pd.read_json(‘itau_tweets.json’) # sorting by fav count df_itau = df_itau.sort_values(by=[‘favorite_count’],ascending=False).copy() df_itau = df_itau.reset_index(drop=True) df_itau = df_itau[df_itau[‘created_at’] &gt; ‘2018–01–01’] df_itau[‘original_tweet’] = df_itau[‘text’] # Data Cleaning # Lower case df_itau[‘text’] = df_itau[‘text’].apply(lambda x: “ “.join(x.lower() for x in x.split())) # Removing usernames df_itau[‘text’] = df_itau[‘text’].str.replace(‘@[^ s]+’,””) # Removing urls df_itau[‘text’] = df_itau[‘text’].str.replace(‘https?: / /.*[ r n]*’,’’) # Removing punctuation df_itau[‘text’] = df_itau[‘text’].str.replace(‘[^ w s]’,’’) # Removing stopwords from nltk.corpus import stopwords stop = stopwords.words(‘portuguese’) df_itau[‘text’] = df_itau[‘text’].apply(lambda x: “ “.join(x for x in x.split() if x not in stop)) # Removing common brazilian names -&gt; you will need a .txt with these, it is easy to find it online names = pd.read_csv(‘nomes.txt’, encoding=’latin’, header=None) name_list = (names[0].str.lower()).tolist() df_itau[‘text’] = df_itau[‘text’].apply(lambda x: “ “.join(x for x in x.split() if x not in name_list)) # Removing numbers df_itau[‘text’] = df_itau[‘text’].str.replace(‘ d+’,’’) # Removing small words -&gt; e.g.: é, tá, lá, pra, etc df_itau[‘text’] = df_itau[‘text’].str.replace(r’ b( w{1,3}) b’, ‘’) # Normalizing it df_itau.text = df_itau.text.str.normalize(‘NFKD’) .str.encode(‘ascii’, errors=’ignore’) .str.decode(‘utf-8’) # Removing empty rows df_itau[‘text’].replace(‘’, np.nan, inplace=True) df_itau.dropna(subset=[‘text’], inplace=True) # Using only the first 10k tweets df_itau = df_itau[:10000] text_itau = “ “.join(review for review in df_itau.text) # Wordcloud stopwords = set(STOPWORDS) wordcloud = WordCloud(width = 3000, height = 2000, background_color = ‘black’, stopwords = stopwords).generate(text_itau) fig = plt.figure(figsize = (40, 30), facecolor = ‘k’, edgecolor = ‘k’) plt.imshow(wordcloud, interpolation = ‘bilinear’) plt.axis(‘off’) plt.tight_layout(pad=0) plt.show() . . Word clouds are not the best way of interpreting the data. Comparing sizes and intensities of colors is not straight forward. . Scattertext . From the github repository: . “A tool for finding distinguishing terms in small-to-medium-sized corpora, and presenting them in a sexy, interactive scatter plot with non-overlapping term labels. Exploratory data analysis just got more fun.” . We will use it to create a scatter plot of the words based on the frequency and without overlapping the labels. . Let’s combine both categories and use the ‘created_at’ and ‘original_tweet’ columns as metadata. . df_nubank[‘bank’] = ‘nubank’ df_nubank[‘metadata’] = df_nubank.created_at.map(str) + “ | “ + df_nubank.original_tweet df_nubank = df_nubank[[‘metadata’, ‘bank’, ‘text’]] df_itau[‘bank’] = ‘itau’ df_itau[‘metadata’] = df_itau.created_at.map(str) + “ | “ + df_itau.original_tweet df_itau = df_itau[[‘metadata’, ‘bank’, ‘text’]] df_scatter = df_nubank.append(df_itau) . Creating the scatter plot . import scattertext as st import spacy nlp = spacy.load(‘pt’) corpus = (st.CorpusFromPandas(df_scatter, category_col=’bank’, text_col=’text’, nlp=nlp) .build() .get_unigram_corpus() .compact(st.ClassPercentageCompactor(term_count=1, term_ranker=st.OncePerDocFrequencyRanker))) html = st.produce_scattertext_explorer(corpus, category=’nubank’, category_name=’Nubank’, not_category_name=’Itaú’, width_in_pixels=1000, metadata=corpus.get_df()[‘metadata’]) open(“visualization_nubank_itau.html”, ‘wb’).write(html.encode(‘utf-8’)) . Exploring the Scattertext html file . The code above generates an .html file. You can access a live version on this link, but I will post some screenshots to illustrate. . . On the top-left corner you can see the words used more by Nubank and less by Itaú. On the bottom-right corner you see the words used more by Itaú and less by Nubank. . Bottom-left corner shows the least frequent terms and top-right corner the most frequent ones. . Searching for specific terms . We can see that ‘ajudar’ (help in Portuguese) is the most characteristic word of this corpus. You can click on the word or type it on the search box and see the frequency it appears in each category. . . The term ‘agência’ (agency in Portuguese) is the top term from Itaú, appearing 586 times. It appears only 4 times in the Nubank category. . Itaú was founded in January 2, 1945, while Nubank was founded in May 6, 2013. It is reasonable that we find contrasts like this. . . The term ‘roxinho’ (a cute way of saying purple in Portuguese) is the top term from Nubank. The bank uses this word as a nickname for their flagship product: their credit card. . . Emoji analysis . Scattertext introducing text says: . “Exploratory data analysis just got more fun.” . Let’s take fun to the next level, we will compare the emoji on the tweets from both banks! Play with the live version on this link. . corpus = st.CorpusFromParsedDocuments(df_scatter, parsed_col=’metadata’, category_col=’bank’, feats_from_spacy_doc=st.FeatsFromSpacyDocOnlyEmoji()).build() html = st.produce_scattertext_explorer(corpus, category=’nubank’, category_name=’Nubank’, not_category_name=’Itaú’, metadata=(corpus.get_df()[‘metadata’]), width_in_pixels=1000) open(“emoji_nubank_itau.html”, ‘wb’).write(html.encode(‘utf-8’)) . . A scatter plot of emoji based on their frequency of use. How awesome is that?! . We see hearts everywhere, but a clear distinction between their color. Keep in mind that we filtered the tweets only for those starting from January 2018, and 2018 was a World Cup year. . . The color of the hearts from Itaú were most from the Brazilian flag and the bank’s main color: orange. The bank is an official sponsor of the Brazilian Soccer Team. . . The hearts from Nubank followed the signature color of the brand: purple. . Wrapping up . Play around with the plots to see how powerful they are. If you liked this article, recommend it and share it with your friends. . Disclaimer . This analysis was done based on public available data from the brand’s social media channels to illustrate the application of an algorithm developed by our engineers. We don’t have any affiliation with those brands. . Citation for the Scattertext library: . Jason S. Kessler. Scattertext: a Browser-Based Tool for Visualizing how Corpora Differ. ACL System Demonstrations. 2017. .",
            "url": "https://millengustavo.github.io/blog/data%20visualization/data%20science/machine%20learning/2019/03/15/digitalblindness.html",
            "relUrl": "/data%20visualization/data%20science/machine%20learning/2019/03/15/digitalblindness.html",
            "date": " • Mar 15, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Technology enthusiast, avid Python programmer and DevOps practitioner. Worked in many Health Industry projects as Data Scientist doing from ad-hoc statistical analysis to Full Stack Machine Learning applications. Firm proponent of using cloud platforms like AWS and Google Cloud Provider to ensure safety and scalability of applications. . Working with Biomedical and Health related data since my undergraduate degree. Experienced with several tools, including: Python (Pandas, Numpy, Sklearn, Matplotlib, Seaborn, Jupyter Notebook, Dash, Flask, Tensorflow, Keras), SQL, Git, Docker, Linux, Javascript. . Led technology consulting projects starting with the Design Thinking to the Deployment and Monitoring for continuous improvement, using Agile methodologies of development. . Former neuroscience researcher at COPPE/UFRJ having studied Machine Learning and Deep Learning techniques applied to EEG signals. Hold a masters degree in Biomedical Engineering and an undergraduate degree in Mechatronic Engineering. Always learning through several courses and from participating in inspiring communities (Github, Meetups and social media). . Passionate about high performance, ownership and making meaningful impact with my work. .",
          "url": "https://millengustavo.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://millengustavo.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}