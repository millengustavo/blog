<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow | Gustavo Millen</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="My notes and highlights on the book." />
<meta property="og:description" content="My notes and highlights on the book." />
<link rel="canonical" href="https://millengustavo.github.io/blog/book/machine%20learning/data%20science/2019/12/24/handson-ml.html" />
<meta property="og:url" content="https://millengustavo.github.io/blog/book/machine%20learning/data%20science/2019/12/24/handson-ml.html" />
<meta property="og:site_name" content="Gustavo Millen" />
<meta property="og:image" content="https://millengustavo.github.io/blog/images/handson_ml/handson_ml.jpeg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-12-24T00:00:00-06:00" />
<script type="application/ld+json">
{"headline":"Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow","dateModified":"2019-12-24T00:00:00-06:00","datePublished":"2019-12-24T00:00:00-06:00","description":"My notes and highlights on the book.","image":"https://millengustavo.github.io/blog/images/handson_ml/handson_ml.jpeg","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://millengustavo.github.io/blog/book/machine%20learning/data%20science/2019/12/24/handson-ml.html"},"url":"https://millengustavo.github.io/blog/book/machine%20learning/data%20science/2019/12/24/handson-ml.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://millengustavo.github.io/blog/feed.xml" title="Gustavo Millen" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow | Gustavo Millen</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="My notes and highlights on the book." />
<meta property="og:description" content="My notes and highlights on the book." />
<link rel="canonical" href="https://millengustavo.github.io/blog/book/machine%20learning/data%20science/2019/12/24/handson-ml.html" />
<meta property="og:url" content="https://millengustavo.github.io/blog/book/machine%20learning/data%20science/2019/12/24/handson-ml.html" />
<meta property="og:site_name" content="Gustavo Millen" />
<meta property="og:image" content="https://millengustavo.github.io/blog/images/handson_ml/handson_ml.jpeg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-12-24T00:00:00-06:00" />
<script type="application/ld+json">
{"headline":"Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow","dateModified":"2019-12-24T00:00:00-06:00","datePublished":"2019-12-24T00:00:00-06:00","description":"My notes and highlights on the book.","image":"https://millengustavo.github.io/blog/images/handson_ml/handson_ml.jpeg","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://millengustavo.github.io/blog/book/machine%20learning/data%20science/2019/12/24/handson-ml.html"},"url":"https://millengustavo.github.io/blog/book/machine%20learning/data%20science/2019/12/24/handson-ml.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://millengustavo.github.io/blog/feed.xml" title="Gustavo Millen" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Gustavo Millen</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2019-12-24T00:00:00-06:00" itemprop="datePublished">
        Dec 24, 2019
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      74 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#book">book</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#machine learning">machine learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#data science">data science</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#part-i-the-fundamentals-of-machine-learning">Part I, The Fundamentals of Machine Learning</a></li>
<li class="toc-entry toc-h1"><a href="#ch1-the-machine-learning-landscape">CH1. The Machine Learning Landscape</a>
<ul>
<li class="toc-entry toc-h3"><a href="#supervisedunsupervised-learning">Supervised/Unsupervised Learning</a></li>
<li class="toc-entry toc-h3"><a href="#batch-and-online-learning">Batch and Online Learning</a></li>
<li class="toc-entry toc-h3"><a href="#instance-based-x-model-based-learning">Instance-Based x Model-Based Learning</a></li>
<li class="toc-entry toc-h3"><a href="#nonrepresentative-training-data">Nonrepresentative Training Data</a></li>
<li class="toc-entry toc-h3"><a href="#poor-quality-data">Poor-Quality Data</a></li>
<li class="toc-entry toc-h3"><a href="#irrelevant-features">Irrelevant Features</a></li>
<li class="toc-entry toc-h3"><a href="#overfitting-the-training-data">Overfitting the Training Data</a></li>
<li class="toc-entry toc-h3"><a href="#underfitting-the-training-data">Underfitting the Training Data</a></li>
<li class="toc-entry toc-h3"><a href="#testing-and-validating">Testing and Validating</a></li>
<li class="toc-entry toc-h3"><a href="#hyperparameter-tuning-and-model-selection">Hyperparameter Tuning and Model Selection</a></li>
<li class="toc-entry toc-h3"><a href="#data-mismatch">Data Mismatch</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#ch2-end-to-end-machine-learning-project">CH2. End-to-End Machine Learning Project</a>
<ul>
<li class="toc-entry toc-h3"><a href="#pull-out-your-ml-project-checklist">Pull out your ML project checklist</a></li>
<li class="toc-entry toc-h3"><a href="#frame-the-problem-what-exactly-the-business-objective-is-how-does-the-company-expect-to-use-and-benefit-from-this-model">Frame the problem: what exactly the business objective is. How does the company expect to use and benefit from this model?</a></li>
<li class="toc-entry toc-h3"><a href="#select-a-performance-measure-rmse-mae-accuracy-etc">Select a Performance Measure: RMSE, MAE, accuracy, etc</a></li>
<li class="toc-entry toc-h3"><a href="#check-the-assumptions-by-you-or-others">Check the Assumptions (by you or others)</a></li>
<li class="toc-entry toc-h3"><a href="#create-an-isolated-environment">Create an isolated environment</a></li>
<li class="toc-entry toc-h3"><a href="#data-structure">Data Structure</a></li>
<li class="toc-entry toc-h3"><a href="#create-a-test-set-20-or-less-if-the-dataset-is-very-large">Create a Test Set (20% or less if the dataset is very large)</a></li>
<li class="toc-entry toc-h3"><a href="#put-the-test-set-aside-and-only-explore-the-training-set-if-the-training-set-is-large-you-may-want-to-sample-an-exploration-set">Put the test set aside and only explore the training set. If the training set is large, you may want to sample an exploration set</a></li>
<li class="toc-entry toc-h3"><a href="#we-are-very-good-at-spotting-patterns-in-pictures">We are very good at spotting patterns in pictures</a></li>
<li class="toc-entry toc-h3"><a href="#look-for-correlations">Look for Correlations</a></li>
<li class="toc-entry toc-h3"><a href="#experimenting-with-attribute-combinations">Experimenting with Attribute Combinations</a></li>
<li class="toc-entry toc-h3"><a href="#data-cleaning">Data Cleaning</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#the-median-can-only-be-used-on-numerical-attributes">the median can only be used on numerical attributes</a>
<ul>
<li class="toc-entry toc-h3"><a href="#feature-scaling">Feature Scaling</a></li>
<li class="toc-entry toc-h3"><a href="#transformation-pipelines">Transformation Pipelines</a></li>
<li class="toc-entry toc-h3"><a href="#better-evaluation-using-cross-validation">Better Evaluation Using Cross-Validation</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#and-later">and later</a>
<ul>
<li class="toc-entry toc-h3"><a href="#randomized-search">Randomized Search</a></li>
<li class="toc-entry toc-h3"><a href="#ensemble-methods">Ensemble Methods</a></li>
<li class="toc-entry toc-h3"><a href="#analyze-the-best-models-and-their-errors">Analyze the Best Models and Their Errors</a></li>
<li class="toc-entry toc-h3"><a href="#evaluate-your-system-on-the-test-set">Evaluate Your System on the Test Set</a></li>
<li class="toc-entry toc-h3"><a href="#project-prelaunch-phase">Project prelaunch phase</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#ch3-classification">CH3. Classification</a>
<ul>
<li class="toc-entry toc-h3"><a href="#stochastic-gradient-descent-sgd-classifier">Stochastic Gradient Descent (SGD) classifier</a></li>
<li class="toc-entry toc-h3"><a href="#performance-measures">Performance Measures</a></li>
<li class="toc-entry toc-h3"><a href="#measuring-accuracy-using-cross-validation">Measuring Accuracy using Cross-Validation</a></li>
<li class="toc-entry toc-h3"><a href="#confusion-matrix">Confusion Matrix</a></li>
<li class="toc-entry toc-h3"><a href="#precision">Precision</a></li>
<li class="toc-entry toc-h3"><a href="#recall-or-sensitivity-or-true-positive-rate-tpr">Recall or Sensitivity, or True Positive Rate (TPR)</a></li>
<li class="toc-entry toc-h3"><a href="#f1-score">F1 Score</a></li>
<li class="toc-entry toc-h3"><a href="#precisionrecall-trade-off">Precision/Recall trade-off</a></li>
<li class="toc-entry toc-h3"><a href="#the-roc-curve">The ROC Curve</a></li>
<li class="toc-entry toc-h3"><a href="#area-under-the-curve-auc">Area under the curve (AUC)</a></li>
<li class="toc-entry toc-h3"><a href="#binary-classifiers">Binary classifiers</a></li>
<li class="toc-entry toc-h3"><a href="#multiclass-classification">Multiclass Classification</a></li>
<li class="toc-entry toc-h3"><a href="#error-analysis">Error Analysis</a></li>
<li class="toc-entry toc-h3"><a href="#multilabel-classification">Multilabel Classification</a></li>
<li class="toc-entry toc-h3"><a href="#multioutput-classification-multioutput-multiclass-classification">Multioutput Classification (multioutput-multiclass classification)</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#ch4-training-models">CH4. Training Models</a>
<ul>
<li class="toc-entry toc-h3"><a href="#linear-regression">Linear Regression</a></li>
<li class="toc-entry toc-h3"><a href="#gradient-descent">Gradient Descent</a></li>
<li class="toc-entry toc-h3"><a href="#batch-vs-stochastic-gradient-descent">Batch vs Stochastic Gradient Descent</a></li>
<li class="toc-entry toc-h3"><a href="#learning-curves">Learning Curves</a></li>
<li class="toc-entry toc-h3"><a href="#biasvariance-trade-off">Bias/Variance Trade-off</a></li>
<li class="toc-entry toc-h3"><a href="#ridge-regression-tikhonov-regularization---l2">Ridge Regression (Tikhonov regularization) - L2</a></li>
<li class="toc-entry toc-h3"><a href="#lasso-regression---l1">Lasso Regression - L1</a></li>
<li class="toc-entry toc-h3"><a href="#elastic-net">Elastic Net</a></li>
<li class="toc-entry toc-h3"><a href="#early-stopping">Early Stopping</a></li>
<li class="toc-entry toc-h3"><a href="#logistic-regression-logit-regression">Logistic Regression (Logit Regression)</a></li>
<li class="toc-entry toc-h3"><a href="#softmax-regression-multinomial-logistic-regression">Softmax Regression (Multinomial Logistic Regression)</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#ch5-support-vector-machines">CH5. Support Vector Machines</a></li>
<li class="toc-entry toc-h1"><a href="#ch6-decision-trees">CH6. Decision Trees</a>
<ul>
<li class="toc-entry toc-h3"><a href="#whiteblack-box-models">White/Black box models</a></li>
<li class="toc-entry toc-h3"><a href="#pruning">Pruning</a></li>
<li class="toc-entry toc-h3"><a href="#problems">Problems</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#ch7-ensemble-learning-and-random-forests">CH7. Ensemble Learning and Random Forests</a>
<ul>
<li class="toc-entry toc-h3"><a href="#hard-voting">Hard voting</a></li>
<li class="toc-entry toc-h3"><a href="#independent-classifiers">Independent classifiers</a></li>
<li class="toc-entry toc-h3"><a href="#soft-voting">Soft voting</a></li>
<li class="toc-entry toc-h3"><a href="#bagging-and-pasting">Bagging and Pasting</a></li>
<li class="toc-entry toc-h3"><a href="#bootstrapping">Bootstrapping</a></li>
<li class="toc-entry toc-h3"><a href="#out-of-bag-evaluation">Out-of-bag evaluation</a></li>
<li class="toc-entry toc-h3"><a href="#random-patches-and-random-subspaces">Random Patches and Random Subspaces</a></li>
<li class="toc-entry toc-h3"><a href="#boosting">Boosting</a></li>
<li class="toc-entry toc-h3"><a href="#adaboost">AdaBoost</a></li>
<li class="toc-entry toc-h3"><a href="#gradient-boosting">Gradient Boosting</a></li>
<li class="toc-entry toc-h3"><a href="#xgboost">XGBoost</a></li>
<li class="toc-entry toc-h3"><a href="#stacking">Stacking</a></li>
<li class="toc-entry toc-h3"><a href="#brew">Brew</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#ch8-dimensionality-reduction">CH8. Dimensionality Reduction</a>
<ul>
<li class="toc-entry toc-h3"><a href="#the-curse-of-dimensionality">The Curse of Dimensionality</a></li>
<li class="toc-entry toc-h3"><a href="#projection">Projection</a></li>
<li class="toc-entry toc-h3"><a href="#manifold">Manifold</a></li>
<li class="toc-entry toc-h3"><a href="#pca">PCA</a></li>
<li class="toc-entry toc-h3"><a href="#principal-components">Principal Components</a></li>
<li class="toc-entry toc-h3"><a href="#explained-variance-ratio">Explained Variance Ratio</a></li>
<li class="toc-entry toc-h2"><a href="#kernel-pca">Kernel PCA</a></li>
<li class="toc-entry toc-h2"><a href="#locally-linear-embedding-lle">Locally Linear Embedding (LLE)</a></li>
<li class="toc-entry toc-h2"><a href="#other-dimensionality-reductions-techniques">Other Dimensionality Reductions Techniques</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#ch9-unsupervised-learning-techniques">CH9. Unsupervised Learning Techniques</a>
<ul>
<li class="toc-entry toc-h3"><a href="#clustering">Clustering</a></li>
<li class="toc-entry toc-h3"><a href="#k-means">K-Means</a></li>
<li class="toc-entry toc-h3"><a href="#k-means-1">K-Means++</a></li>
<li class="toc-entry toc-h3"><a href="#accelerated-k-means-and-mini-batch-k-means">Accelerated K-Means and mini-batch K-Means</a></li>
<li class="toc-entry toc-h3"><a href="#finding-the-optimal-number-of-clusters">Finding the optimal number of clusters</a></li>
<li class="toc-entry toc-h3"><a href="#limits-of-k-means">Limits of K-Means</a>
<ul>
<li class="toc-entry toc-h4"><a href="#active-learning-uncertainty-sampling">Active learning (uncertainty sampling)</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#dbscan">DBSCAN</a></li>
<li class="toc-entry toc-h3"><a href="#other-clustering-algorithms">Other Clustering Algorithms</a></li>
<li class="toc-entry toc-h3"><a href="#gaussian-mixtures">Gaussian Mixtures</a></li>
<li class="toc-entry toc-h3"><a href="#anomaly-detection-using-gaussian-mixtures">Anomaly Detection using Gaussian Mixtures</a></li>
<li class="toc-entry toc-h3"><a href="#selecting-the-number-of-clusters">Selecting the Number of Clusters</a></li>
<li class="toc-entry toc-h3"><a href="#bayesian-gaussian-mixture-models">Bayesian Gaussian Mixture Models</a></li>
<li class="toc-entry toc-h3"><a href="#other-algorithms-for-anomaly-and-novelty-detection">Other Algorithms for Anomaly and Novelty Detection</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#part-ii-neural-networks-and-deep-learning">Part II, Neural Networks and Deep Learning</a></li>
<li class="toc-entry toc-h1"><a href="#ch10-introduction-to-artificial-neural-networks-with-keras">CH10. Introduction to Artificial Neural Networks with Keras</a>
<ul>
<li class="toc-entry toc-h3"><a href="#perceptron">Perceptron</a></li>
<li class="toc-entry toc-h3"><a href="#the-multilayer-perceptron-and-backpropagation">The Multilayer Perceptron and Backpropagation</a></li>
<li class="toc-entry toc-h3"><a href="#backpropagation">Backpropagation</a></li>
<li class="toc-entry toc-h3"><a href="#activation-functions">Activation functions</a></li>
<li class="toc-entry toc-h3"><a href="#regression-mlp-architecture">Regression MLP architecture</a></li>
<li class="toc-entry toc-h3"><a href="#classification-mlp-architecture">Classification MLP architecture</a></li>
<li class="toc-entry toc-h3"><a href="#implementing-mlps-with-keras">Implementing MLPs with Keras</a></li>
<li class="toc-entry toc-h3"><a href="#creating-a-sequential-model">Creating a Sequential model</a></li>
<li class="toc-entry toc-h3"><a href="#compiling-the-model">Compiling the model</a></li>
<li class="toc-entry toc-h3"><a href="#training-and-evaluating-the-model">Training and evaluating the model</a></li>
<li class="toc-entry toc-h3"><a href="#building-complex-models-using-the-functional-api">Building Complex Models Using the Functional API</a></li>
<li class="toc-entry toc-h3"><a href="#using-the-subclassing-api-to-build-dynamic-models">Using the Subclassing API to Build Dynamic Models</a></li>
<li class="toc-entry toc-h3"><a href="#saving-and-restoring-a-model">Saving and Restoring a Model</a></li>
<li class="toc-entry toc-h3"><a href="#using-callbacks">Using Callbacks</a></li>
<li class="toc-entry toc-h3"><a href="#using-tensorboard-for-visualization">Using TensorBoard for Visualization</a></li>
<li class="toc-entry toc-h3"><a href="#fine-tuning-neural-network-hyperparameters">Fine-Tuning Neural Network Hyperparameters</a></li>
<li class="toc-entry toc-h3"><a href="#number-of-hidden-layers">Number of Hidden Layers</a></li>
<li class="toc-entry toc-h3"><a href="#number-of-neurons-per-hidden-layer">Number of Neurons per Hidden Layer</a></li>
<li class="toc-entry toc-h3"><a href="#learning-rate-batch-size-and-other-hyperparameters">Learning Rate, Batch Size, and Other Hyperparameters</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#ch11-training-deep-neural-networks">CH11. Training Deep Neural Networks</a>
<ul>
<li class="toc-entry toc-h3"><a href="#the-vanishingexploding-gradients-problems">The Vanishing/Exploding Gradients Problems</a></li>
<li class="toc-entry toc-h3"><a href="#glorot-and-he-initialization">Glorot and He Initialization</a></li>
<li class="toc-entry toc-h3"><a href="#nonsaturating-activation-functions">Nonsaturating Activation Functions</a></li>
<li class="toc-entry toc-h3"><a href="#batch-normalization">Batch Normalization</a></li>
<li class="toc-entry toc-h3"><a href="#gradient-clipping">Gradient Clipping</a></li>
<li class="toc-entry toc-h3"><a href="#reusing-pretrained-layers">Reusing Pretrained Layers</a></li>
<li class="toc-entry toc-h3"><a href="#transfer-learning-with-keras">Transfer Learning with Keras</a></li>
<li class="toc-entry toc-h3"><a href="#unsupervised-pretraining">Unsupervised Pretraining</a></li>
<li class="toc-entry toc-h3"><a href="#faster-optimizers">Faster Optimizers</a></li>
<li class="toc-entry toc-h3"><a href="#momentum-optimization">Momentum Optimization</a></li>
<li class="toc-entry toc-h3"><a href="#nesterov-accelerated-gradient-nag">Nesterov Accelerated Gradient (NAG)</a></li>
<li class="toc-entry toc-h3"><a href="#adagrad">AdaGrad</a></li>
<li class="toc-entry toc-h3"><a href="#rmsprop">RMSProp</a></li>
<li class="toc-entry toc-h3"><a href="#adam-and-nadam-optimization">Adam and Nadam Optimization</a></li>
<li class="toc-entry toc-h3"><a href="#learning-rate-scheduling">Learning Rate Scheduling</a></li>
<li class="toc-entry toc-h3"><a href="#avoiding-overfitting-through-regularization">Avoiding Overfitting Through Regularization</a></li>
<li class="toc-entry toc-h3"><a href="#l1-and-l2-regularization">L1 and L2 Regularization</a></li>
<li class="toc-entry toc-h3"><a href="#dropout">Dropout</a></li>
<li class="toc-entry toc-h3"><a href="#monte-carlo-mc-dropout">Monte Carlo (MC) Dropout</a></li>
<li class="toc-entry toc-h3"><a href="#default-dnn-configuration">Default DNN configuration</a></li>
<li class="toc-entry toc-h3"><a href="#dnn-configuration-for-a-self-normalizing-net">DNN configuration for a self-normalizing net</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#ch12-custom-models-and-training-with-tensorflow">CH12. Custom Models and Training with TensorFlow</a></li>
<li class="toc-entry toc-h1"><a href="#ch13-loading-and-preprocessing-data-with-tensorflow">CH13. Loading and Preprocessing Data with TensorFlow</a></li>
<li class="toc-entry toc-h1"><a href="#ch14-deep-computer-vision-using-convolutional-neural-networks">CH14. Deep Computer Vision Using Convolutional Neural Networks</a></li>
<li class="toc-entry toc-h1"><a href="#ch15-processing-sequences-using-rnns-and-cnns">CH15. Processing Sequences Using RNNs and CNNs</a>
<ul>
<li class="toc-entry toc-h2"><a href="#recurrent-neurons-and-layers">Recurrent Neurons and Layers</a>
<ul>
<li class="toc-entry toc-h3"><a href="#memory-cells">Memory Cells</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#input-and-output-sequences">Input and Output Sequences</a></li>
<li class="toc-entry toc-h2"><a href="#training-rnns">Training RNNs</a></li>
<li class="toc-entry toc-h2"><a href="#forecasting-a-time-series">Forecasting a Time Series</a>
<ul>
<li class="toc-entry toc-h3"><a href="#baseline-metrics">Baseline Metrics</a></li>
<li class="toc-entry toc-h3"><a href="#implementing-a-simple-rnn">Implementing a Simple RNN</a></li>
<li class="toc-entry toc-h3"><a href="#deep-rnns">Deep RNNs</a></li>
<li class="toc-entry toc-h3"><a href="#forecasting-several-time-steps-ahead">Forecasting Several Time Steps Ahead</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#handling-long-sequences">Handling Long Sequences</a>
<ul>
<li class="toc-entry toc-h3"><a href="#fighting-the-unstable-gradients-problem">Fighting the Unstable Gradients Problem</a></li>
<li class="toc-entry toc-h3"><a href="#tackling-the-short-term-memory-problem">Tackling the Short-Term Memory Problem</a></li>
<li class="toc-entry toc-h3"><a href="#lstm-long-short-term-memory-cells">LSTM (Long Short-Term Memory) cells</a></li>
<li class="toc-entry toc-h3"><a href="#gru-gated-recurrent-unit-cells">GRU (Gated Recurrent Unit) cells</a></li>
<li class="toc-entry toc-h3"><a href="#using-1d-convolutional-layers-to-process-sequences">Using 1D convolutional layers to process sequences</a></li>
<li class="toc-entry toc-h3"><a href="#wavenet">WaveNet</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#ch16-natural-language-processing-with-rnns-and-attention">CH16. Natural Language Processing with RNNs and Attention</a>
<ul>
<li class="toc-entry toc-h2"><a href="#character-rnn">Character RNN</a>
<ul>
<li class="toc-entry toc-h3"><a href="#stateful-rnn">Stateful RNN</a></li>
<li class="toc-entry toc-h3"><a href="#sentiment-analysis">Sentiment Analysis</a></li>
<li class="toc-entry toc-h3"><a href="#reusing-pretrained-embeddings">Reusing Pretrained Embeddings</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#an-encoder-decoder-network-for-neural-machine-translation-nmt">An Encoder-Decoder Network for Neural Machine Translation (NMT)</a>
<ul>
<li class="toc-entry toc-h3"><a href="#bidirectional-rnns">Bidirectional RNNs</a></li>
<li class="toc-entry toc-h3"><a href="#beam-search">Beam Search</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#attention-mechanisms">Attention Mechanisms</a>
<ul>
<li class="toc-entry toc-h3"><a href="#visual-attention">Visual Attention</a></li>
<li class="toc-entry toc-h3"><a href="#attention-is-all-you-need-the-transformer-architecture">Attention is All You Need: The Transformer Architecture</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#ch17-representation-learning-and-generative-learning-using-autoencoders-and-gans">CH17. Representation Learning and Generative Learning Using Autoencoders and GANs</a></li>
<li class="toc-entry toc-h1"><a href="#ch18-reinforcement-learning">CH18. Reinforcement Learning</a></li>
<li class="toc-entry toc-h1"><a href="#ch19-training-and-deploying-tensorflow-models-at-scale">CH19. Training and Deploying TensorFlow Models at Scale</a></li>
</ul><p>My notes and highlights on the book.</p>

<p>Author: Aurélien Geron</p>

<h1 id="part-i-the-fundamentals-of-machine-learning">
<a class="anchor" href="#part-i-the-fundamentals-of-machine-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Part I, The Fundamentals of Machine Learning</h1>

<h1 id="ch1-the-machine-learning-landscape">
<a class="anchor" href="#ch1-the-machine-learning-landscape" aria-hidden="true"><span class="octicon octicon-link"></span></a>CH1. The Machine Learning Landscape</h1>
<p>Machine Learning is great for:</p>
<ul>
  <li>Problems for which existing solutions require a lot of fine-tuning or long lists of rules: one Machine Learning algorithm can often simplify code and perform better than the traditional approach.</li>
  <li>Complex problems for which using a traditional approach yields no good solution: the best Machine Learning techniques can perhaps find a solution.</li>
  <li>Fluctuating environments: a Machine Learning system can adapt to new data.</li>
  <li>Getting insights about complex problems and large amounts of data.</li>
</ul>

<p>Broad categories of ML systems:</p>
<ul>
  <li>Trained with human supervision? (supervised, unsupervised, semisupervised, and Reinforcement Learning)</li>
  <li>Can learn incrementally on the fly? (online versus batch learning)</li>
  <li>Whether they work by simply comparing new data points to known data points, or instead by detecting patterns in the training data and building a predictive model, much like scientists do (instance-based versus model-based learning)</li>
</ul>

<h3 id="supervisedunsupervised-learning">
<a class="anchor" href="#supervisedunsupervised-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Supervised/Unsupervised Learning</h3>
<ul>
  <li>Supervised: the training set you feed to the algorithm includes the desired solutions (labels). e.g.: k-NN, Linear Regression, Logistic Regression, SVM, Decision Trees, Random Forests, Neural networks</li>
  <li>Unsupervised: the training data is unlabeled. e.g.: K-Means, DBSCAN, HCA, One-class SVM, Isolation Forest, PCA, Kernel PCA, LLE, t-SNE, Apriori, Eclat</li>
</ul>

<blockquote>
  <p><strong>Tip</strong>: It’s a good idea to reduce the dimension of your training data before feeding to another ML algorithm (e.g. supervised). Run faster, use less disk/memory, may perform better</p>
</blockquote>

<ul>
  <li>Semisupervised: data is partially labeled. e.g.: deep belief networks (DBNs) are based on unsupervised components called restricted Boltzmann machines (RBMs)</li>
  <li>Reinforcement: agent, rewards, penalties and policy</li>
</ul>

<h3 id="batch-and-online-learning">
<a class="anchor" href="#batch-and-online-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Batch and Online Learning</h3>
<ul>
  <li>Batch: incapable of learning incrementally, must be trained using all the available data (offline learning). This process can be automated, but the training process can take many hours/resources</li>
  <li>Online: train incrementally by feeding data sequentially (individually or mini-batches). <em>Out-of-core</em>: loads part of the data, train, repeat until has run on all of the data (usually done offline, so <em>online</em>~<em>incremental</em>). Learning rate: high = rapid adaption, quickly forget old data; low = more inertia, learn slowly, less sensitive to noise.</li>
</ul>

<h3 id="instance-based-x-model-based-learning">
<a class="anchor" href="#instance-based-x-model-based-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Instance-Based x Model-Based Learning</h3>
<ul>
  <li>Instance-based: learns the examples by heart, generalize by using similarity measures</li>
  <li>Model-based: build a model of the examples and use that model to make predictions</li>
</ul>

<blockquote>
  <p><strong>Data versus algorithms</strong> (2001 Microsoft researchers paper):
“these results suggest that we may want to reconsider the trade-off between spending time and money on algorithm development versus spending it on corpus development.”</p>
</blockquote>

<h3 id="nonrepresentative-training-data">
<a class="anchor" href="#nonrepresentative-training-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Nonrepresentative Training Data</h3>
<p>It is crucial that your training data be representative of the new cases you want to generalize to</p>
<ul>
  <li>Small sample: high chance of <em>sampling noise</em>
</li>
  <li>Large sample: if sampling method is flawed = <em>sampling bias</em>
</li>
</ul>

<h3 id="poor-quality-data">
<a class="anchor" href="#poor-quality-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Poor-Quality Data</h3>
<p>Training data full of errors, outliers and noise (e.g. poor-quality measurements) -&gt; often worth the effort of cleaning the training data.</p>
<ul>
  <li>Outliers: discard or fix the errors manually may help</li>
  <li>Missing: ignore the attribute, the instances, fill the missing values, train one model with the feature and one without it</li>
</ul>

<h3 id="irrelevant-features">
<a class="anchor" href="#irrelevant-features" aria-hidden="true"><span class="octicon octicon-link"></span></a>Irrelevant Features</h3>
<p>Feature engineering:</p>
<ul>
  <li>Feature selection: most useful features</li>
  <li>Feature extraction: combining existing features to produce a more useful one (e.g. dimensionality reduction)</li>
  <li>Creating new features by gathering new data</li>
</ul>

<h3 id="overfitting-the-training-data">
<a class="anchor" href="#overfitting-the-training-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Overfitting the Training Data</h3>
<p>The model performs well on the training data, but it does not generalize well</p>
<ul>
  <li>noisy training set</li>
  <li>small training set (<em>sampling noise</em>)</li>
</ul>

<blockquote>
  <p>Overfitting due to model being too complex relative to the amount and noise of the training data. 
Solutions:</p>
  <ul>
    <li>Simplify the model (fewer parameters), reduce number of attributes, constrain the model (regularization)</li>
    <li>Gather more training data</li>
    <li>Reduce noise (e.g. fix data errors, remove outliers)</li>
  </ul>
</blockquote>

<h3 id="underfitting-the-training-data">
<a class="anchor" href="#underfitting-the-training-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Underfitting the Training Data</h3>
<p>Model is too simple to learn the underlying structure of the data.</p>
<blockquote>
  <p>Solutions:</p>
  <ul>
    <li>Select a more powerful model, with more parameters</li>
    <li>Feed better features to the learning algorithm (feature engineering)</li>
    <li>Reduce the constraints of the model (e.g. reduce the regularization hyperparameter)</li>
  </ul>
</blockquote>

<h3 id="testing-and-validating">
<a class="anchor" href="#testing-and-validating" aria-hidden="true"><span class="octicon octicon-link"></span></a>Testing and Validating</h3>
<p>Split your data into two sets: training and test. The error rate is called <em>generalization error</em> (<em>out-of-sample error</em>), by evaluating your model on the test set, you get an estimate of this error</p>

<p>Training error low, but generalization error high -&gt; model is overfitting the training data</p>

<h3 id="hyperparameter-tuning-and-model-selection">
<a class="anchor" href="#hyperparameter-tuning-and-model-selection" aria-hidden="true"><span class="octicon octicon-link"></span></a>Hyperparameter Tuning and Model Selection</h3>
<p><strong>Holdout validation</strong>: hold out part of the training set to evaluate several candidate models and select the best one. The held-out set is called <em>validation set</em> (or <em>development set</em>, or <em>dev set</em>). After the validation process, you train the best model on the full training set (including the validation set), and this gives you the final model. Lastly, you evaluate this final model on the test set to get an estimate of the generalization error</p>

<p>If the validation set is too small, model evaluations will be imprecise -&gt; suboptimal model by mistake.</p>

<p>If the validation set is too large, the remaining training set will be much smaller than the full training set.</p>

<p>Solution: perform repeated <em>cross-validation</em>, using many validation sets. Each model is evaluated once per validation set after it is trained on the rest of the data. By averaging out all the evaluations of a model, you have a more accurate measure of its performance (but… more training time).</p>

<h3 id="data-mismatch">
<a class="anchor" href="#data-mismatch" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data Mismatch</h3>
<p>The validation set and test set must be as representative as possible of the data you expect to use in production.</p>

<p>If this happens, hold out some training data in another set -&gt; <em>train-dev</em> set. If after the model trained on the training set performs well on the <em>train-dev</em>, then the model is not overfitting. If it performs poorly on the validation set, it’s probably a data mismatch problem. Conversely, if the model performs poorly on the train-dev set, it must have overfitted the training set.</p>

<blockquote>
  <p><strong>NO FREE LUNCH THEOREM</strong>:  If you make absolutely no assumption about the data, then there is no reason to prefer one model over any other. There is no model that is a priori guaranteed to work better A model is a simplified version of the observations.  The simplifications are meant to discard the superfluous details that are unlikely to generalize to new instances</p>
</blockquote>

<h1 id="ch2-end-to-end-machine-learning-project">
<a class="anchor" href="#ch2-end-to-end-machine-learning-project" aria-hidden="true"><span class="octicon octicon-link"></span></a>CH2. End-to-End Machine Learning Project</h1>

<p>This chapter presents an example ml project:</p>

<ol>
  <li><strong>Look at the big picture</strong></li>
</ol>

<h3 id="pull-out-your-ml-project-checklist">
<a class="anchor" href="#pull-out-your-ml-project-checklist" aria-hidden="true"><span class="octicon octicon-link"></span></a>Pull out your ML project checklist</h3>
<h3 id="frame-the-problem-what-exactly-the-business-objective-is-how-does-the-company-expect-to-use-and-benefit-from-this-model">
<a class="anchor" href="#frame-the-problem-what-exactly-the-business-objective-is-how-does-the-company-expect-to-use-and-benefit-from-this-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Frame the problem: what exactly the business objective is. How does the company expect to use and benefit from this model?</h3>

<blockquote>
  <p><strong>PIPELINES</strong>: Sequence of data processing components = data pipeline. Each component is fairly self-contained</p>
</blockquote>

<ul>
  <li>What the current solution looks like?</li>
  <li>Frame the problem: supervised? classification/regression? batch/online? etc
    <h3 id="select-a-performance-measure-rmse-mae-accuracy-etc">
<a class="anchor" href="#select-a-performance-measure-rmse-mae-accuracy-etc" aria-hidden="true"><span class="octicon octicon-link"></span></a>Select a Performance Measure: RMSE, MAE, accuracy, etc</h3>
    <h3 id="check-the-assumptions-by-you-or-others">
<a class="anchor" href="#check-the-assumptions-by-you-or-others" aria-hidden="true"><span class="octicon octicon-link"></span></a>Check the Assumptions (by you or others)</h3>
  </li>
</ul>

<ol>
  <li><strong>Get the data</strong></li>
</ol>

<h3 id="create-an-isolated-environment">
<a class="anchor" href="#create-an-isolated-environment" aria-hidden="true"><span class="octicon octicon-link"></span></a>Create an isolated environment</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 <span class="nt">-m</span> pip <span class="nb">install</span> <span class="nt">--user</span> <span class="nt">-U</span> virtualenv
virtualenv my_env
<span class="nb">source </span>my_env/bin/activate
<span class="c"># deactivate</span>
</code></pre></div></div>
<h3 id="data-structure">
<a class="anchor" href="#data-structure" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data Structure</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pandas DataFrame methods
</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
<span class="o">.</span><span class="n">info</span><span class="p">()</span>
<span class="o">.</span><span class="n">describe</span><span class="p">()</span>
<span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="o">.</span><span class="n">hist</span><span class="p">()</span>
</code></pre></div></div>
<h3 id="create-a-test-set-20-or-less-if-the-dataset-is-very-large">
<a class="anchor" href="#create-a-test-set-20-or-less-if-the-dataset-is-very-large" aria-hidden="true"><span class="octicon octicon-link"></span></a>Create a Test Set (20% or less if the dataset is very large)</h3>
<blockquote>
  <p><strong>WARNING</strong>: before you look at the data any further, you need to create a test set, put it aside, and never look at it -&gt; avoid the <em>data snooping</em> bias
```python
from sklearn.model_selection import train_test_split</p>
</blockquote>

<p>train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>### Option: Stratified sampling
```python
from sklearn.model_selection import StratifiedShuffleSplit

split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_index, test_index in split.split(housing, housing["income_cat"]):
    strat_train_set = housing.loc[train_index]
    strat_test_set = housing.loc[test_index]

for set_ in (strat_train_set, strat_test_set):
    set_.drop("income_cat", axis=1, inplace=True) 
</code></pre></div></div>

<ol>
  <li>Discover and visualize the data to gain insights</li>
</ol>

<h3 id="put-the-test-set-aside-and-only-explore-the-training-set-if-the-training-set-is-large-you-may-want-to-sample-an-exploration-set">
<a class="anchor" href="#put-the-test-set-aside-and-only-explore-the-training-set-if-the-training-set-is-large-you-may-want-to-sample-an-exploration-set" aria-hidden="true"><span class="octicon octicon-link"></span></a>Put the test set aside and only explore the training set. If the training set is large, you may want to sample an exploration set</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># make a copy to avoid harming the training set
</span><span class="n">housing</span> <span class="o">=</span> <span class="n">strat_train_set</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</code></pre></div></div>
<h3 id="we-are-very-good-at-spotting-patterns-in-pictures">
<a class="anchor" href="#we-are-very-good-at-spotting-patterns-in-pictures" aria-hidden="true"><span class="octicon octicon-link"></span></a>We are very good at spotting patterns in pictures</h3>
<h3 id="look-for-correlations">
<a class="anchor" href="#look-for-correlations" aria-hidden="true"><span class="octicon octicon-link"></span></a>Look for Correlations</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">corr_matrix</span> <span class="o">=</span> <span class="n">housing</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
<span class="c1"># how each attribute correlates with one specific
</span><span class="n">corr_matrix</span><span class="p">[</span><span class="s">"specific_attr"</span><span class="p">]</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="c1"># alternative
</span><span class="kn">from</span> <span class="nn">pandas.plotting</span> <span class="kn">import</span> <span class="n">scatter_matrix</span>
<span class="n">scatter_matrix</span><span class="p">(</span><span class="n">housing</span><span class="p">[</span><span class="n">attributes</span><span class="p">],</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
</code></pre></div></div>
<blockquote>
  <p><strong>WARNING</strong>: Correlation coefficient only measures linear correlation, it may completely miss out on non-linear relationships!</p>
  <h3 id="experimenting-with-attribute-combinations">
<a class="anchor" href="#experimenting-with-attribute-combinations" aria-hidden="true"><span class="octicon octicon-link"></span></a>Experimenting with Attribute Combinations</h3>
  <p>Attributes with tail-heavy distribution? You may want to transform then (e.g. logarithm)</p>
</blockquote>

<p>After engineering features, you may want to look at the correlations again to check if the features created are more correlated with the target.</p>

<p>This is an iterative process: get a prototype up and running, analyze its output, come back to this exploration step</p>

<ol>
  <li><strong>Prepare the data for ML algorithms</strong></li>
</ol>

<p>Instead of doing this manually, you should <strong>write functions</strong> for this purpose: reproductibility, reuse in your live system, quickly try various transformations to see which combination works best</p>

<h3 id="data-cleaning">
<a class="anchor" href="#data-cleaning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data Cleaning</h3>
<p>Missing values</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># pandas methods
</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>
<span class="o">.</span><span class="n">drop</span><span class="p">()</span>
<span class="o">.</span><span class="n">fillna</span><span class="p">()</span>
</code></pre></div></div>
<blockquote>
  <p><strong>WARNING</strong>: if you choose to fill the missing values, save the value you computed to fill the training set. You will need it later to replace missing values in the test set.
```python
from sklearn.impute import SimpleImputer</p>
</blockquote>

<p>imputer = SimpleImputer(strategy=”median”)</p>
<h1 id="the-median-can-only-be-used-on-numerical-attributes">
<a class="anchor" href="#the-median-can-only-be-used-on-numerical-attributes" aria-hidden="true"><span class="octicon octicon-link"></span></a>the median can only be used on numerical attributes</h1>
<p>imputer.fit(housing_num)
X = imputer.transform(housing_num)</p>

<p>housing_num_tr = pd.DataFrame(X, 
                    columns=housing_num.columns,
                    index=housing_num.index)</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
### Handling Text and Categorical Attributes
- Ordinal Encoder
```python
from sklearn.preprocessing import OrdinalEncoder
ordinal_encoder = OrdinalEncoder()
housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)
# problem: ML algorithms will assume two nearby values are more similar than two distant values. That may not be the case
</code></pre></div></div>

<ul>
  <li>One Hot Encoder
    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>
<span class="n">cat_encoder</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">()</span>
<span class="n">housing_cat_1hot</span> <span class="o">=</span> <span class="n">cat_encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">housing_cat</span><span class="p">)</span>
</code></pre></div>    </div>
    <blockquote>
      <p><strong>TIP</strong>: attributes with large number of possible categories = large number of input features. You may want to replace the categorical input with useful numerical features related to it</p>
    </blockquote>
  </li>
</ul>

<h3 id="feature-scaling">
<a class="anchor" href="#feature-scaling" aria-hidden="true"><span class="octicon octicon-link"></span></a>Feature Scaling</h3>
<p>With few exceptions, ML algorithms don’t perform well when the input numerical attributes have very different scales</p>

<ul>
  <li>Min-max scaling (<em>normalization</em>) -&gt; values are shifted and rescaled so that they end up ranging from 0 to 1. (x_i - min_x) / (max_x - min_x). <em>MinMaxScaler</em> on Scikit-Learn</li>
  <li>Standardization (zero mean) -&gt; (x_i - mean_x) / std_x. Doesn’t bound values to a specific range, but is much less affected by outliers. <strong>StandardScaler</strong> on Scikit-Learn.</li>
</ul>

<h3 id="transformation-pipelines">
<a class="anchor" href="#transformation-pipelines" aria-hidden="true"><span class="octicon octicon-link"></span></a>Transformation Pipelines</h3>
<p>Scikit-Learn provides the <em>Pipeline</em> class to help with sequences of transformations</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">num_pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
        <span class="p">(</span><span class="s">'imputer'</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s">"median"</span><span class="p">)),</span>
        <span class="p">(</span><span class="s">'attribs_adder'</span><span class="p">,</span> <span class="n">CombinedAttributesAdder</span><span class="p">()),</span>
        <span class="p">(</span><span class="s">'std_scaler'</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">])</span>

<span class="n">housing_num_tr</span> <span class="o">=</span> <span class="n">num_pipeline</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">housing_num</span><span class="p">)</span> 
</code></pre></div></div>

<ul>
  <li>We have handled numerical and categorical columns separately, but Scikit-Learn has a single transformer able to handle all columns
```python
from sklearn.compose import ColumnTransformer</li>
</ul>

<p>num_attribs = list(housing_num)
cat_attribs = [“ocean_proximity”]</p>

<p>full_pipeline = ColumnTransformer([
    (“num”, num_pipeline, num_attribs),
    (“cat”, OneHotEncoder(), cat_attribs),
])</p>

<p>housing_prepared = full_pipeline.fit_transform(housing)</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
&gt; **Tip**: by default, the remaining columns will be dropped. To avoid this, you can specify "passthrough"

5. **Select a model and train it**

### Training and Evaluating on the Training Set
```python
from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(housing_prepared, housing_labels)

from sklearn.metrics import mean_squared_error
housing_predictions = lin_reg.predict(housing_prepared)
lin_mse = mean_squared_error(housing_labels, housing_predictions)
lin_rmse = np.sqrt(lin_mse)
</code></pre></div></div>

<h3 id="better-evaluation-using-cross-validation">
<a class="anchor" href="#better-evaluation-using-cross-validation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Better Evaluation Using Cross-Validation</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">tree_reg</span><span class="p">,</span> <span class="n">housing_prepared</span><span class="p">,</span> <span class="n">housing_labels</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s">"neg_mean_squared_error"</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">tree_rmse_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="o">-</span><span class="n">scores</span><span class="p">)</span> 
<span class="c1"># cross_val_score expects a utility fn (greater is better)
# rather than a cost fn (lower is better)
</span></code></pre></div></div>

<p>Try out many other models from various categories, without spending too much time tweaking the hyperparameters. The goal is to shortlist a few (two to five) promising models.</p>

<blockquote>
  <p><strong>Tip</strong>: save the models you experiment
```python
import joblib</p>
</blockquote>

<p>joblib.dump(my_model, “my_model.pkl”)</p>
<h1 id="and-later">
<a class="anchor" href="#and-later" aria-hidden="true"><span class="octicon octicon-link"></span></a>and later</h1>
<p>my_model_loaded = joblib.load(“my_model.pkl”)</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
6. **Fine-tune your model**
### Grid Search
```python
from sklearn.model_selection import GridSearchCV

param_grid = [
    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},
    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},
  ]

forest_reg = RandomForestRegressor()

grid_search = GridSearchCV(forest_reg, param_grid, cv=5,
                           scoring='neg_mean_squared_error',
                           return_train_score=True)

grid_search.fit(housing_prepared, housing_labels)

# and after
grid_search.best_params_
grid_search.best_estimator_
</code></pre></div></div>

<h3 id="randomized-search">
<a class="anchor" href="#randomized-search" aria-hidden="true"><span class="octicon octicon-link"></span></a>Randomized Search</h3>
<p>When the hyperparameter search space is large, it is often preferable to use <em>RandomizedSearchCV</em></p>

<h3 id="ensemble-methods">
<a class="anchor" href="#ensemble-methods" aria-hidden="true"><span class="octicon octicon-link"></span></a>Ensemble Methods</h3>
<p>Another way to fine-tune your system is to try to combine the models that perform best. The group (<em>ensemble</em>) will often perform better than the best individual model</p>

<h3 id="analyze-the-best-models-and-their-errors">
<a class="anchor" href="#analyze-the-best-models-and-their-errors" aria-hidden="true"><span class="octicon octicon-link"></span></a>Analyze the Best Models and Their Errors</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">feature_importances</span> <span class="o">=</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">feature_importances_</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">extra_attribs</span> <span class="o">=</span> <span class="p">[</span><span class="s">"rooms_per_hhold"</span><span class="p">,</span> <span class="s">"pop_per_hhold"</span><span class="p">,</span> <span class="s">"bedrooms_per_room"</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">cat_encoder</span> <span class="o">=</span> <span class="n">full_pipeline</span><span class="o">.</span><span class="n">named_transformers_</span><span class="p">[</span><span class="s">"cat"</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">cat_one_hot_attribs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">cat_encoder</span><span class="o">.</span><span class="n">categories_</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">attributes</span> <span class="o">=</span> <span class="n">num_attribs</span> <span class="o">+</span> <span class="n">extra_attribs</span> <span class="o">+</span> <span class="n">cat_one_hot_attribs</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">feature_importances</span><span class="p">,</span> <span class="n">attributes</span><span class="p">),</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="p">[(</span><span class="mf">0.3661589806181342</span><span class="p">,</span> <span class="s">'median_income'</span><span class="p">),</span>
 <span class="p">(</span><span class="mf">0.1647809935615905</span><span class="p">,</span> <span class="s">'INLAND'</span><span class="p">),</span>
 <span class="p">(</span><span class="mf">0.10879295677551573</span><span class="p">,</span> <span class="s">'pop_per_hhold'</span><span class="p">),</span>
 <span class="p">(</span><span class="mf">0.07334423551601242</span><span class="p">,</span> <span class="s">'longitude'</span><span class="p">),</span>
 <span class="p">(</span><span class="mf">0.0629090704826203</span><span class="p">,</span> <span class="s">'latitude'</span><span class="p">),</span>
 <span class="p">(</span><span class="mf">0.05641917918195401</span><span class="p">,</span> <span class="s">'rooms_per_hhold'</span><span class="p">),</span>
 <span class="p">(</span><span class="mf">0.05335107734767581</span><span class="p">,</span> <span class="s">'bedrooms_per_room'</span><span class="p">),</span>
 <span class="p">(</span><span class="mf">0.041143798478729635</span><span class="p">,</span> <span class="s">'housing_median_age'</span><span class="p">),</span>
 <span class="p">(</span><span class="mf">0.014874280890402767</span><span class="p">,</span> <span class="s">'population'</span><span class="p">),</span>
 <span class="p">(</span><span class="mf">0.014672685420543237</span><span class="p">,</span> <span class="s">'total_rooms'</span><span class="p">),</span>
 <span class="p">(</span><span class="mf">0.014257599323407807</span><span class="p">,</span> <span class="s">'households'</span><span class="p">),</span>
 <span class="p">(</span><span class="mf">0.014106483453584102</span><span class="p">,</span> <span class="s">'total_bedrooms'</span><span class="p">),</span>
 <span class="p">(</span><span class="mf">0.010311488326303787</span><span class="p">,</span> <span class="s">'&lt;1H OCEAN'</span><span class="p">),</span>
 <span class="p">(</span><span class="mf">0.002856474637320158</span><span class="p">,</span> <span class="s">'NEAR OCEAN'</span><span class="p">),</span>
 <span class="p">(</span><span class="mf">0.00196041559947807</span><span class="p">,</span> <span class="s">'NEAR BAY'</span><span class="p">),</span>
 <span class="p">(</span><span class="mf">6.028038672736599e-05</span><span class="p">,</span> <span class="s">'ISLAND'</span><span class="p">)]</span>
</code></pre></div></div>
<p>You may want to try dropping less useful features or understand errors your system makes</p>

<h3 id="evaluate-your-system-on-the-test-set">
<a class="anchor" href="#evaluate-your-system-on-the-test-set" aria-hidden="true"><span class="octicon octicon-link"></span></a>Evaluate Your System on the Test Set</h3>
<p>Get the predictors and the labels from your test set, run your <em>full_pipeline</em> to transform the data (call transform(), not fit_transform()), and evaluate the final model on the test set:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">final_model</span> <span class="o">=</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_estimator_</span>

<span class="n">X_test</span> <span class="o">=</span> <span class="n">strat_test_set</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s">"median_house_value"</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">strat_test_set</span><span class="p">[</span><span class="s">"median_house_value"</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="n">X_test_prepared</span> <span class="o">=</span> <span class="n">full_pipeline</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">final_predictions</span> <span class="o">=</span> <span class="n">final_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_prepared</span><span class="p">)</span>

<span class="n">final_mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">final_predictions</span><span class="p">)</span>
<span class="n">final_rmse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">final_mse</span><span class="p">)</span>   <span class="c1"># =&gt; evaluates to 47,730.2
</span></code></pre></div></div>

<blockquote>
  <p>Computing a 95% confidence interval for the generalization</p>
  <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;</span> <span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
<span class="o">&gt;&gt;</span> <span class="n">confidence</span> <span class="o">=</span> <span class="mf">0.95</span>
<span class="o">&gt;&gt;</span> <span class="n">squared_errors</span> <span class="o">=</span> <span class="p">(</span><span class="n">final_predictions</span> <span class="o">-</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
<span class="o">&gt;&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">interval</span><span class="p">(</span><span class="n">confidence</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">squared_errors</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
<span class="o">...</span>                          <span class="n">loc</span><span class="o">=</span><span class="n">squared_errors</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span>
<span class="o">...</span>                          <span class="n">scale</span><span class="o">=</span><span class="n">stats</span><span class="o">.</span><span class="n">sem</span><span class="p">(</span><span class="n">squared_errors</span><span class="p">)))</span>
<span class="o">...</span>
<span class="n">array</span><span class="p">([</span><span class="mf">45685.10470776</span><span class="p">,</span> <span class="mf">49691.25001878</span><span class="p">])</span>
</code></pre></div>  </div>
</blockquote>

<p>If you did a lot of hyperparameter tuning, the performance may be worse than your cv (slightly overfit to the training set). Resist the temptation to tweak the hyperparameters to make the numbers look good on the test set</p>

<ol>
  <li><strong>Present your solution</strong></li>
</ol>

<h3 id="project-prelaunch-phase">
<a class="anchor" href="#project-prelaunch-phase" aria-hidden="true"><span class="octicon octicon-link"></span></a>Project prelaunch phase</h3>
<ul>
  <li>What you have learned, what worked/did not, what assumptions were made, what your system’s limitations are</li>
  <li>Document everything</li>
  <li>Create nice presentations with clear visualizations and easy-to-remember statements</li>
</ul>

<ol>
  <li>
<strong>Launch, monitor, and maintain your system</strong>
Get your solution ready for production (e.g., polish the code, write documentation and tests, and so on).</li>
</ol>

<p>Then you can deploy your model to your production environment.</p>

<p>One way to do this is to save the trained Scikit-Learn model (e.g., using joblib), including the full preprocessing and prediction pipeline, then load this trained model within your production environment and use it to make predictions by calling its predict() method.</p>

<p>But deployment is not the end of the story. You also need to write monitoring code to check your system’s live performance at regular intervals and trigger alerts when it drops</p>

<p>If the data keeps evolving, you will need to update your datasets and retrain your model regularly. You should probably automate the whole process as much as possible. Here are a few things you can automate:</p>

<ul>
  <li>Collect fresh data regularly and label it (e.g., using human raters).</li>
  <li>Write a script to train the model and fine-tune the hyperparameters automatically. This script could run automatically, for example every day or every week, depending on your needs.</li>
  <li>Write another script that will evaluate both the new model and the previous model on the updated test set, and deploy the model to production if the performance has not decreased (if it did, make sure you investigate why)</li>
  <li>Evaluate the model’s input data quality (missing features, stdev drifts too far from the training set, new categories)</li>
  <li>Keep backups of every model you create and have the process and tools in place to roll back to a previous model quickly</li>
  <li>Keep backups of every version of the datasets too</li>
</ul>

<h1 id="ch3-classification">
<a class="anchor" href="#ch3-classification" aria-hidden="true"><span class="octicon octicon-link"></span></a>CH3. Classification</h1>
<blockquote>
  <p>Some learning algorithms are sensitive to the order of the training instances, and they perform poorly if they get many similar instances in a row. <strong>Shuffling</strong> the dataset ensures that this won’t happen</p>
</blockquote>

<h3 id="stochastic-gradient-descent-sgd-classifier">
<a class="anchor" href="#stochastic-gradient-descent-sgd-classifier" aria-hidden="true"><span class="octicon octicon-link"></span></a>Stochastic Gradient Descent (SGD) classifier</h3>
<p><em>SGDClassifier</em> on sklearn. Has the advantage of being capable of handling very large datasets efficiently. Deals with training instances independently, one at a time (suited for online learning)</p>

<h3 id="performance-measures">
<a class="anchor" href="#performance-measures" aria-hidden="true"><span class="octicon octicon-link"></span></a>Performance Measures</h3>
<h3 id="measuring-accuracy-using-cross-validation">
<a class="anchor" href="#measuring-accuracy-using-cross-validation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Measuring Accuracy using Cross-Validation</h3>
<p>The snippet below does roughly the same thing as <em>cross_val_score()</em> from sklearn, but with stratified sampling</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">StratifiedKFold</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">clone</span>

<span class="n">skfolds</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="n">skfolds</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train_5</span><span class="p">):</span>
    <span class="n">clone_clf</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="n">sgd_clf</span><span class="p">)</span>
    <span class="n">X_train_folds</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">train_index</span><span class="p">]</span>
    <span class="n">y_train_folds</span> <span class="o">=</span> <span class="n">y_train_5</span><span class="p">[</span><span class="n">train_index</span><span class="p">]</span>
    <span class="n">X_test_fold</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
    <span class="n">y_test_fold</span> <span class="o">=</span> <span class="n">y_train_5</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>

    <span class="n">clone_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_folds</span><span class="p">,</span> <span class="n">y_train_folds</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">clone_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_fold</span><span class="p">)</span>
    <span class="n">n_correct</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">==</span> <span class="n">y_test_fold</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">n_correct</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_pred</span><span class="p">))</span>  <span class="c1"># prints 0.9502, 0.96565, and 0.96495”
</span></code></pre></div></div>
<blockquote>
  <p>High accuracy can be deceiving if you are dealing with <em>skewed datasets</em> (i.e., when some classes are much more frequent than others)</p>
</blockquote>

<h3 id="confusion-matrix">
<a class="anchor" href="#confusion-matrix" aria-hidden="true"><span class="octicon octicon-link"></span></a>Confusion Matrix</h3>
<p>Count the number of times instances of class A are classified as class B</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_train_5</span><span class="p">,</span> <span class="n">y_train_pred</span><span class="p">)</span>
</code></pre></div></div>

<p>Each row represents an <em>actual class</em>, while each column represents a <em>predicted class</em></p>

<h3 id="precision">
<a class="anchor" href="#precision" aria-hidden="true"><span class="octicon octicon-link"></span></a>Precision</h3>
<p>Accuracy of the positive predictions</p>

<blockquote>
  <p>precision = TP/(TP + FP)</p>
</blockquote>

<p>TP is the number of true positives, and FP is the number of false positives</p>

<h3 id="recall-or-sensitivity-or-true-positive-rate-tpr">
<a class="anchor" href="#recall-or-sensitivity-or-true-positive-rate-tpr" aria-hidden="true"><span class="octicon octicon-link"></span></a>Recall or Sensitivity, or True Positive Rate (TPR)</h3>
<p>Ratio of positive instances that are correctly detected by the classifier</p>

<blockquote>
  <p>recall = TP/(TP + FN)</p>
</blockquote>

<p>FN is the number of false negatives</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span>
<span class="n">precision_score</span><span class="p">(</span><span class="n">y_train_5</span><span class="p">,</span> <span class="n">y_train_pred</span><span class="p">)</span>
<span class="n">recall_score</span><span class="p">(</span><span class="n">y_train_5</span><span class="p">,</span> <span class="n">y_train_pred</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="f1-score">
<a class="anchor" href="#f1-score" aria-hidden="true"><span class="octicon octicon-link"></span></a>F1 Score</h3>
<p>Harmonic mean of the precision and recall. The harmonic mean gives much more weight to low values, so the classifier will only get a high F1 score if both recall and precision are high</p>

<blockquote>
  <p>F1 = 2<em>(precision</em>recall)/(precision+recall)</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span>
<span class="n">f1_score</span><span class="p">(</span><span class="n">y_train_5</span><span class="p">,</span> <span class="n">y_train_pred</span><span class="p">)</span>
</code></pre></div></div>

<p>The F1 score favors classifiers that have similar precision and recall</p>

<h3 id="precisionrecall-trade-off">
<a class="anchor" href="#precisionrecall-trade-off" aria-hidden="true"><span class="octicon octicon-link"></span></a>Precision/Recall trade-off</h3>
<blockquote>
  <p><em>Precision/recall trade-off</em>: increasing precision reduces recall, and vice versa. e.g., videos safe for kids: prefer reject many good videos (low recall), but keeps only safe ones (high precision)</p>
</blockquote>

<p>Scikit-Learn gives you access to the decision scores that it uses to make predicitions, <em>.decision_function()</em> method, which returns a score for each instance and then use any threshold you want to make predictions based on those scores</p>

<p>For RandomForestClassifier for example, the method to use is <em>.predict_proba()</em>, which returns an array conatining a row per instance and a column per class, each containing the probability that the given instance belongs to the given class.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_scores</span> <span class="o">=</span> <span class="n">cross_val_predict</span><span class="p">(</span><span class="n">sgd_clf</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train_5</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">"decision_function"</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">precision_recall_curve</span>
<span class="n">precisions</span><span class="p">,</span> <span class="n">recalls</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">precision_recall_curve</span><span class="p">(</span><span class="n">y_train_5</span><span class="p">,</span> <span class="n">y_scores</span><span class="p">)</span>
</code></pre></div></div>

<p>You can plot precision_recall_vs_threshold and choose a good threshold for your project, or plot precision directly against recall (generally you select a precision/recall just before the drop in the plot)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># define threshold
</span><span class="n">threshold_90_precision</span> <span class="o">=</span> <span class="n">thresholds</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">precisions</span> <span class="o">&gt;=</span> <span class="mf">0.90</span><span class="p">)]</span>
<span class="c1"># make predictions
</span><span class="n">y_train_pred_90</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_scores</span> <span class="o">&gt;=</span> <span class="n">threshold_90_precision</span><span class="p">)</span>
<span class="c1"># check results
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">y_train_5</span><span class="p">,</span> <span class="n">y_train_pred_90</span><span class="p">)</span>
<span class="mf">0.9000380083618396</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y_train_5</span><span class="p">,</span> <span class="n">y_train_pred_90</span><span class="p">)</span>
<span class="mf">0.4368197749492714</span>
</code></pre></div></div>

<h3 id="the-roc-curve">
<a class="anchor" href="#the-roc-curve" aria-hidden="true"><span class="octicon octicon-link"></span></a>The ROC Curve</h3>
<p><em>Receiver operating characteristic</em> (ROC) curve. Plots the <em>true positive rate</em> (recall) against the <em>false positive rate</em> (FPR). The FPR is the ratio of negative instances that are incorrectly classified as positive. It is equal to 1 - <em>true negative rate</em> (TNR, or <em>specificity</em>) which is the ratio of negative instances that are correctly classified as negative</p>

<p>ROC curve plots sensitivity (recall) versus 1 - specificity (<em>.roc_curve()</em>)</p>

<blockquote>
  <p>The higher the recall (TPR), the more false positives (FPR) the classifier produces. The purely random classifier is the diagonal line in the plot, a good classifier stays as far away from that line as possible (toward the top-left corner)</p>
</blockquote>

<h3 id="area-under-the-curve-auc">
<a class="anchor" href="#area-under-the-curve-auc" aria-hidden="true"><span class="octicon octicon-link"></span></a>Area under the curve (AUC)</h3>
<p>A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5 (<em>.roc_auc_score()</em>)</p>

<blockquote>
  <p>You should prefer the PR curve whenever the positive class is rare or when you care more about the false positives than the false negatives. Otherwise, use the ROC curve.</p>
</blockquote>

<h3 id="binary-classifiers">
<a class="anchor" href="#binary-classifiers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Binary classifiers</h3>
<ol>
  <li>Choose the appropriate metrics</li>
  <li>Evaluate your classifiers using cross-validation</li>
  <li>Select the precision/recall trade-off that fits your needs</li>
  <li>Use ROC curves and ROC AUC scores to compare various models</li>
</ol>

<h3 id="multiclass-classification">
<a class="anchor" href="#multiclass-classification" aria-hidden="true"><span class="octicon octicon-link"></span></a>Multiclass Classification</h3>
<p>Some algorithms are not capable of handling multiple classes natively (e.g., Logistic Regression, SVM). For 10 classes you would train 10 binary classifiers and select the class whose classifier outputs the highest score. This is the <em>one-versus-the-rest</em> (OvR) strategy (also called <em>one-versus-all</em>)</p>

<p><em>One-versus-one</em> (OvO) strategy: trains a binary classifier for every pair of digits. (N*(N-1))/2)classifiers! Good strategy for SVM that scales poorly with the size of the training set</p>

<blockquote>
  <p>Scikit-Learn detects when you try to use a binary classification algorithm for a multiclass classification task, and it automatically runs OvR or OvO, depending on the algorithm</p>
</blockquote>

<h3 id="error-analysis">
<a class="anchor" href="#error-analysis" aria-hidden="true"><span class="octicon octicon-link"></span></a>Error Analysis</h3>
<p>Analyzing the confusion matrix often gives you insights into ways to improve your classifier.</p>

<h3 id="multilabel-classification">
<a class="anchor" href="#multilabel-classification" aria-hidden="true"><span class="octicon octicon-link"></span></a>Multilabel Classification</h3>
<p>Outputs multiple binary tags e.g., face recognition with Alice, Bob and Charlie; only Alice and Charlie in a picture -&gt; output [1, 0, 1]</p>

<blockquote>
  <p>Evaluate a multilabel classifier: One approach is to measure the F1 score for each individual label, then simply compute the average score</p>
</blockquote>

<h3 id="multioutput-classification-multioutput-multiclass-classification">
<a class="anchor" href="#multioutput-classification-multioutput-multiclass-classification" aria-hidden="true"><span class="octicon octicon-link"></span></a>Multioutput Classification (multioutput-multiclass classification)</h3>
<p>Generalization of multilabel classification where each label can be multiclass (i.e., it can have more than two possible values)</p>

<h1 id="ch4-training-models">
<a class="anchor" href="#ch4-training-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>CH4. Training Models</h1>

<h3 id="linear-regression">
<a class="anchor" href="#linear-regression" aria-hidden="true"><span class="octicon octicon-link"></span></a>Linear Regression</h3>
<p>A linear model makes a prediction by simply computing a weighted sum of the input features, plus a constant called the bias term (also called the intercept term)</p>

<p>A <em>closed-form solution</em>, a mathematical equation that gives the result directly</p>

<h3 id="gradient-descent">
<a class="anchor" href="#gradient-descent" aria-hidden="true"><span class="octicon octicon-link"></span></a>Gradient Descent</h3>
<p>Generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function.</p>

<p>It measures the local gradient of the error function with regard to the parameter vector θ, and it goes in the direction of descending gradient. Once the gradient is zero, you have reached a minimum!</p>

<p>The size of the steps, is determined by the <strong>learning rate</strong> hyperparameter. If the learning rate is too small, then the algorithm will have to go through many iterations to converge, which will take a long time. If the learning rate is too high, you might jump across the valley and end up on the other side, possibly even higher up than you were before. This might make the algorithm diverge, with larger and larger values, failing to find a good solution</p>

<blockquote>
  <p>The MSE cost function for a Linear Regression is a <em>convex function</em>: if you pick any two points on the curve, the line segment joining them never crosses the curve. This implies that there are no local minima, just one global minimum. It is also a continuous function with a slope that never changes abruptly. Consequence: Gradient Descent is guaranteed to approach arbitrarily close the global minimum (if you wait long enough and if the learning rate is not too high).</p>
</blockquote>

<blockquote>
  <p>When using Gradient Descent, you should ensure that all features have a similar scale, or else it will take much longer to converge.</p>
</blockquote>

<h3 id="batch-vs-stochastic-gradient-descent">
<a class="anchor" href="#batch-vs-stochastic-gradient-descent" aria-hidden="true"><span class="octicon octicon-link"></span></a>Batch vs Stochastic Gradient Descent</h3>
<p>The main problem with Batch Gradient Descent is that it uses the whole training set to compute the gradients at every step -&gt; very slow when training set is large</p>

<p>Stochastic Gradient Descent picks a random instance in the training set at every step and computes the gradients based only on that single instance -&gt; algorithm much faster because it has very little data to manipulate at every iteration. Possible to train on huge training sets, since only one instance in memory at each iteration</p>

<blockquote>
  <p>Randomness is good to escape from local optima, but bad because it means that the algorithm can never settle at the minimum -&gt; solution to this dilemma is to gradually reduce the learning rate</p>
</blockquote>

<h3 id="learning-curves">
<a class="anchor" href="#learning-curves" aria-hidden="true"><span class="octicon octicon-link"></span></a>Learning Curves</h3>
<p>Learning curves typical of a model that’s underfitting: Both curves have reached a plateau; they are close and fairly high.</p>

<blockquote>
  <p>If your model is underfitting the training data, adding more training examples will not help. You need to use a more complex model or come up with better features</p>
</blockquote>

<p>If there is a gap between the curves. This means that the model performs significantly better on the training data than on the validation data, which is the hallmark of an overfitting model. If you used a much larger training set, however, the two curves would continue to get closer -&gt; feed more training data until the validation error reaches the training error</p>

<h3 id="biasvariance-trade-off">
<a class="anchor" href="#biasvariance-trade-off" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bias/Variance Trade-off</h3>
<ul>
  <li>
<strong>Bias</strong>: Error due to wrong assumptions. A high-bias model is most likely to underfit the training data</li>
  <li>
<strong>Variance</strong>: Error due to model’s excessive sensitivity to small variations in the training data. Model with many degrees of freedom is likely to have high variance and thus overfit the training data</li>
  <li>
<strong>Irreducible error</strong>: due to the noiseness of the data itself. The only way to reduce this part of the error is to clean up the data</li>
</ul>

<blockquote>
  <p>Increasing a model’s complexity will typically increase its variance and reduce its bias. Conversely, reducing a model’s complexity increases its bias and reduces its variance. This is why it is called a trade-off.</p>
</blockquote>

<h3 id="ridge-regression-tikhonov-regularization---l2">
<a class="anchor" href="#ridge-regression-tikhonov-regularization---l2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Ridge Regression (Tikhonov regularization) - L2</h3>
<p>Keep the models weights as small as possible. It is important to scale the data before performing Ridge Regression, as it is sensitive to the scale of the input features. This is true of most regularized models.</p>

<p>Note that the regularization term should only be added to the cost function during training. Once the model is trained, you want to use the unregularized performance measure to evaluate the model’s performance.</p>

<blockquote>
  <p>It is quite common for the cost function used during training to be different from the performance measure used for testing. Apart from regularization, another reason they might be different is that a good training cost function should have optimization-friendly derivatives, while the performance measure used for testing should be as close as possible to the final objective.</p>
</blockquote>

<h3 id="lasso-regression---l1">
<a class="anchor" href="#lasso-regression---l1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Lasso Regression - L1</h3>
<p><em>Least Absolute Shrinkage and Selection Operator Regression</em>. Tends to eliminate the weights of the least important features</p>

<h3 id="elastic-net">
<a class="anchor" href="#elastic-net" aria-hidden="true"><span class="octicon octicon-link"></span></a>Elastic Net</h3>
<p>Regularization term is a simple mix of both Ridge and Lasso’s regularization terms</p>

<blockquote>
  <p><strong>When should you use plain Linear Regression (i.e., without any regularization), Ridge, Lasso, or Elastic Net?</strong>
It is almost always preferable to have at least a little bit of regularization, so generally you should avoid plain Linear Regression. Ridge is a good default, but if you suspect that only a few features are useful, you should prefer Lasso or Elastic Net because they tend to reduce the useless features’ weights down to zero. In general, Elastic Net is preferred over Lasso because Lasso may behave erratically when the number of features is greater than the number of training instances or when several features are strongly correlated</p>
</blockquote>

<h3 id="early-stopping">
<a class="anchor" href="#early-stopping" aria-hidden="true"><span class="octicon octicon-link"></span></a>Early Stopping</h3>
<p>Another way to regularize iterative learning algorithms. Stop training as soon as the validation error reaches a minimum. “Beautiful free lunch”, Geoffrey Hinton</p>

<h3 id="logistic-regression-logit-regression">
<a class="anchor" href="#logistic-regression-logit-regression" aria-hidden="true"><span class="octicon octicon-link"></span></a>Logistic Regression (Logit Regression)</h3>
<p>Estimate the <strong>probability</strong> that an instance belongs to a particular class. Greater than 50% -&gt; positive class, else negative class (binary classifier)</p>

<p>Logistic Regression cost function = log loss</p>

<blockquote>
  <p>logit(p) = ln(p/(1-p)) -&gt; also called log-odds</p>
</blockquote>

<h3 id="softmax-regression-multinomial-logistic-regression">
<a class="anchor" href="#softmax-regression-multinomial-logistic-regression" aria-hidden="true"><span class="octicon octicon-link"></span></a>Softmax Regression (Multinomial Logistic Regression)</h3>
<p>Computes a score for each class, then estimates the probability of each class by applying the <em>softmax function</em> (<em>normalized exponential</em>) to the scores</p>

<blockquote>
  <p>Cross entropy -&gt; frequently used to measure how well a set of estimated class probabilities matches the target classes (when k=2 -&gt; equivalent to log loss)</p>
</blockquote>

<h1 id="ch5-support-vector-machines">
<a class="anchor" href="#ch5-support-vector-machines" aria-hidden="true"><span class="octicon octicon-link"></span></a>CH5. Support Vector Machines</h1>

<p>Soon…</p>

<h1 id="ch6-decision-trees">
<a class="anchor" href="#ch6-decision-trees" aria-hidden="true"><span class="octicon octicon-link"></span></a>CH6. Decision Trees</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">export_graphviz</span> 

 <span class="n">export_graphviz</span> <span class="p">(</span> 
         <span class="n">tree_clf</span> <span class="p">,</span> 
         <span class="n">out_file</span> <span class="o">=</span> <span class="n">image_path</span> <span class="p">(</span> <span class="s">"iris_tree.dot"</span> <span class="p">),</span> 
         <span class="n">feature_names</span> <span class="o">=</span> <span class="n">iris</span> <span class="o">.</span> <span class="n">feature_names</span> <span class="p">[</span> <span class="mi">2</span> <span class="p">:],</span> 
         <span class="n">class_names</span> <span class="o">=</span> <span class="n">iris</span> <span class="o">.</span> <span class="n">target_names</span> <span class="p">,</span> 
         <span class="n">rounded</span> <span class="o">=</span> <span class="bp">True</span> <span class="p">,</span> 
         <span class="n">filled</span> <span class="o">=</span> <span class="bp">True</span> 
     <span class="p">)</span>
</code></pre></div></div>

<p>One of the many qualities of Decision Trees is that they require very little data preparation. In fact, they don’t require feature scaling or centering at all.</p>

<p>Scikit-Learn uses the  CART algorithm, which produces only  binary trees : nonleaf nodes always have two children (i.e., questions only have yes/no answers). However, other algorithms such as ID3 can produce Decision Trees with nodes that have more than two  children.</p>

<h3 id="whiteblack-box-models">
<a class="anchor" href="#whiteblack-box-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>White/Black box models</h3>

<p>Decision Trees are intuitive, and their decisions are easy to interpret. Such models are often  called  white box models. In contrast, as we will see, Random Forests or neural networks are generally considered  black box models</p>

<blockquote>
  <p>The CART algorithm is a  <strong>greedy algorithm</strong>: it greedily searches for an optimum split at the top level, then repeats the process at each subsequent level. It does not check whether or not the split will lead to the lowest possible impurity several levels down. A greedy algorithm often produces a solution that’s reasonably good but not guaranteed to be optimal.</p>
</blockquote>

<p>Making predictions requires traversing the Decision Tree from the root to a leaf. Decision Trees generally are approximately balanced, so traversing the Decision Tree requires going through roughly  O (log 2 ( m )) nodes. 3  Since each node only requires checking the value of one feature, the overall prediction complexity is  O (log 2 ( m )), independent of the number of features. So <strong>predictions are very fast, even when dealing with large training sets</strong>.</p>

<p>Gini impurity tends to isolate the most frequent class in its own branch of the tree, while entropy tends to produce slightly more balanced trees</p>

<p><strong>Nonparametric model</strong>, not because it does not have any parameters but because the number of parameters is not determined prior to training, so the model structure is free to stick closely to the data. In contrast, a  parametric model, such as a linear model, has a predetermined number of parameters, so its degree of freedom is limited, reducing the risk of overfitting (but increasing the risk of underfitting).</p>

<blockquote>
  <p>Increasing  min_*  hyperparameters or reducing  max_*  hyperparameters will regularize the model.</p>
</blockquote>

<h3 id="pruning">
<a class="anchor" href="#pruning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Pruning</h3>

<p>Standard statistical tests, such as the  χ 2   test  (chi-squared test), are used  to estimate the probability that the improvement is purely the result of chance (which is called the  null hypothesis ). If this probability, called the  p-value , is higher than a given threshold (typically 5%, controlled by a hyperparameter), then the node is considered unnecessary and its children are deleted</p>

<p>Decision Trees love orthogonal decision boundaries (all splits are perpendicular to an axis), which  makes them sensitive to training set rotation.</p>

<p>One way to limit this problem is to use Principal Component Analysis, which often results in a better orientation of the training data.</p>

<h3 id="problems">
<a class="anchor" href="#problems" aria-hidden="true"><span class="octicon octicon-link"></span></a>Problems</h3>

<p>The main issue with Decision Trees is that they are very sensitive to small variations in the training data.</p>

<p>Random Forests can limit this instability by averaging predictions over many trees</p>

<h1 id="ch7-ensemble-learning-and-random-forests">
<a class="anchor" href="#ch7-ensemble-learning-and-random-forests" aria-hidden="true"><span class="octicon octicon-link"></span></a>CH7. Ensemble Learning and Random Forests</h1>

<p><em>Wisdom of the crowd</em>: aggregated answer is better than an expert’s answer.</p>

<blockquote>
  <p>A  group of predictors is called an  ensemble ; thus, this technique is called  Ensemble Learning , and an Ensemble Learning algorithm  is called an  Ensemble method</p>
</blockquote>

<h3 id="hard-voting">
<a class="anchor" href="#hard-voting" aria-hidden="true"><span class="octicon octicon-link"></span></a>Hard voting</h3>

<p>Train a group of Decision Tree classifiers, each on a different random subset of the training set. To make predictions, you obtain the predictions of all the individual trees, then predict the class that gets the <strong>most votes</strong></p>

<p>Very simple way to create an even better classifier is to aggregate the predictions of each classifier and predict the class that gets the most votes. This  <strong>majority-vote classifier is called a  hard voting  classifier</strong></p>

<blockquote>
  <p>Even if each classifier is a  weak learner  (meaning it does only slightly better than random guessing), the ensemble can still be a  strong learner  (achieving high accuracy), provided there are a sufficient number of weak learners and they are sufficiently diverse.</p>
</blockquote>

<h3 id="independent-classifiers">
<a class="anchor" href="#independent-classifiers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Independent classifiers</h3>

<p>Ensemble methods  work best when the predictors are as independent from one another as possible. One way to get diverse classifiers is to train them using very different algorithms. This increases the chance that they will make very different types of errors, improving the ensemble’s accuracy.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">VotingClassifier</span> 
</code></pre></div></div>

<h3 id="soft-voting">
<a class="anchor" href="#soft-voting" aria-hidden="true"><span class="octicon octicon-link"></span></a>Soft voting</h3>

<p>If  all classifiers are able to estimate class probabilities (i.e., they all have a  predict_proba()  method), then you can tell Scikit-Learn to predict the class with the highest class probability, averaged over all the individual classifiers. This is called  <strong>soft voting</strong>. It often achieves higher performance than hard voting because it gives more weight to highly confident votes. All you need to do is replace  voting=”hard”  with  voting=”soft”  and ensure that all classifiers can estimate class probabilities</p>

<p>Generally, the net result is that the ensemble has a similar bias but a lower variance than a single predictor trained on the original training set.</p>

<h3 id="bagging-and-pasting">
<a class="anchor" href="#bagging-and-pasting" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bagging and Pasting</h3>

<p>Scikit-Learn  offers a simple API for both bagging and pasting with the  BaggingClassifier  class (or  BaggingRegressor  for regression).</p>

<h3 id="bootstrapping">
<a class="anchor" href="#bootstrapping" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bootstrapping</h3>

<p>Bootstrapping introduces a bit more diversity in the subsets that each predictor is trained on, so bagging ends up with a slightly higher bias than pasting; but the extra diversity also means that the predictors end up being less correlated, so the ensemble’s variance is reduced. Overall, bagging often results in better models, which explains why it is generally preferred</p>

<h3 id="out-of-bag-evaluation">
<a class="anchor" href="#out-of-bag-evaluation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Out-of-bag evaluation</h3>

<p>In Scikit-Learn, you can set  oob_score=True  when creating a  BaggingClassifier  to request an automatic oob evaluation after training</p>

<h3 id="random-patches-and-random-subspaces">
<a class="anchor" href="#random-patches-and-random-subspaces" aria-hidden="true"><span class="octicon octicon-link"></span></a>Random Patches and Random Subspaces</h3>

<p>Sampling both training instances and features is called the <strong>Random Patches</strong> method. Keeping all training instances (by setting bootstrap=False and max_samples=1.0) but sampling features (by setting bootstrap_features to True and/or max_features to a value smaller than 1.0) is called the <strong>Random Subspaces</strong> method. Sampling features results in even more predictor diversity, trading a bit more bias for a lower variance.</p>

<blockquote>
  <p>Instead of building a  BaggingClassifier  and passing it a  DecisionTreeClassifier , you can instead use the  RandomForestClassifier  class, which is more convenient and optimized for Decision Trees</p>
</blockquote>

<p>Forest of such extremely random trees is called an  <em>Extremely Randomized Trees</em>  ensemble  (<em>Extra-Trees</em>). This technique trades more bias for a lower variance. Much faster to train.</p>

<blockquote>
  <p>It is hard to tell in advance whether a  RandomForestClassifier  will perform better or worse than an  ExtraTreesClassifier . Generally, the only way to know is to try both and compare them using cross-validation (tuning the hyperparameters using grid search).</p>
</blockquote>

<p>Looking at how much the tree nodes that use that feature reduce impurity on average (across all trees in the forest)</p>

<h3 id="boosting">
<a class="anchor" href="#boosting" aria-hidden="true"><span class="octicon octicon-link"></span></a>Boosting</h3>

<p><strong>Boosting</strong>  (originally called  hypothesis boosting ) refers  to any Ensemble method that can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor</p>

<h3 id="adaboost">
<a class="anchor" href="#adaboost" aria-hidden="true"><span class="octicon octicon-link"></span></a>AdaBoost</h3>

<p>One  way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor underfitted. This results in new predictors focusing more and more on the hard cases. This is the technique used by  <strong>AdaBoost</strong> .</p>

<blockquote>
  <p>This sequential learning technique has some similarities with Gradient Descent, except that instead of tweaking a single predictor’s parameters to minimize a cost function, AdaBoost adds predictors to the ensemble, gradually making it better.</p>
</blockquote>

<p>There is one important drawback to this sequential learning technique: it cannot be parallelized (or only partially), since each predictor can only be trained after the previous predictor has been trained and evaluated. As a result, it <em>does not scale as well as bagging or pasting</em>.</p>

<h3 id="gradient-boosting">
<a class="anchor" href="#gradient-boosting" aria-hidden="true"><span class="octicon octicon-link"></span></a>Gradient Boosting</h3>

<p>Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor  to the  residual errors  made by the previous predictor.</p>

<p>The  learning_rate  hyperparameter scales the contribution of each tree. If you set it to a low value, such as  0.1 , you will need more trees in the ensemble to fit the training set, but the predictions will usually generalize better. This  is a regularization technique called  shrinkage .</p>

<p>If  subsample=0.25 , then each tree is trained on 25% of the training instances, selected randomly. As you can probably guess by now, this technique trades a higher bias for a lower variance. It also speeds up training considerably. This  is called  Stochastic Gradient Boosting .</p>

<h3 id="xgboost">
<a class="anchor" href="#xgboost" aria-hidden="true"><span class="octicon octicon-link"></span></a>XGBoost</h3>

<p>Extreme Gradient Boosting. This package was initially developed by Tianqi Chen as part of the Distributed (Deep) Machine Learning Community (DMLC), and it aims to be extremely fast, scalable, and portable. In fact, XGBoost is often an important component of the winning entries in ML competitions.</p>

<h3 id="stacking">
<a class="anchor" href="#stacking" aria-hidden="true"><span class="octicon octicon-link"></span></a>Stacking</h3>

<p>Stacking  (short for  stacked generalization ). 18  It is based on a simple idea: instead of using trivial functions (such as hard voting) to aggregate the predictions of all predictors in an ensemble, why don’t we train a model to perform this aggregation?</p>

<p>The final predictor  (called a  blender , or a  meta learner )</p>

<blockquote>
  <p>To train the blender, a common approach is to use a hold-out set</p>
</blockquote>

<p>It is actually possible to train several different blenders this way (e.g., one using Linear Regression, another using Random Forest Regression), to get a whole layer of blenders. The trick is to split the training set into three subsets: the first one is used to train the first layer, the second one is used to create the training set used to train the second layer (using predictions made by the predictors of the first layer), and the third one is used to create the training set to train the third layer (using predictions made by the predictors of the second layer). Once this is done, we can make a prediction for a new instance by going through each layer sequentially</p>

<h3 id="brew">
<a class="anchor" href="#brew" aria-hidden="true"><span class="octicon octicon-link"></span></a>Brew</h3>

<p>Scikit-Learn does not support stacking directly, but it is not too hard to roll out your own implementation (see the following exercises). Alternatively, you can use an open source implementation such as  brew .</p>

<h1 id="ch8-dimensionality-reduction">
<a class="anchor" href="#ch8-dimensionality-reduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>CH8. Dimensionality Reduction</h1>

<blockquote>
  <p>In some cases, reducing the dimensionality of the training data may filter out some noise and unnecessary details and thus result in higher performance, but in general it won’t; it will just speed up training</p>
</blockquote>

<p>It is also extremely useful for <strong>data visualization</strong></p>

<h3 id="the-curse-of-dimensionality">
<a class="anchor" href="#the-curse-of-dimensionality" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Curse of Dimensionality</h3>
<p>High-dimensional datasets are at risk of being very sparse: most training instances are likely to be far away from each other. This also means that a new instance will likely be far away from any training instance, making predictions much less reliable than in lower dimensions, since they will be based on much larger extrapolations -&gt; <strong>the more dimensions the training set has, the greater the risk of overfitting it</strong></p>

<blockquote>
  <p>The number of training instances required to reach a given density grows exponentially with the number of dimensions</p>
</blockquote>

<h3 id="projection">
<a class="anchor" href="#projection" aria-hidden="true"><span class="octicon octicon-link"></span></a>Projection</h3>
<p>Project every training instance perpendicularly onto this subspace</p>

<h3 id="manifold">
<a class="anchor" href="#manifold" aria-hidden="true"><span class="octicon octicon-link"></span></a>Manifold</h3>
<p>Many dimensionality reduction algorithms work by modeling the manifold on which the training instances lie; this is called Manifold Learning. It relies on the manifold assumption, also called the manifold hypothesis, which holds that most real-world high-dimensional datasets lie close to a much lower-dimensional manifold. This assumption is very often empirically observed</p>

<h3 id="pca">
<a class="anchor" href="#pca" aria-hidden="true"><span class="octicon octicon-link"></span></a>PCA</h3>
<p>Identifies the hyperplane that lies closest to the data, and then it projects the data onto it</p>

<h3 id="principal-components">
<a class="anchor" href="#principal-components" aria-hidden="true"><span class="octicon octicon-link"></span></a>Principal Components</h3>
<blockquote>
  <p>For each principal component, PCA finds a zero-centered unit vector pointing in the direction of the PC. Since two opposing unit vectors lie on the same axis, the direction of the unit vectors returned by PCA is not stable: if you perturb the training set slightly and run PCA again, the unit vectors may point in the opposite direction as the original vectors</p>
</blockquote>

<p>PCA assumes that the dataset is centered around the origin (Scikit-Learn take care of this)</p>

<h3 id="explained-variance-ratio">
<a class="anchor" href="#explained-variance-ratio" aria-hidden="true"><span class="octicon octicon-link"></span></a>Explained Variance Ratio</h3>
<p>Instead of arbitrarily choosing the number of dimensions to reduce down to, it is simpler to choose the number of dimensions that add up to a sufficiently large portion of the variance (e.g., 95%). Unless, of course, you are reducing dimensionality for data visualization—in that case you will want to reduce the dimensionality down to 2 or 3</p>

<p>It is possible to compress and decompress a dataset (with loss). The mean squared distance between the original data and the reconstructed data is called the <strong>reconstruction error</strong></p>

<h2 id="kernel-pca">
<a class="anchor" href="#kernel-pca" aria-hidden="true"><span class="octicon octicon-link"></span></a>Kernel PCA</h2>
<blockquote>
  <p><strong>Kernel trick</strong> -&gt; math technique that implicity maps instances into a very high-dimensional space (<em>feature space</em>). A linear decision boundary in the high-dimensional feature space corresponds to a complex nonlinear decision boundary in the original space</p>
</blockquote>

<h2 id="locally-linear-embedding-lle">
<a class="anchor" href="#locally-linear-embedding-lle" aria-hidden="true"><span class="octicon octicon-link"></span></a>Locally Linear Embedding (LLE)</h2>
<p>Powerful <em>nonlinear dimensionality reduction</em> (NLDR) technique. Does not rely on projections, like the previous algorithms do.</p>

<blockquote>
  <p>LLE works by first measuring how each training instance linearly relates to its closest neighbors, and then looking for a low-dimensional representation of the training set where these local relationships are best preserved</p>
</blockquote>

<p>Scale poorly to very large datasets</p>

<h2 id="other-dimensionality-reductions-techniques">
<a class="anchor" href="#other-dimensionality-reductions-techniques" aria-hidden="true"><span class="octicon octicon-link"></span></a>Other Dimensionality Reductions Techniques</h2>
<ul>
  <li>Random Projections: project the data to lower-dimensional space using a random linear projection</li>
  <li>Multidimensional Scaling (MDS): try to preserve the distances between the instances</li>
  <li>Isomap: creates a graph by connecting each instance to its nearest neighbors (try to preserve the geodesic distances between the instances)</li>
  <li>t-Distributed Stochastic Neighbor Embedding (t-SNE): try to keep similar instances close and dissimilar instances apart. Mostly used for visualization -&gt; clusters of instances in high-dimensional space</li>
  <li>Linear Discriminant Analysis (LDA): classification algorithm -&gt; learns the most discriminative axes between the classes -&gt; can be used to define a hyperplane to project the data</li>
</ul>

<h1 id="ch9-unsupervised-learning-techniques">
<a class="anchor" href="#ch9-unsupervised-learning-techniques" aria-hidden="true"><span class="octicon octicon-link"></span></a>CH9. Unsupervised Learning Techniques</h1>
<blockquote>
  <p>“If intelligence was a cake, unsupervised learning would be the cake, supervised learning would be the icing on the cake, and reinforcement learning would be the cherry on the cake” - Yann LeCun</p>
</blockquote>

<h3 id="clustering">
<a class="anchor" href="#clustering" aria-hidden="true"><span class="octicon octicon-link"></span></a>Clustering</h3>
<p>Identifying similar instances and assigning them to <em>clusters</em>, or groups of similar instances</p>

<ul>
  <li>
<strong>Customer Segmentation</strong>: i.e., <em>recommender systems</em> to suggest X that other users in the same cluster enjoyed</li>
  <li><strong>Data Analysis</strong></li>
  <li>
<strong>Dimensionality Reduction</strong>: Once a dataset has been clustered, it is usually possible to measure how well an instance fits into a cluster (<em>affinity</em>). Each instance’s feature vector x can then be replaced with the vector of its cluster affinities.</li>
  <li>
<strong>Anomaly Detection / Outlier Detection</strong>: Any instance that has a low affinity to all the clusters is likely to be an anomaly. Useful in detecting defects in manufacturing, or for <em>fraud detection</em>
</li>
  <li>
<strong>Semi-supervised Learning</strong>: If you only have a few labels, you could perform clustering and propagate the labels to all the instances in the same cluster. This technique can greatly increase the number of labels available for a subsequent supervised learning algorithm, and thus improve its performance</li>
  <li><strong>Search Engines</strong></li>
  <li>
<strong>Segment an image</strong>: By clustering pixels according to their color, then replacing each pixel’s color with the mean color of its cluster, it is possible to considerably reduce the number of different colors in the image -&gt; used in many object detection and tracking systems -&gt; makes it easier to detect the contour of each object</li>
</ul>

<p><strong>Algorithms</strong>:</p>
<ul>
  <li>Instances centered around a particular point -&gt; <em>centroid</em>
</li>
  <li>Continuous regions of densely packed instances</li>
  <li>Hierarchical, clusters of clusters</li>
</ul>

<h3 id="k-means">
<a class="anchor" href="#k-means" aria-hidden="true"><span class="octicon octicon-link"></span></a>K-Means</h3>
<p>Sometimes referred to as <em>LLoyd-Forgy</em></p>

<blockquote>
  <p><em>Voronoi tessellation/diagram/decomposition/partition</em>: is a partition of a plane into regions close to each of a given set of objects</p>
</blockquote>

<p><strong>Hard clustering</strong>: assigning each instance to a single cluster. <strong>Soft clustering</strong>: give each instance a score per cluster (can be the distance between the instance and the centroid, or the affinity score such as the Guassian Radial Basis Function)</p>

<ol>
  <li>Place the centroids randomly (pick <em>k</em> instances at random and use their locations as centroids)</li>
  <li>Label the instances</li>
  <li>Update the centroids</li>
  <li>Label the instances</li>
  <li>Update the centroids</li>
  <li>Repeat until the centroids stop moving</li>
</ol>

<blockquote>
  <p>The algorithm is guaranteed to converge in a finite a number of steps (usually quite small). K-Means is generally one of the fastest clustering algorithms</p>
</blockquote>

<h3 id="k-means-1">
<a class="anchor" href="#k-means-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>K-Means++</h3>
<p>Introduced a <strong>smarter initialization step</strong> that tends to select centroids that are distant from one another -&gt; makes the algorithm much less likely to converge to a suboptimal solution</p>

<h3 id="accelerated-k-means-and-mini-batch-k-means">
<a class="anchor" href="#accelerated-k-means-and-mini-batch-k-means" aria-hidden="true"><span class="octicon octicon-link"></span></a>Accelerated K-Means and mini-batch K-Means</h3>
<p>Accelerated -&gt; exploits the triangle inequality</p>

<p>Mini-batches -&gt; speeds up the algorithm by a factor of 3 or 4 -&gt; makes it possible to cluster huge datasets that do not fit in memory (<em>MiniBatchKMeans</em> in Scikit-Learn)</p>

<blockquote>
  <p>If the dataset does not fit in memory, the simplest option is to use the <em>memmap</em> class. Alternatively, you can pass one mini-batch at a time to the <em>partial_fit()</em> method, but this will require much more work, since you will need to perform multiple initializations and select the best one yourself.</p>
</blockquote>

<h3 id="finding-the-optimal-number-of-clusters">
<a class="anchor" href="#finding-the-optimal-number-of-clusters" aria-hidden="true"><span class="octicon octicon-link"></span></a>Finding the optimal number of clusters</h3>
<p>Plotting the inertia as a function of the number of clusters k, the curve often contains an inflexion point called the <strong>“elbow”</strong></p>

<p>A more precise approach (but also more computationally expensive) is to use the silhouette score, which is the mean <strong>silhouette coefficient</strong> over all the instances. An instance’s silhouette coefficient is equal to (b – a) / max(a, b), where a is the mean distance to the other instances in the same cluster (i.e., the mean intra-cluster distance) and b is the mean nearest-cluster distance (i.e., the mean distance to the instances of the next closest cluster, defined as the one that minimizes b, excluding the instance’s own cluster). The silhouette coefficient can vary between –1 and +1. A coefficient close to +1 means that the instance is well inside its own cluster and far from other clusters, while a coefficient close to 0 means that it is close to a cluster boundary, and finally a coefficient close to –1 means that the instance may have been assigned to the wrong cluster.</p>

<blockquote>
  <p><strong>Silhouette diagram</strong>: more informative visualization -&gt; plot every instance’s silhouette coefficient, sorted by the cluster they are assigned to and by the value of the coefficient</p>
</blockquote>

<h3 id="limits-of-k-means">
<a class="anchor" href="#limits-of-k-means" aria-hidden="true"><span class="octicon octicon-link"></span></a>Limits of K-Means</h3>
<ul>
  <li>Necessary to run several times to avoid suboptimal solutions</li>
  <li>You need to specify the number of clusters</li>
  <li>Does not behave well when the clusters have varying sizes, different densities or nonspherical shapes</li>
</ul>

<blockquote>
  <p>It is important to scale the input features before you run K-Means, or the clusters may be very stretched and K-Means will perform poorly. Scaling the features does not guarantee that all the clusters will be nice and spherical, but it generally improves things</p>
</blockquote>

<blockquote>
  <p><strong>NOTE</strong>: remember to check the book again, there are some useful practical examples on clustering for preprocessing, semi-supervised learning (<em>label propagation</em>)</p>
</blockquote>

<h4 id="active-learning-uncertainty-sampling">
<a class="anchor" href="#active-learning-uncertainty-sampling" aria-hidden="true"><span class="octicon octicon-link"></span></a>Active learning (<em>uncertainty sampling</em>)</h4>
<ol>
  <li>The model is trained on the labeled instances gathered so far, and this model is used to make predictions on all the unlabeled instances.</li>
  <li>The instances for which the model is most uncertain (i.e., when its estimated probability is lowest) are given to the expert to be labeled.</li>
  <li>You iterate this process until the performance improvement stops being worth the labeling effort.</li>
</ol>

<h3 id="dbscan">
<a class="anchor" href="#dbscan" aria-hidden="true"><span class="octicon octicon-link"></span></a>DBSCAN</h3>

<p>Defines clusters as continuous regions of high density. Works well if all the clusters are dense enough and if they are well separated by low-density regions</p>

<p>It is robust to outliers, and it has just two hyperparameters (<em>eps</em> and <em>min_samples</em>)</p>

<h3 id="other-clustering-algorithms">
<a class="anchor" href="#other-clustering-algorithms" aria-hidden="true"><span class="octicon octicon-link"></span></a>Other Clustering Algorithms</h3>
<ul>
  <li><strong>Agglomerative clustering</strong></li>
  <li>
<strong>BIRCH</strong>: Balanced Iterative Reducing and Clustering using Hierarchies -&gt; designed specifically for very large datasets</li>
  <li>
<strong>Mean-Shift</strong>: computational complexity is O(m^2), not suited for large datasets</li>
  <li>
<strong>Affinity propagation</strong>: same problem as mean-shift</li>
  <li>
<strong>Spectral clustering</strong>: does not scale very well to large numbers of instances and it does not behave well when the clusters have very different sizes</li>
</ul>

<h3 id="gaussian-mixtures">
<a class="anchor" href="#gaussian-mixtures" aria-hidden="true"><span class="octicon octicon-link"></span></a>Gaussian Mixtures</h3>
<p>A <em>Gaussian mixture model</em> (GMM) is a probabilistic model that assumes that the instances were generated from a mixture of several Gaussian distributions whose parameters are unknown.</p>

<p>GMM is a <em>generative model</em> -&gt; you can sample new instances from it</p>

<h3 id="anomaly-detection-using-gaussian-mixtures">
<a class="anchor" href="#anomaly-detection-using-gaussian-mixtures" aria-hidden="true"><span class="octicon octicon-link"></span></a>Anomaly Detection using Gaussian Mixtures</h3>
<p>Using a GMM for anomaly detection is quite simple: any instance located in a low-density region can be considered an anomaly. You must define what density threshold you want to use.</p>

<p>A closely related task is <em>novelty detection</em>: it differs from anomaly detection in that the algorithm is assumed to be trained on a “clean” dataset, uncontaminated by outliers, whereas anomaly detection oes not make this assumption. Outlier detection is often used to clean up a dataset.</p>

<h3 id="selecting-the-number-of-clusters">
<a class="anchor" href="#selecting-the-number-of-clusters" aria-hidden="true"><span class="octicon octicon-link"></span></a>Selecting the Number of Clusters</h3>
<p>Find the model that minimizes a theoretical information criterion -&gt; <em>Bayesian information criterion</em> (BIC) or the <em>Akaike information criterion</em> (AIC)</p>

<h3 id="bayesian-gaussian-mixture-models">
<a class="anchor" href="#bayesian-gaussian-mixture-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bayesian Gaussian Mixture Models</h3>
<p>Rather than manually searching for the optimal number of clusters, you can use the BayesianGaussianMixture class, which is capable of giving weights equal (or close) to zero to unnecessary clusters. Set the number of clusters n_components to a value that you have good reason to believe is greater than the optimal number of clusters (this assumes some minimal knowledge about the problem at hand), and the algorithm will eliminate the unnecessary clusters automatically.</p>

<blockquote>
  <p>GMM work great on clusters with ellipsoidal shapes, but if you try to fit a dataset with different shapes, you may have bad surprises</p>
</blockquote>

<h3 id="other-algorithms-for-anomaly-and-novelty-detection">
<a class="anchor" href="#other-algorithms-for-anomaly-and-novelty-detection" aria-hidden="true"><span class="octicon octicon-link"></span></a>Other Algorithms for Anomaly and Novelty Detection</h3>
<ul>
  <li>
<strong>PCA</strong> (<em>inverse_transform()</em> method): If you compare the reconstruction error of a normal instance with the reconstruction error of an anomaly, the latter will usually be much larger. This is a simple and often quite efficient anomaly detection approach</li>
  <li>
<strong>Fast-MCD</strong> (minimum covariance determinant): Implemented by the EllipticEnvelope class, this algorithm is useful for outlier detection, in particular to clean up a dataset</li>
  <li>
<strong>Isolation Forest</strong>: efficient in high-dimensional datasets. Anomalies are usually far from other instances, so on average they tend to get isolated in fewer steps than normal instances</li>
  <li>
<strong>Local Outlier Factor (LOF)</strong>: compares the density of instances around a given instance to the density around its neighbors</li>
  <li>
<strong>One-class SVM</strong>: better suited for novelty detection. Works great, especially with high-dimensional datasets, but like all SVMs it does not scale to large datasets</li>
</ul>

<h1 id="part-ii-neural-networks-and-deep-learning">
<a class="anchor" href="#part-ii-neural-networks-and-deep-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Part II, Neural Networks and Deep Learning</h1>

<h1 id="ch10-introduction-to-artificial-neural-networks-with-keras">
<a class="anchor" href="#ch10-introduction-to-artificial-neural-networks-with-keras" aria-hidden="true"><span class="octicon octicon-link"></span></a>CH10. Introduction to Artificial Neural Networks with Keras</h1>

<p>ANNs are the very core of Deep Learning -&gt; versatile, powerful and scalable</p>

<p>Renewed interest in ANNs:</p>
<ul>
  <li>Huge quantity of data -&gt; ANNs frequently outperform other ML techniques (large and complex problems)</li>
  <li>Increase in computing power -&gt; GPU cards and cloud computing</li>
  <li>Tweaks to the training algorithms</li>
  <li>Reach fairly close to global optimum</li>
  <li>Virtuous circle of funding and progress</li>
</ul>

<h3 id="perceptron">
<a class="anchor" href="#perceptron" aria-hidden="true"><span class="octicon octicon-link"></span></a>Perceptron</h3>
<p>Simple ANN architecture. Based on <em>threshold logic unit</em> (TLU), or <em>linear threshold unit</em> (LTU).</p>

<p>A Perceptron is simply composed of a single layer of TLUs, with each TLU connected to all the inputs. <em>Fully conected layer / dense layer</em>: when all the neurons in a layer are connected to every neuron in the previous layer.</p>

<blockquote>
  <p><strong>Hebb’s rule / Hebbian learning</strong>: “Cells that fire together, wire together”, the connection weight between two neurons tends to increase when they fire simultaneously</p>
</blockquote>

<p>The Perceptron learning algorithm strongly resembles Stochastic Gradient Descent. Contrary to Logistic Regression classifiers, Perceptrons do not output a class probability; rather, they make predictions based on a hard threshold</p>

<h3 id="the-multilayer-perceptron-and-backpropagation">
<a class="anchor" href="#the-multilayer-perceptron-and-backpropagation" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Multilayer Perceptron and Backpropagation</h3>

<p>Perceptrons are incapable of solving some trivial problems (e.g., XOR) -&gt; true of any linear classification model. This can be solved by stacking multiple Perceptrons -&gt; <em>Multilayer Perceptron</em> (MLP)</p>

<p>An MLP is composed of one (passthrough) <em>input layer</em>, one or more layers of TLUs, called <em>hidden layers</em>, and one final layer of TLUs called the <em>output layer</em>. The layers close to the input layer are usually called the <em>lower layers</em>, and the ones close to the outputs are usually called the <em>upper layers</em>. Every layer except the output layer includes a bias neuron and is fully connected to the next layer</p>

<blockquote>
  <p><em>Feedforward neural network</em> (FNN): signal flows only in one direction (from the inputs to the outputs)</p>
</blockquote>

<p>When an ANN contains a deep stack of hidden layers -&gt; <em>Deep neural network</em> (DNN)</p>

<h3 id="backpropagation">
<a class="anchor" href="#backpropagation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Backpropagation</h3>

<p>Training algorithm. It is Gradient Descent using and efficient technique for computing the gradients automatically: in just two passes through the network (one forward, one backward), the backpropagation algorithm is able to compute the gradient of the network’s error with regard to every single model parameter. It can find out how each connection weight and each bias term should be tweaked in order to reduce the error. Once it has these gradients, it just performs a regular Gradient Descent step, and the whole process is repeated until the network converges to the solution.</p>

<blockquote>
  <p>Automatically computing gradients is called automatic differentiation, or <em>autodiff</em>. There are various autodiff techniques, with different pros and cons. The one used by backpropagation is called <em>reverse-mode autodiff</em>. It is fast and precise, and is well suited when the function to differentiate has many variables (e.g., connection weights) and few outputs (e.g., one loss).</p>
</blockquote>

<p>For each training instance, the backpropagation algorithm first makes a prediction (forward pass) and measures the error, then goes through each layer in reverse to measure the error contribution from each connection (reverse pass), and finally tweaks the connection weights to reduce the error (Gradient Descent step)</p>

<blockquote>
  <p>It is important to initialize all the hidden layers’ connection weights randomly, or else training will fail. For example, if you initialize all weights and biases to zero, then all neurons in a given layer will be perfectly identical, and thus backpropagation will affect them in exactly the same way, so they will remain identical</p>
</blockquote>

<h3 id="activation-functions">
<a class="anchor" href="#activation-functions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Activation functions</h3>
<p>You need to have some nonlinearity between layers to solve very complex problems</p>

<p>Examples:</p>

<ul>
  <li>Logistic (sigmoid) function</li>
  <li>Hyperbolic tangent (tanh)</li>
  <li>Rectified Linear Unit (ReLU) -&gt; fast to compute, has become the default</li>
</ul>

<h3 id="regression-mlp-architecture">
<a class="anchor" href="#regression-mlp-architecture" aria-hidden="true"><span class="octicon octicon-link"></span></a>Regression MLP architecture</h3>
<p><strong>Hyperparameter - Typical value</strong></p>

<p>input neurons - One per input feature (e.g., 28 x 28 = 784 for MNIST)</p>

<p>hidden layers - Depends on the problem, but typically 1 to 5</p>

<p>neurons per hidden layer - Depends on the problem, but typically 10 to 100</p>

<p>output neurons - 1 per prediction dimension</p>

<p>Hidden activation - ReLU (or SELU, see Chapter 11)</p>

<p>Output activation - None, or ReLU/softplus (if positive outputs) or logistic/tanh (if bounded outputs)</p>

<p>Loss function - MSE or MAE/Huber (if outliers)</p>

<h3 id="classification-mlp-architecture">
<a class="anchor" href="#classification-mlp-architecture" aria-hidden="true"><span class="octicon octicon-link"></span></a>Classification MLP architecture</h3>

<p><strong>Hyperparameter - Binary classification - Multilabel binary classification - Multiclass classification</strong></p>

<p>Input and hidden layers - Same as regression - Same as regression - Same as regression</p>

<p>output neurons - 1 - 1 per label - 1 per class</p>

<p>Output layer activation - Logistic - Logistic - Softmax</p>

<p>Loss function - Cross entropy - Cross entropy - Cross entropy</p>

<h3 id="implementing-mlps-with-keras">
<a class="anchor" href="#implementing-mlps-with-keras" aria-hidden="true"><span class="octicon octicon-link"></span></a>Implementing MLPs with Keras</h3>

<p>Tensorflow 2 is arguably just as simple as PyTorch, as it has adopted Keras as its official high-level API and its developers have greatly simplified and cleaned up the rest of the API</p>

<blockquote>
  <p>Since we are going to train the neural network using Gradient Descent, we must scale the input features</p>
</blockquote>

<h3 id="creating-a-sequential-model">
<a class="anchor" href="#creating-a-sequential-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Creating a Sequential model</h3>

<p>You can pass a list of layers when creating the Sequential model:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">]),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"softmax"</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div>

<p>The model’s summary() method displays all the model’s layers, including each layer’s name (which is automatically generated unless you set it when creating the layer), its output shape (None means the batch size can be anything), and its number of parameters. The summary ends with the total number of parameters, including trainable and non-trainable parameters</p>

<blockquote>
  <p>Dense layers often have a lot of parameters. This gives the model quite a lot of flexibility to fit the training data, but it also means that the model runs the risk of overfitting, especially when you do not have a lot of training data</p>
</blockquote>

<h3 id="compiling-the-model">
<a class="anchor" href="#compiling-the-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Compiling the model</h3>

<p>After a model is created, you must call its compile() method to specify the loss function and the optimizer to use. Optionally, you can specify a list of extra metrics to compute during training and evaluation:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">"sparse_categorical_crossentropy"</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="s">"sgd"</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">"accuracy"</span><span class="p">])</span>
</code></pre></div></div>

<h3 id="training-and-evaluating-the-model">
<a class="anchor" href="#training-and-evaluating-the-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training and evaluating the model</h3>

<p>Now the model is ready to be trained. For this we simply need to call its fit() method:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
<span class="o">...</span>                     <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">))</span>
</code></pre></div></div>

<p>If the training set was very skewed, with some classes being overrepresented and others underrepresented, it would be useful to set the <strong>class_weight</strong> argument when calling the fit() method, which would give a larger weight to underrepresented classes and a lower weight to overrepresented classes. These weights would be used by Keras when computing the loss</p>

<blockquote>
  <p>The fit() method returns a History object containing the training parameters (history.params), the list of epochs it went through (history.epoch), and most importantly a dictionary (history.history) containing the loss and extra metrics it measured at the end of each epoch on the training set and on the validation set (if any) -&gt; use this dictionary to create a pandas DataFrame and call its plot() method to get the learning curves</p>
</blockquote>

<blockquote>
  <p>When plotting the training curve, it should be shifted by half an epoch to the left</p>
</blockquote>

<p>If you are not satisfied with the performance of your model, you should go back and <strong>tune the hyperparameters</strong>. The first one to check is the learning rate. If that doesn’t help, try another optimizer (and always retune the learning rate after changing any hyperparameter). If the performance is still not great, then try tuning model hyperparameters such as the number of layers, the number of neurons per layer, and the types of activation functions to use for each hidden layer. You can also try tuning other hyperparameters, such as the batch size (it can be set in the fit() method using the batch_size argument, which defaults to 32).</p>

<h3 id="building-complex-models-using-the-functional-api">
<a class="anchor" href="#building-complex-models-using-the-functional-api" aria-hidden="true"><span class="octicon octicon-link"></span></a>Building Complex Models Using the Functional API</h3>
<p><em>Wide &amp; Deep</em> neural network: nonsequential, connects all or part of the inputs directly to the output layer. Makes it possible for the NN to learn both deep patterns (using the deep path) and simple rules (through the short path). Regular MLP forces all the data to flow through the full stack of layers -&gt; simple patterns may end up being distorted by the sequence of transformations</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">input_</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
<span class="n">hidden1</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">)(</span><span class="n">input_</span><span class="p">)</span>
<span class="n">hidden2</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">)(</span><span class="n">hidden1</span><span class="p">)</span>
<span class="n">concat</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Concatenate</span><span class="p">()([</span><span class="n">input_</span><span class="p">,</span> <span class="n">hidden2</span><span class="p">])</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">concat</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">input_</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">output</span><span class="p">])</span>
</code></pre></div></div>

<h3 id="using-the-subclassing-api-to-build-dynamic-models">
<a class="anchor" href="#using-the-subclassing-api-to-build-dynamic-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>Using the Subclassing API to Build Dynamic Models</h3>
<p>Sequential and Functional API are declarative:</p>
<ul>
  <li>model can easily be saved, clone, shared</li>
  <li>structure can be displayed and analyzed</li>
  <li>framework can infer shapes and check types (caught errors early)</li>
  <li>easy to debug -&gt; static graph of layers</li>
  <li>
<strong>problem</strong>: it’s static, so dynamic behaviors like loops, varying shapes and conditional branching are not easy</li>
</ul>

<p>Subclassing API -&gt; imperative programming style</p>

<p>Simply subclass the Model class, crete the layers you need in the constructor, and use them to perform the computations you want in the <em>call()</em> method.</p>

<blockquote>
  <p>Great API for researches experimenting with new ideas</p>
</blockquote>

<p>Cons:</p>
<ul>
  <li>model’s architecture is hidden within the <em>call()</em> method -&gt; cannot inspect, save, clone</li>
  <li>Keras cannot check types and shapes ahead of time</li>
</ul>

<blockquote>
  <p>Unles you really need that extra flexibility, you should probably stick to the Sequential/Functional API</p>
</blockquote>

<h3 id="saving-and-restoring-a-model">
<a class="anchor" href="#saving-and-restoring-a-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Saving and Restoring a Model</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span><span class="o">...</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">([</span><span class="o">...</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="o">...</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s">"my_keras_model.h5"</span><span class="p">)</span>
<span class="c1"># loading the model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s">"my_keras_model.h5"</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="using-callbacks">
<a class="anchor" href="#using-callbacks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Using Callbacks</h3>
<p>The fit() method accepts a callbacks argument that lets you specify a list of objects that Keras will call at the start and end of training, at the start and end of each epoch, and even before and after processing each batch. For example, the ModelCheckpoint callback saves checkpoints of your model at regular intervals during training, by default at the end of each epoch:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="c1"># build and compile the model
</span><span class="n">checkpoint_cb</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">ModelCheckpoint</span><span class="p">(</span><span class="s">"my_keras_model.h5"</span><span class="p">)</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">checkpoint_cb</span><span class="p">])</span>
</code></pre></div></div>

<p>Moreover, if you use a validation set during training, you can set save_best_only=True when creating the ModelCheckpoint. In this case, it will only save your model when its performance on the validation set is the best so far.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">early_stopping_cb</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                                                  <span class="n">restore_best_weights</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">),</span>
                    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">checkpoint_cb</span><span class="p">,</span> <span class="n">early_stopping_cb</span><span class="p">])</span>
</code></pre></div></div>

<h3 id="using-tensorboard-for-visualization">
<a class="anchor" href="#using-tensorboard-for-visualization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Using TensorBoard for Visualization</h3>
<p>Interactive visualization tool:</p>
<ul>
  <li>view learning curves during training</li>
  <li>compare learning curves between multiple runs</li>
  <li>visualize the computation graph</li>
  <li>analyze training statistics</li>
  <li>view images generated by your model</li>
  <li>visualize complex multidimensional data projected down to 3D and automatically clustered</li>
</ul>

<p>To use it, modify your program so that it outputs the data you want to visualize to special binary log files called <em>event files</em>. Each binary data record is called a <em>summary</em></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="n">root_logdir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">curdir</span><span class="p">,</span> <span class="s">"my_logs"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_run_logdir</span><span class="p">():</span>
    <span class="kn">import</span> <span class="nn">time</span>
    <span class="n">run_id</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s">"run_</span><span class="si">%</span><span class="s">Y_</span><span class="si">%</span><span class="s">m_</span><span class="si">%</span><span class="s">d-</span><span class="si">%</span><span class="s">H_</span><span class="si">%</span><span class="s">M_</span><span class="si">%</span><span class="s">S"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">root_logdir</span><span class="p">,</span> <span class="n">run_id</span><span class="p">)</span>

<span class="n">run_logdir</span> <span class="o">=</span> <span class="n">get_run_logdir</span><span class="p">()</span> <span class="c1"># e.g., './my_logs/run_2019_06_07-15_15_22'
</span></code></pre></div></div>
<p>The good news is that Keras provides a nice TensorBoard() callback:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="c1"># Build and compile your model
</span><span class="n">tensorboard_cb</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">TensorBoard</span><span class="p">(</span><span class="n">run_logdir</span><span class="p">)</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">),</span>
                    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">tensorboard_cb</span><span class="p">])</span>
</code></pre></div></div>

<p>You need to start the TensorBoard server:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>tensorboard <span class="nt">--logdir</span><span class="o">=</span>./my_logs <span class="nt">--port</span><span class="o">=</span>6006
TensorBoard 2.0.0 at http://mycomputer.local:6006/ <span class="o">(</span>Press CTRL+C to quit<span class="o">)</span>
</code></pre></div></div>

<p>To use directly withing Jupyter</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">tensorboard</span>
<span class="o">%</span><span class="n">tensorboard</span> <span class="o">--</span><span class="n">logdir</span><span class="o">=./</span><span class="n">my_logs</span> <span class="o">--</span><span class="n">port</span><span class="o">=</span><span class="mi">6006</span>
</code></pre></div></div>

<h3 id="fine-tuning-neural-network-hyperparameters">
<a class="anchor" href="#fine-tuning-neural-network-hyperparameters" aria-hidden="true"><span class="octicon octicon-link"></span></a>Fine-Tuning Neural Network Hyperparameters</h3>
<p>NN are flexible. Drawback: many hyperparameters to tweak</p>

<p>You can wrap the Keras models in objects that mimic regular Scikit-Learn algorithms -&gt; then use GridSearchCV or RandomizedSearchCV</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">reciprocal</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RandomizedSearchCV</span>

<span class="n">param_distribs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"n_hidden"</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
    <span class="s">"n_neurons"</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
    <span class="s">"learning_rate"</span><span class="p">:</span> <span class="n">reciprocal</span><span class="p">(</span><span class="mf">3e-4</span><span class="p">,</span> <span class="mf">3e-2</span><span class="p">),</span>
<span class="p">}</span>

<span class="n">rnd_search_cv</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">keras_reg</span><span class="p">,</span> <span class="n">param_distribs</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">rnd_search_cv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                  <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">),</span>
                  <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">)])</span>
</code></pre></div></div>

<p>There are many techniques to explore a search space much more efficiently than randomly -&gt; when a region of the space turns out to be good, it should be explored more:</p>
<ul>
  <li>
<strong>Hyperopt</strong>: popular lib for optimizing over all sorts of complex search spaces</li>
  <li>
<strong>Hyperas, kopt, or Talos</strong>: libs for optimizing hyperparameters for Keras models</li>
  <li>
<strong>Keras Tuner</strong>: lib by Google for Keras models</li>
  <li><strong>Skicit-Optimize (skopt)</strong></li>
  <li>
<strong>Spearmint</strong>: bayesian</li>
  <li><strong>Hyperband</strong></li>
  <li>
<strong>Sklearn-Deep</strong>: evolutionary algorithms</li>
</ul>

<blockquote>
  <p>Hyperparameter tuning is still an active area of research, and evolutionary algorithms are making a comeback</p>
</blockquote>

<h3 id="number-of-hidden-layers">
<a class="anchor" href="#number-of-hidden-layers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Number of Hidden Layers</h3>
<p>For complex problems, deep networks have a much higher <em>parameter efficiency</em> than shallow ones -&gt; they can model complex functions using exponentially fewer neurons than shallow nets, allowing them to reach much better performance with the same amount of training data. Ramp up the number of hidden layers until you start overfitting the training set</p>

<blockquote>
  <p><strong>Transfer Learning</strong>: Instead of randomly initializing the weights and biases of the first few layers of the new neural network, you can initialize them to the values of the weights and biases of the lower layers of the first network. This way the network will not have to learn from scratch all the low-level structures that occur in most pictures; it will only have to learn the higher-level structures</p>
</blockquote>

<h3 id="number-of-neurons-per-hidden-layer">
<a class="anchor" href="#number-of-neurons-per-hidden-layer" aria-hidden="true"><span class="octicon octicon-link"></span></a>Number of Neurons per Hidden Layer</h3>
<p>The number of neurons in the input and output layers is determined by the type of input and output your task requires</p>

<p>Hidden layers: it used to be common to size them to form a pyramid, with fewer neurons at each layer -&gt; many low-level features can coalesce into far fewer high-level features -&gt; this practice has been largely abandoned -&gt; Using the same number of neurons in all hidden layers performs just as well in most cases or even better; plus, there is only one hyperparameter to tune, insted of one per layer</p>

<p>Depending on the dataset, first hidden layer bigger can be good</p>

<blockquote>
  <p>Pick a model with more layers and neurons than you actually need, then use early stopping or other regularization techniques to prevent overfitting -&gt; <strong>“Stretch pants”</strong></p>
</blockquote>

<p>Increasing the number of layers » increase the number of neurons per layer</p>

<h3 id="learning-rate-batch-size-and-other-hyperparameters">
<a class="anchor" href="#learning-rate-batch-size-and-other-hyperparameters" aria-hidden="true"><span class="octicon octicon-link"></span></a>Learning Rate, Batch Size, and Other Hyperparameters</h3>
<p>Plot the loss as a function of the learning rate (using a log scale for the learning rate), you should see it dropping at first. But after a while, the learning rate will be too large, so the loss will shoot back up: the optimal learning rate will be a bit lower than the point at which the loss starts to climb (typically about 10 times lower than the turning point)</p>

<p>Benefit of using large batch sizes -&gt; GPUs can process them efficiently -&gt; use the largest batch size that can fit in GPU RAM</p>

<blockquote>
  <p>Try to use a large batch size, using learning rate warmup. If training is unstable or bad performance -&gt; try using a small batch size instead</p>
</blockquote>

<p>ReLU activation function is a good default</p>

<blockquote>
  <p>The optimal learning rate depends on the other hyperparameters—especially the batch size—so if you modify any hyperparameter, make sure to update the learning rate as well.</p>
</blockquote>

<h1 id="ch11-training-deep-neural-networks">
<a class="anchor" href="#ch11-training-deep-neural-networks" aria-hidden="true"><span class="octicon octicon-link"></span></a>CH11. Training Deep Neural Networks</h1>

<h3 id="the-vanishingexploding-gradients-problems">
<a class="anchor" href="#the-vanishingexploding-gradients-problems" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Vanishing/Exploding Gradients Problems</h3>

<p><strong>Vanishing gradients problem</strong>: gradients often geet smaller as the algorithm progresses down to the lower layers -&gt; Gradient Descent update leaves the lower layers’ connection weights virtually unchanged, and training never converges to a good solution</p>

<p><strong>Exploding gradients problem</strong>: gradients can grow bigger until layers get insanely large weight updates and the algorithm diverges</p>

<p>More generally, DNNs suffer from unstable gradients, different layers may learn at widely different speeds</p>

<h3 id="glorot-and-he-initialization">
<a class="anchor" href="#glorot-and-he-initialization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Glorot and He Initialization</h3>
<p>Using Glorot initialization can speed up training considerably</p>

<p>ReLU actv fn and its variants, sometimes called <em>He initialization</em></p>

<p>SELU actv fn should be used with LeCun initialization (with normal distribution)</p>

<h3 id="nonsaturating-activation-functions">
<a class="anchor" href="#nonsaturating-activation-functions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Nonsaturating Activation Functions</h3>
<ul>
  <li>
<strong>Dying ReLUs</strong>: during training some neurons stops outputting anything other than 0.</li>
  <li>
<strong>leaky ReLU</strong>: small slope ensures that leaky ReLUs never die -&gt; always outperformed the strict ReLU actv fn</li>
  <li>
<strong>randomized leaky ReLU (RReLU)</strong>: perform well, seemed to act as regularizer</li>
  <li>
<strong>parametric leaky ReLU (PReLU)</strong>: strongly outperformed ReLU on large image datasets, overfit on smaller datasets</li>
  <li>
<strong>exponential linear unit (ELU)</strong>: outperformed ReLU, faster convergence rate, despite being slower to compute</li>
  <li>
<strong>Scaled ELU (SELU)</strong>: self-normalize the network (mean=0, std=1) -&gt; solves vanishing/exploding gradients. Significantly outperforms other actv fn, but need to be configured correcly (some assumptions for self-normalization).</li>
</ul>

<blockquote>
  <p><strong>In general</strong>: SELU &gt; ELU &gt; leaky ReLU &gt; ReLU &gt; tanh &gt; logistic</p>
</blockquote>

<h3 id="batch-normalization">
<a class="anchor" href="#batch-normalization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Batch Normalization</h3>
<p>Also help solve vanishing/exploding gradients problems.</p>

<p>Add an operation just before or after the actv fn of each hidden layer: zero-centers and normalizes each input, then scales and shifts the result using two new parameter vectors per layer: one for scaling, other for shifting -&gt; many cases if the BN layer as the very first of the NN, you do not need to standardize your training set</p>

<p>BN also acts like a regularizer, reducing the need for other regularization techniques</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">]),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"elu"</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">"he_normal"</span><span class="p">),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"elu"</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">"he_normal"</span><span class="p">),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"softmax"</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div>

<blockquote>
  <p>BatchNormalization -&gt; one of the most-used layers in DNNs, often omitted in the diagrams, it is assumed BN is added after every layer</p>
</blockquote>

<h3 id="gradient-clipping">
<a class="anchor" href="#gradient-clipping" aria-hidden="true"><span class="octicon octicon-link"></span></a>Gradient Clipping</h3>
<p>Mitigate the exploding gradients problem. Clip the gradients during backpropagation, never exceed some threshold.</p>

<p>Most often used in RNNs (BN is trickier to use here)</p>

<h3 id="reusing-pretrained-layers">
<a class="anchor" href="#reusing-pretrained-layers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Reusing Pretrained Layers</h3>
<p><strong>Transfer Learning</strong>: Not a good idea to train a very large DNN from scratch: find an existing NN that accomplishes a similar task. Speed up training, require significantly less training data</p>

<blockquote>
  <p>Transfer learning will work best when the inputs have similar low-level features (resize inputs to the size expected by the original model). The output layer should be replaced according to the new task</p>
</blockquote>

<p>More similar tasks = more layers you want to reuse (starting with the lower layers)</p>

<h3 id="transfer-learning-with-keras">
<a class="anchor" href="#transfer-learning-with-keras" aria-hidden="true"><span class="octicon octicon-link"></span></a>Transfer Learning with Keras</h3>

<p>Cloning a model with their weights</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_A_clone</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">clone_model</span><span class="p">(</span><span class="n">model_A</span><span class="p">)</span>
<span class="n">model_A_clone</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">model_A</span><span class="o">.</span><span class="n">get_weights</span><span class="p">())</span>
</code></pre></div></div>

<p>Freezing layers up until the last</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model_B_on_A</span><span class="o">.</span><span class="n">layers</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
    <span class="n">layer</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="bp">False</span>
<span class="c1"># You must always compile the model after freezing/unfreezing layers
</span><span class="n">model_B_on_A</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">"binary_crossentropy"</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s">"sgd"</span><span class="p">,</span>
                     <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">"accuracy"</span><span class="p">])</span>
</code></pre></div></div>

<blockquote>
  <p>Transfer Learning does not work very well with small dense networks. Works best with deep CNN -&gt; tend to learn feature detectors that are much more general</p>
</blockquote>

<h3 id="unsupervised-pretraining">
<a class="anchor" href="#unsupervised-pretraining" aria-hidden="true"><span class="octicon octicon-link"></span></a>Unsupervised Pretraining</h3>
<p>Often cheap to gather unlabeled training examples, but expensive to label them. You can try to use the unlabeled data to train an unsupervised model, such as an autoencoder or a generative adversarial network (GAN). Then reuse the lower layers of the autoencoder/GAN’s discriminator, add the output layer for your task and fine-tune the final network using supervised learning</p>

<p>Before -&gt; restricted Boltzmann machines (RBMs) for unsupervised learning</p>

<blockquote>
  <p>Self-supervised learning is when you automatically generate the labels from the data itself, then you train a model on the resulting “labeled” dataset using supervised learning techniques. Since this approach requires no human labeling whatsoever, it is best classified as a form of unsupervised learning</p>
</blockquote>

<h3 id="faster-optimizers">
<a class="anchor" href="#faster-optimizers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Faster Optimizers</h3>
<p>Ways to speed up training:</p>
<ul>
  <li>initialization strategy for the weights</li>
  <li>activation function</li>
  <li>batch normalization</li>
  <li>reusing parts of a pretrained network</li>
  <li>faster optimizers than regular gradient descent</li>
</ul>

<h3 id="momentum-optimization">
<a class="anchor" href="#momentum-optimization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Momentum Optimization</h3>
<ul>
  <li>with momentum the system may oscillate before stabilizing -&gt; it’s good to have a bit of friction in the system</li>
  <li>momentum value = 0.9 -&gt; usually works well in practice</li>
</ul>

<h3 id="nesterov-accelerated-gradient-nag">
<a class="anchor" href="#nesterov-accelerated-gradient-nag" aria-hidden="true"><span class="octicon octicon-link"></span></a>Nesterov Accelerated Gradient (NAG)</h3>
<ul>
  <li>NAG ends up being significantly faster than regular momentum optimization</li>
  <li>less oscillations and converges faster</li>
  <li>nesterov=True</li>
</ul>

<h3 id="adagrad">
<a class="anchor" href="#adagrad" aria-hidden="true"><span class="octicon octicon-link"></span></a>AdaGrad</h3>
<ul>
  <li><em>adaptive learning rate</em></li>
  <li>efficient for simpler tasks such as Linear Regression</li>
  <li>should NOT be used to train DNNs</li>
</ul>

<h3 id="rmsprop">
<a class="anchor" href="#rmsprop" aria-hidden="true"><span class="octicon octicon-link"></span></a>RMSProp</h3>
<ul>
  <li>better than AdaGrad on more complex problems</li>
</ul>

<h3 id="adam-and-nadam-optimization">
<a class="anchor" href="#adam-and-nadam-optimization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Adam and Nadam Optimization</h3>
<ul>
  <li>Adam: <em>adaptive moment estimation</em>
</li>
  <li>requires less tuning of the learning rate</li>
</ul>

<p>Variations:</p>

<ul>
  <li>AdaMax: can be more stable than Adam in some datasets, try if experiencing problems with Adam</li>
  <li>Nadam: Adam + Nesterov -&gt; often converge slightly faster than Adam</li>
</ul>

<p>The optimizers discussed rely on <em>First-order partial derivatives (Jacobians)</em>. Second-order partial derivatives (Hessians) exists in literature, but are just too slow to compute (when they fit in memory!)</p>

<blockquote>
  <p><strong>Training Sparse Models</strong>: to achieve fast model at runtime with less memory. Get rid of tiny weights. Apply strong L1 regularization during training. If still insufficient -&gt; Tensorflow Model Optimization Toolkit (TF-MOT)</p>
</blockquote>

<h3 id="learning-rate-scheduling">
<a class="anchor" href="#learning-rate-scheduling" aria-hidden="true"><span class="octicon octicon-link"></span></a>Learning Rate Scheduling</h3>
<p>Learning schedules -&gt; vary the lr during training</p>
<ul>
  <li>Power scheduling: lr drops at each step; first drops quickly, then more and more slowly</li>
  <li>
<strong>Exponential scheduling</strong>: slashs the lr by a factor of 10 every s steps</li>
  <li>Piecewise constant scheduling: requires fiddling with the sequence of steps</li>
  <li>
<strong>Performance scheduling</strong>: measure validation error, reduce the lr when the error stops dropping</li>
  <li>
<strong>1cycle scheduling</strong>: increases then decreases and plays with momentum -&gt; paper showing speed up in training and better performance in fewer epochs</li>
</ul>

<h3 id="avoiding-overfitting-through-regularization">
<a class="anchor" href="#avoiding-overfitting-through-regularization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Avoiding Overfitting Through Regularization</h3>
<ul>
  <li>early stopping is one of the best regularization techniques</li>
  <li>batch normalization is very good too</li>
</ul>

<h3 id="l1-and-l2-regularization">
<a class="anchor" href="#l1-and-l2-regularization" aria-hidden="true"><span class="octicon octicon-link"></span></a>L1 and L2 Regularization</h3>
<ul>
  <li>L2 -&gt; constrain NN’s weights</li>
  <li>L1 -&gt; if you want a sparse model (many weights = 0)</li>
</ul>

<blockquote>
  <p>functools.partial() -&gt; create a thin wrapper for any callable, with some default arguments</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="n">RegularizedDense</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">,</span>
                           <span class="n">activation</span><span class="o">=</span><span class="s">"elu"</span><span class="p">,</span>
                           <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">"he_normal"</span><span class="p">,</span>
                           <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="mf">0.01</span><span class="p">))</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">]),</span>
    <span class="n">RegularizedDense</span><span class="p">(</span><span class="mi">300</span><span class="p">),</span>
    <span class="n">RegularizedDense</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
    <span class="n">RegularizedDense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"softmax"</span><span class="p">,</span>
                     <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">"glorot_uniform"</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div>

<h3 id="dropout">
<a class="anchor" href="#dropout" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dropout</h3>
<p>One of the most popular regularization techniques for DNNs</p>
<ul>
  <li>at every training step, every neuron (only exception = output neurons) has a probability <code class="highlighter-rouge">p</code> (10-50%) of being temporarily ignored (dropped out) -&gt; may be active in the next step</li>
</ul>

<blockquote>
  <p>in practice, usually apply dropout to the neurons in the top one to three layers (excluding output layer)</p>
</blockquote>

<ul>
  <li>
    <p>dropout is only active during training -&gt; comparing training x validation loss can be misleading -&gt; make sure to evaluate the training loss without dropout (after training)</p>
  </li>
  <li>
    <p>many state of the art only use dropout after the last hidden layer</p>
  </li>
</ul>

<h3 id="monte-carlo-mc-dropout">
<a class="anchor" href="#monte-carlo-mc-dropout" aria-hidden="true"><span class="octicon octicon-link"></span></a>Monte Carlo (MC) Dropout</h3>

<ul>
  <li>
    <p>dropout networks have a profound connection with approximate Bayesian inference -&gt; solid math justification</p>
  </li>
  <li>
    <p>MC Dropout -&gt; boost the performance of any trained dropout model without having to retrain it or even modify it at all</p>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_probas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">model</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
                     <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">)])</span>
<span class="n">y_proba</span> <span class="o">=</span> <span class="n">y_probas</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>Averaging over multiple predictions with dropout on gives us a Monte Carlo estimate that is generally more reliable than the result of a single prediction with dropout off</p>

<h3 id="default-dnn-configuration">
<a class="anchor" href="#default-dnn-configuration" aria-hidden="true"><span class="octicon octicon-link"></span></a>Default DNN configuration</h3>
<p><strong>Hyperparameter</strong> - <strong>Default value</strong></p>

<p>Kernel initializer - He initialization</p>

<p>Activation function - ELU</p>

<p>Normalization - None if shallow; Batch Norm if deep</p>

<p>Regularization - Early stopping (+ℓ2 reg. if needed)</p>

<p>Optimizer - Momentum optimization (or RMSProp or Nadam)</p>

<p>Learning rate schedule - 1cycle</p>

<h3 id="dnn-configuration-for-a-self-normalizing-net">
<a class="anchor" href="#dnn-configuration-for-a-self-normalizing-net" aria-hidden="true"><span class="octicon octicon-link"></span></a>DNN configuration for a self-normalizing net</h3>
<p><strong>Hyperparameter</strong> - <strong>Default value</strong></p>

<p>Kernel initializer - LeCun initialization</p>

<p>Activation function - SELU</p>

<p>Normalization - None (self-normalization)</p>

<p>Regularization - Alpha dropout if needed</p>

<p>Optimizer - Momentum optimization (or RMSProp or Nadam)</p>

<p>Learning rate schedule - 1cycle</p>

<blockquote>
  <p><strong>TIP</strong>: Refer back to the summary at the end of Chapter 11!</p>
</blockquote>

<h1 id="ch12-custom-models-and-training-with-tensorflow">
<a class="anchor" href="#ch12-custom-models-and-training-with-tensorflow" aria-hidden="true"><span class="octicon octicon-link"></span></a>CH12. Custom Models and Training with TensorFlow</h1>

<h1 id="ch13-loading-and-preprocessing-data-with-tensorflow">
<a class="anchor" href="#ch13-loading-and-preprocessing-data-with-tensorflow" aria-hidden="true"><span class="octicon octicon-link"></span></a>CH13. Loading and Preprocessing Data with TensorFlow</h1>

<h1 id="ch14-deep-computer-vision-using-convolutional-neural-networks">
<a class="anchor" href="#ch14-deep-computer-vision-using-convolutional-neural-networks" aria-hidden="true"><span class="octicon octicon-link"></span></a>CH14. Deep Computer Vision Using Convolutional Neural Networks</h1>

<h1 id="ch15-processing-sequences-using-rnns-and-cnns">
<a class="anchor" href="#ch15-processing-sequences-using-rnns-and-cnns" aria-hidden="true"><span class="octicon octicon-link"></span></a>CH15. Processing Sequences Using RNNs and CNNs</h1>

<h2 id="recurrent-neurons-and-layers">
<a class="anchor" href="#recurrent-neurons-and-layers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Recurrent Neurons and Layers</h2>
<p>RNN -&gt; much like a feedforward NN, except it also has connections pointing backward.</p>

<p><em>Unrolling the network through time</em></p>

<p>Each neuron has two sets of weights: one for the inputs and other for the outputs of the previous time step</p>

<h3 id="memory-cells">
<a class="anchor" href="#memory-cells" aria-hidden="true"><span class="octicon octicon-link"></span></a>Memory Cells</h3>
<ul>
  <li>
<em>Memory</em>: the output of a recurrent neuron at time step <code class="highlighter-rouge">t</code> is a function of all the inputs from previous steps</li>
  <li>
<em>Memory cell (or just cell)</em>: part of a NN that preserves some state across time steps. Capable of learning short patterns (about 10 steps long depending on the task)</li>
</ul>

<h2 id="input-and-output-sequences">
<a class="anchor" href="#input-and-output-sequences" aria-hidden="true"><span class="octicon octicon-link"></span></a>Input and Output Sequences</h2>
<ul>
  <li>
<strong>Sequence-to-sequence network</strong>: RNN that takes a sequence of inputs and produce a sequence of outputs. Useful for predicting time series</li>
  <li>
<strong>Sequence-to-vector network</strong>: feed the network a sequence of inputs and ignore all outputs except for the last one</li>
  <li>
<strong>Vector-to-sequence</strong>: feed the same input vector over and over again at each time step and let it output a sequence</li>
  <li>
<strong>Encoder (sequence-to-vector)-Decoder (vector-to-sequence)</strong>: i.e., translation: feed the network a sentence in one language, the encoder convert into a single vector representation, and then the decoder decode the vector into a sentence in another language</li>
</ul>

<h2 id="training-rnns">
<a class="anchor" href="#training-rnns" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training RNNs</h2>
<ul>
  <li>
<em>Backpropagation through time (BPTT)</em>: forward pass throught the unrolled network, then the output sequence is evaluated using a cost function. The gradients of that cost function are then propagated backward through the unrolled network. Finally the model parameters are updated using the gradients computed during BPTT.</li>
</ul>

<blockquote>
  <p>The gradients flow backward through all the outputs used by the cost function, not just the final output. Since the same parameters <code class="highlighter-rouge">W</code> and <code class="highlighter-rouge">b</code> are used at each time step, backpropagation will do the right thing and sum over all time steps</p>
</blockquote>

<h2 id="forecasting-a-time-series">
<a class="anchor" href="#forecasting-a-time-series" aria-hidden="true"><span class="octicon octicon-link"></span></a>Forecasting a Time Series</h2>
<p>Time series: the input features are generally represented as 3D arrays of shape [batch size, time steps, dimensionallity], where dimensionallity is 1 for <em>univariate time series</em> and more for <em>multivariate time series</em></p>

<h3 id="baseline-metrics">
<a class="anchor" href="#baseline-metrics" aria-hidden="true"><span class="octicon octicon-link"></span></a>Baseline Metrics</h3>
<ul>
  <li>
<em>Naive forecasting</em>: predict the last value in each series</li>
  <li>Fully connected network</li>
</ul>

<h3 id="implementing-a-simple-rnn">
<a class="anchor" href="#implementing-a-simple-rnn" aria-hidden="true"><span class="octicon octicon-link"></span></a>Implementing a Simple RNN</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
  <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">SimpleRNN</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="p">])</span>
</code></pre></div></div>

<blockquote>
  <p><strong>Trend and Seasonality</strong>: when using RNNs, it is generally not necessary to remove trend/seasonality before fitting, but it may improve performance in some cases, since the model will not have to learn the trend and the seasonality</p>
</blockquote>

<h3 id="deep-rnns">
<a class="anchor" href="#deep-rnns" aria-hidden="true"><span class="octicon octicon-link"></span></a>Deep RNNs</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">SimpleRNN</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">SimpleRNN</span><span class="p">(</span><span class="mi">20</span><span class="p">),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div>

<h3 id="forecasting-several-time-steps-ahead">
<a class="anchor" href="#forecasting-several-time-steps-ahead" aria-hidden="true"><span class="octicon octicon-link"></span></a>Forecasting Several Time Steps Ahead</h3>
<ul>
  <li>First option, use the model already trained, make it predict the next value, then add that value to the inputs, and use the model again to predict the following value… Errors might accumulate</li>
  <li>Second option, train an RNN to predict all 10 next values at once</li>
</ul>

<blockquote>
  <p>It may be surprising that the targets will contain values that appear in the inputs (there is a lot of overlap between X_train and Y_train). Isn’t that cheating? Fortunately, not at all: at each time step, the model only knows about past time steps, so it cannot look ahead. It is said to be a <em>causal model</em>.</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">SimpleRNN</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">SimpleRNN</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">TimeDistributed</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="p">])</span>
</code></pre></div></div>

<blockquote>
  <p>Forecasting: often useful to have some error bars along with your predictions. Add an MC Dropout layer within each memory cell, dropping part of the inputs and hidden states. After training, to forecast a new time series, use the model many times and compute the mean and stdev of the predictions at each time step</p>
</blockquote>

<h2 id="handling-long-sequences">
<a class="anchor" href="#handling-long-sequences" aria-hidden="true"><span class="octicon octicon-link"></span></a>Handling Long Sequences</h2>
<h3 id="fighting-the-unstable-gradients-problem">
<a class="anchor" href="#fighting-the-unstable-gradients-problem" aria-hidden="true"><span class="octicon octicon-link"></span></a>Fighting the Unstable Gradients Problem</h3>
<ul>
  <li>Good parameter initialization</li>
  <li>Faster optimizers</li>
  <li>Dropout</li>
  <li>Saturating activation function: hyperbolic tangent</li>
</ul>

<p>Batch Normalization cannot be used as efficiently with RNNs -&gt; another form of normalization often works better: <strong>Layer Normalization</strong> -&gt; similar no BN, but instead of normalizing across the batch dimension, it normalizes across the features dimension</p>

<h3 id="tackling-the-short-term-memory-problem">
<a class="anchor" href="#tackling-the-short-term-memory-problem" aria-hidden="true"><span class="octicon octicon-link"></span></a>Tackling the Short-Term Memory Problem</h3>
<p>Due to the transformations that the data goes through when traversing an RNN, some information is lost at each time step. After a while, the RNN’s state contains virtually no trace of the first inputs</p>

<h3 id="lstm-long-short-term-memory-cells">
<a class="anchor" href="#lstm-long-short-term-memory-cells" aria-hidden="true"><span class="octicon octicon-link"></span></a>LSTM (Long Short-Term Memory) cells</h3>
<p>LSTM cell looks exactly like a regular cell, except that its state is split into two vectors: h(t) and c(t) (“c” stands for “cell”). You can think of h(t) as the short-term state and c(t) as the long-term state</p>

<p>The key idea is that the network can learn what to store in the long-term state, what to throw away, and what to read from it</p>

<blockquote>
  <p>LSTM cell can learn to recognize an important input (that’s the role of the input gate), store it in the long-term state, preserve it for as long as it is needed (that’s the role of the forget gate), and extract it whenever it is needed</p>
</blockquote>

<h3 id="gru-gated-recurrent-unit-cells">
<a class="anchor" href="#gru-gated-recurrent-unit-cells" aria-hidden="true"><span class="octicon octicon-link"></span></a>GRU (Gated Recurrent Unit) cells</h3>
<p>Simplified version of the LSTM cell that performs just as well</p>

<p>LSTM and GRU cells are one of the main reasons behind the success of RNNs. Yet while they can tackle much longer sequences than simple RNNs, they still have a fairly limited short-term memory, and they have a hard time learning long-term patterns in sequences of 100 time steps or more, such as audio samples, long time series, or long sentences. One way to solve this is to shorten the input sequences, for example using 1D convolutional layers</p>

<h3 id="using-1d-convolutional-layers-to-process-sequences">
<a class="anchor" href="#using-1d-convolutional-layers-to-process-sequences" aria-hidden="true"><span class="octicon octicon-link"></span></a>Using 1D convolutional layers to process sequences</h3>
<p>A 1D convolutional layer slides several kernels across a sequence, producing a 1D feature map per kernel. Each kernel will learn to detect a single very short sequential pattern (no longer than the kernel size)</p>

<p>By shortening the sequences, the convolutional layer may help the GRU layers detect longer patterns</p>

<blockquote>
  <p>It is actually possible to use only 1D convolutional layers and drop the recurrent layers entirely</p>
</blockquote>

<h3 id="wavenet">
<a class="anchor" href="#wavenet" aria-hidden="true"><span class="octicon octicon-link"></span></a>WaveNet</h3>
<p>WaveNet: Stacked 1D convolutional layers, doubling the dilation rate (how spread apart each neuron’s inputs are) at every layer</p>

<p>Lower layers learn short-term patterns, while the higher layers learn long-term patterns. Thanks to the doubling dilation rate, the network can process extremely large sequences very efficiently</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">InputLayer</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
<span class="k">for</span> <span class="n">rate</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span><span class="p">:</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv1D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">"causal"</span><span class="p">,</span>
                                  <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">,</span> <span class="n">dilation_rate</span><span class="o">=</span><span class="n">rate</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv1D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">"mse"</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s">"adam"</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">last_time_step_mse</span><span class="p">])</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">Y_valid</span><span class="p">))</span>
</code></pre></div></div>

<h1 id="ch16-natural-language-processing-with-rnns-and-attention">
<a class="anchor" href="#ch16-natural-language-processing-with-rnns-and-attention" aria-hidden="true"><span class="octicon octicon-link"></span></a>CH16. Natural Language Processing with RNNs and Attention</h1>
<ul>
  <li>
<em>character RNN</em>: predict the next character in a sentence</li>
  <li>
<em>stateless RNN</em>: learns on random portions of text at each iteration, without any information on the rest of the text</li>
  <li>
<em>stateful RNN</em>: preserves the hidden state between training iterations and continues reading where it left off, allowing it to learn longer patterns</li>
</ul>

<blockquote>
  <p><em>Char-RNN</em>: “The Unreasonable Effectiveness of Recurrent Neural Networks”, Andrej Karpathy</p>
</blockquote>

<h2 id="character-rnn">
<a class="anchor" href="#character-rnn" aria-hidden="true"><span class="octicon octicon-link"></span></a>Character RNN</h2>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">preprocessing</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">Tokenizer</span><span class="p">(</span><span class="n">char_level</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">([</span><span class="n">shakespeare_text</span><span class="p">])</span>
</code></pre></div></div>

<p><strong>How to Split a Sequential Dataset</strong></p>

<p>Assuming the time series is <em>stationary</em> -&gt; split across time</p>

<ul>
  <li>
<em>truncated backpropagation through time</em>: RNN is unrolled over the length of every short substring of the whole text (<em>window()</em> method)</li>
</ul>

<blockquote>
  <p>n_steps = 100 -&gt; RNN will only be able to learn patterns shorter or equal to n_steps (100 in this case). Higher n_steps -&gt; harder to train!</p>
</blockquote>

<h3 id="stateful-rnn">
<a class="anchor" href="#stateful-rnn" aria-hidden="true"><span class="octicon octicon-link"></span></a>Stateful RNN</h3>
<ul>
  <li>shift = n_steps (instead of 1 like stateless RNN) when calling <em>window()</em>
</li>
  <li>we must obviously not call <em>shuffle()</em> method</li>
  <li>harder to do batching</li>
</ul>

<blockquote>
  <p>After the stateful model is trained, it will only be possible to use it to make predictions for batches of the same size as were used during training. To avoid this restriction, create an identical stateless model, and copy the stateful model’s weights to this model.</p>
</blockquote>

<h3 id="sentiment-analysis">
<a class="anchor" href="#sentiment-analysis" aria-hidden="true"><span class="octicon octicon-link"></span></a>Sentiment Analysis</h3>
<ul>
  <li>
<em>Google’s SentencePiece</em>: unsupervised learning technique to tokenize/detokenize text at the subword level in a language-independent way, treating spaces like other characters</li>
  <li><em>Byte pair encoding</em></li>
  <li>
<em>TF.Text</em> -&gt; <em>WordPiece</em>
</li>
</ul>

<h3 id="reusing-pretrained-embeddings">
<a class="anchor" href="#reusing-pretrained-embeddings" aria-hidden="true"><span class="octicon octicon-link"></span></a>Reusing Pretrained Embeddings</h3>
<p>Tensorflow Hub project: model components called <em>modules</em>. Browse the TF Hub repository -&gt; copy the code example into your project -&gt; module will be downloaded, along with its pretrained weights, and included in your model</p>

<blockquote>
  <p>Warning: Not all TF Hub modules support TensorFlow 2 -&gt; check before</p>
</blockquote>

<h2 id="an-encoder-decoder-network-for-neural-machine-translation-nmt">
<a class="anchor" href="#an-encoder-decoder-network-for-neural-machine-translation-nmt" aria-hidden="true"><span class="octicon octicon-link"></span></a>An Encoder-Decoder Network for Neural Machine Translation (NMT)</h2>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow_addons</span> <span class="k">as</span> <span class="n">tfa</span>

<span class="n">encoder_inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">decoder_inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">sequence_lengths</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>

<span class="n">encoder_embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">(</span><span class="n">encoder_inputs</span><span class="p">)</span>
<span class="n">decoder_embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">(</span><span class="n">decoder_inputs</span><span class="p">)</span>

<span class="n">encoder</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">return_state</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">state_h</span><span class="p">,</span> <span class="n">state_c</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">encoder_embeddings</span><span class="p">)</span>
<span class="n">encoder_state</span> <span class="o">=</span> <span class="p">[</span><span class="n">state_h</span><span class="p">,</span> <span class="n">state_c</span><span class="p">]</span>

<span class="n">sampler</span> <span class="o">=</span> <span class="n">tfa</span><span class="o">.</span><span class="n">seq2seq</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">TrainingSampler</span><span class="p">()</span>

<span class="n">decoder_cell</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LSTMCell</span><span class="p">(</span><span class="mi">512</span><span class="p">)</span>
<span class="n">output_layer</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">tfa</span><span class="o">.</span><span class="n">seq2seq</span><span class="o">.</span><span class="n">basic_decoder</span><span class="o">.</span><span class="n">BasicDecoder</span><span class="p">(</span><span class="n">decoder_cell</span><span class="p">,</span> <span class="n">sampler</span><span class="p">,</span>
                                                 <span class="n">output_layer</span><span class="o">=</span><span class="n">output_layer</span><span class="p">)</span>
<span class="n">final_outputs</span><span class="p">,</span> <span class="n">final_state</span><span class="p">,</span> <span class="n">final_sequence_lengths</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span>
    <span class="n">decoder_embeddings</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">encoder_state</span><span class="p">,</span>
    <span class="n">sequence_length</span><span class="o">=</span><span class="n">sequence_lengths</span><span class="p">)</span>
<span class="n">Y_proba</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">final_outputs</span><span class="o">.</span><span class="n">rnn_output</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">encoder_inputs</span><span class="p">,</span> <span class="n">decoder_inputs</span><span class="p">,</span> <span class="n">sequence_lengths</span><span class="p">],</span>
                    <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">Y_proba</span><span class="p">])</span>
</code></pre></div></div>

<h3 id="bidirectional-rnns">
<a class="anchor" href="#bidirectional-rnns" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bidirectional RNNs</h3>
<p>Normally -&gt; only looks at past and present inputs before generating its outputs -&gt; it’s “causal” (cannot look into the future) -&gt; makes sense for forecasting time series</p>

<p>For many NLP tasks, often is preferable to look ahead at the next words -&gt; <em>bidirectional recurrent layer</em> (<code class="highlighter-rouge">keras.layers.Bidirectional</code>)</p>

<h3 id="beam-search">
<a class="anchor" href="#beam-search" aria-hidden="true"><span class="octicon octicon-link"></span></a>Beam Search</h3>
<p>Keeps track of a short list of the <code class="highlighter-rouge">k</code> most promising sentences, and at each decoder step it tries to extend them by one word, keeping only the <code class="highlighter-rouge">k</code> most likely sentences. <code class="highlighter-rouge">k</code> = <em>beam width</em></p>

<h2 id="attention-mechanisms">
<a class="anchor" href="#attention-mechanisms" aria-hidden="true"><span class="octicon octicon-link"></span></a>Attention Mechanisms</h2>
<p>Allow the decoder to focus on the appropriate words (as encoded by the encoder) at each time step -&gt; the path from an input word to its translation is now much shorter, so the short-term memory limitations of RNNs have much less impact</p>

<blockquote>
  <p><em>Alignment model / attention layer</em>: small neural network trained jointly with the rest of the Encoder-Decoder model</p>
</blockquote>

<ul>
  <li>
<em>Bahdanau attention</em>: concatenative/additive attention</li>
  <li>
<em>Luong attention</em>: multiplicative attention</li>
</ul>

<h3 id="visual-attention">
<a class="anchor" href="#visual-attention" aria-hidden="true"><span class="octicon octicon-link"></span></a>Visual Attention</h3>
<p>Generate image captions using visual atention: a CNN processes the image and outputs some feature maps, then a decoded RNN with an attention mechanism generates the caption, one word at a time</p>

<blockquote>
  <p><strong>Explainability</strong>: Attention mechanisms make it easier to understand what led the model to produce its output -&gt; especially useful when the model makes a mistake (check what the model focused on). Explainability may be a legal requirement in some applications, i.e., system deciding whether or not it should grant a loan</p>
</blockquote>

<p>“Attention mechanisms are so powerful that you can actually build state-of-the-art models using only attention mechanisms”</p>

<h3 id="attention-is-all-you-need-the-transformer-architecture">
<a class="anchor" href="#attention-is-all-you-need-the-transformer-architecture" aria-hidden="true"><span class="octicon octicon-link"></span></a>Attention is All You Need: The Transformer Architecture</h3>
<p>Google’s research -&gt; <em>Transformer</em>. Improved state of the art NMT without using recurrent or convolutional layers, just attention mechanisms. Also faster to train and easier to parallelize.</p>

<h1 id="ch17-representation-learning-and-generative-learning-using-autoencoders-and-gans">
<a class="anchor" href="#ch17-representation-learning-and-generative-learning-using-autoencoders-and-gans" aria-hidden="true"><span class="octicon octicon-link"></span></a>CH17. Representation Learning and Generative Learning Using Autoencoders and GANs</h1>

<h1 id="ch18-reinforcement-learning">
<a class="anchor" href="#ch18-reinforcement-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>CH18. Reinforcement Learning</h1>

<h1 id="ch19-training-and-deploying-tensorflow-models-at-scale">
<a class="anchor" href="#ch19-training-and-deploying-tensorflow-models-at-scale" aria-hidden="true"><span class="octicon octicon-link"></span></a>CH19. Training and Deploying TensorFlow Models at Scale</h1>

  </div><a class="u-url" href="/blog/book/machine%20learning/data%20science/2019/12/24/handson-ml.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Data Science and Machine Learning blog.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/millengustavo" title="millengustavo"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/millengustavo" title="millengustavo"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/millengustavo" title="millengustavo"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
