<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Python Machine Learning | Gustavo Millen</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Python Machine Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="My notes and highlights on the book." />
<meta property="og:description" content="My notes and highlights on the book." />
<link rel="canonical" href="https://millengustavo.github.io/blog/book/machine%20learning/data%20science/2019/12/26/python-ml.html" />
<meta property="og:url" content="https://millengustavo.github.io/blog/book/machine%20learning/data%20science/2019/12/26/python-ml.html" />
<meta property="og:site_name" content="Gustavo Millen" />
<meta property="og:image" content="https://millengustavo.github.io/blog/images/python_ml/python_ml.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-12-26T00:00:00-06:00" />
<script type="application/ld+json">
{"headline":"Python Machine Learning","dateModified":"2019-12-26T00:00:00-06:00","description":"My notes and highlights on the book.","datePublished":"2019-12-26T00:00:00-06:00","@type":"BlogPosting","image":"https://millengustavo.github.io/blog/images/python_ml/python_ml.jpg","url":"https://millengustavo.github.io/blog/book/machine%20learning/data%20science/2019/12/26/python-ml.html","mainEntityOfPage":{"@type":"WebPage","@id":"https://millengustavo.github.io/blog/book/machine%20learning/data%20science/2019/12/26/python-ml.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://millengustavo.github.io/blog/feed.xml" title="Gustavo Millen" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Python Machine Learning | Gustavo Millen</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Python Machine Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="My notes and highlights on the book." />
<meta property="og:description" content="My notes and highlights on the book." />
<link rel="canonical" href="https://millengustavo.github.io/blog/book/machine%20learning/data%20science/2019/12/26/python-ml.html" />
<meta property="og:url" content="https://millengustavo.github.io/blog/book/machine%20learning/data%20science/2019/12/26/python-ml.html" />
<meta property="og:site_name" content="Gustavo Millen" />
<meta property="og:image" content="https://millengustavo.github.io/blog/images/python_ml/python_ml.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-12-26T00:00:00-06:00" />
<script type="application/ld+json">
{"headline":"Python Machine Learning","dateModified":"2019-12-26T00:00:00-06:00","description":"My notes and highlights on the book.","datePublished":"2019-12-26T00:00:00-06:00","@type":"BlogPosting","image":"https://millengustavo.github.io/blog/images/python_ml/python_ml.jpg","url":"https://millengustavo.github.io/blog/book/machine%20learning/data%20science/2019/12/26/python-ml.html","mainEntityOfPage":{"@type":"WebPage","@id":"https://millengustavo.github.io/blog/book/machine%20learning/data%20science/2019/12/26/python-ml.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://millengustavo.github.io/blog/feed.xml" title="Gustavo Millen" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Gustavo Millen</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Python Machine Learning</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2019-12-26T00:00:00-06:00" itemprop="datePublished">
        Dec 26, 2019
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      38 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#book">book</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#machine learning">machine learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#data science">data science</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#ch1-giving-computers-the-ability-to-learn-from-data">Ch1. Giving Computers the Ability to Learn from Data</a>
<ul>
<li class="toc-entry toc-h2"><a href="#supervised-learning">Supervised Learning</a></li>
<li class="toc-entry toc-h2"><a href="#solving-interactive-problems-with-reinforcement-learning">Solving interactive problems with reinforcement learning</a></li>
<li class="toc-entry toc-h2"><a href="#discovering-hidden-structures-with-unsupervised-learning">Discovering hidden structures with unsupervised learning</a>
<ul>
<li class="toc-entry toc-h3"><a href="#clustering">Clustering</a></li>
<li class="toc-entry toc-h3"><a href="#dimensionality-reduction-for-data-compression">Dimensionality reduction for data compression</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#a-roadmap-for-building-ml-systems">A roadmap for building ML systems</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#ch2-training-simple-machine-learning-algorithms-for-classification">Ch2. Training Simple Machine Learning Algorithms for Classification</a>
<ul>
<li class="toc-entry toc-h2"><a href="#perceptron">Perceptron</a></li>
<li class="toc-entry toc-h2"><a href="#adaptive-linear-neurons-and-the-convergence-of-learning">Adaptive linear neurons and the convergence of learning</a></li>
<li class="toc-entry toc-h2"><a href="#minimizing-cost-functions-with-gradient-descent">Minimizing cost functions with gradient descent</a></li>
<li class="toc-entry toc-h2"><a href="#improving-gradient-descent-through-feature-scaling">Improving gradient descent through feature scaling</a></li>
<li class="toc-entry toc-h2"><a href="#large-scale-machine-learning-and-stochastic-gradient-descent">Large-scale machine learning and stochastic gradient descent</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#ch3-a-tour-of-machine-learning-classifiers-using-scikit-learn">Ch3. A Tour of Machine Learning Classifiers Using scikit-learn</a></li>
<li class="toc-entry toc-h1"><a href="#ch4-building-good-training-datasets--data-preprocessing">Ch4. Building Good Training Datasets – Data Preprocessing</a>
<ul>
<li class="toc-entry toc-h3"><a href="#missing-data">Missing Data</a></li>
<li class="toc-entry toc-h3"><a href="#categorical-features">Categorical Features</a></li>
<li class="toc-entry toc-h3"><a href="#feature-scaling">Feature Scaling</a></li>
<li class="toc-entry toc-h3"><a href="#regularization">Regularization</a></li>
<li class="toc-entry toc-h3"><a href="#random-forest-feature-importance">Random Forest feature importance</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#ch5-compressing-data-via-dimensionality-reduction">Ch5. Compressing Data via Dimensionality Reduction</a>
<ul>
<li class="toc-entry toc-h3"><a href="#feature-extraction">Feature Extraction</a></li>
<li class="toc-entry toc-h3"><a href="#pca">PCA</a></li>
<li class="toc-entry toc-h3"><a href="#lda">LDA</a></li>
<li class="toc-entry toc-h3"><a href="#kernel-pca">Kernel PCA</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#ch6-learning-best-practices-for-model-evaluation-and-hyperparameter-tuning">Ch6. Learning Best Practices for Model Evaluation and Hyperparameter Tuning</a>
<ul>
<li class="toc-entry toc-h3"><a href="#combining-transformers-and-estimators-in-a-pipeline">Combining transformers and estimators in a pipeline</a></li>
<li class="toc-entry toc-h3"><a href="#using-k-fold-cross-validation-to-assess-model-performance">Using k-fold cross-validation to assess model performance</a></li>
<li class="toc-entry toc-h3"><a href="#k-fold-cross-validation">K-fold cross-validation</a></li>
<li class="toc-entry toc-h2"><a href="#choosing-k">Choosing K</a>
<ul>
<li class="toc-entry toc-h3"><a href="#stratified-cross-validation">Stratified cross-validation</a></li>
<li class="toc-entry toc-h3"><a href="#bias-x-variance">Bias x Variance</a></li>
<li class="toc-entry toc-h3"><a href="#debugging-algorithms-with-learning-and-validation-curves">Debugging algorithms with learning and validation curves</a></li>
<li class="toc-entry toc-h3"><a href="#fine-tuning-machine-learning-models">Fine-tuning machine learning models</a></li>
<li class="toc-entry toc-h3"><a href="#algorithm-selection-with-nested-cross-validation">Algorithm selection with nested cross-validation</a></li>
<li class="toc-entry toc-h3"><a href="#looking-at-different-performance-evaluation-metrics">Looking at different performance evaluation metrics</a>
<ul>
<li class="toc-entry toc-h4"><a href="#scoring-metrics-for-multiclass-classification">Scoring metrics for multiclass classification</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#dealing-with-class-imbalance">Dealing with class imbalance</a></li>
<li class="toc-entry toc-h3"><a href="#smote">SMOTE</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#ch7-combining-different-models-for-ensemble-learning">Ch7. Combining different models for Ensemble Learning</a>
<ul>
<li class="toc-entry toc-h3"><a href="#voting">Voting</a></li>
<li class="toc-entry toc-h3"><a href="#stacking">Stacking</a></li>
<li class="toc-entry toc-h3"><a href="#bagging">Bagging</a></li>
<li class="toc-entry toc-h3"><a href="#boosting">Boosting</a></li>
<li class="toc-entry toc-h3"><a href="#gradient-boosting">Gradient Boosting</a>
<ul>
<li class="toc-entry toc-h4"><a href="#xgboost">XGBoost</a></li>
<li class="toc-entry toc-h4"><a href="#histgradientboosting">HistGradientBoosting</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#ch8-applying-machine-learning-to-sentiment-analysis">Ch8. Applying Machine Learning to Sentiment Analysis</a>
<ul>
<li class="toc-entry toc-h3"><a href="#bag-of-words">Bag-of-words</a>
<ul>
<li class="toc-entry toc-h4"><a href="#n-grams">N-grams</a></li>
<li class="toc-entry toc-h4"><a href="#tf-idf">TF-IDF</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#cleaning-text">Cleaning text</a>
<ul>
<li class="toc-entry toc-h4"><a href="#stemming">Stemming</a></li>
<li class="toc-entry toc-h4"><a href="#lemmatization">Lemmatization</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#stop-word-removal">Stop-word removal</a></li>
<li class="toc-entry toc-h3"><a href="#naive-bayes-classifier">Naive Bayes Classifier</a></li>
<li class="toc-entry toc-h3"><a href="#out-of-core-learning">Out-of-core learning</a></li>
<li class="toc-entry toc-h3"><a href="#word2vec">word2vec</a></li>
<li class="toc-entry toc-h3"><a href="#topic-modeling">Topic Modeling</a></li>
<li class="toc-entry toc-h3"><a href="#latent-dirichlet-allocation---lda-">Latent Dirichlet Allocation  ( LDA )</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#ch9-embedding-a-machine-learning-model-into-a-web-application">Ch9. Embedding a Machine Learning Model into a Web Application</a>
<ul>
<li class="toc-entry toc-h3"><a href="#serializing-fitted-scikit-learn-estimators">Serializing fitted scikit-learn estimators</a></li>
<li class="toc-entry toc-h3"><a href="#sqlite">SQLite</a></li>
<li class="toc-entry toc-h3"><a href="#flask">Flask</a></li>
<li class="toc-entry toc-h3"><a href="#jinja2">Jinja2</a></li>
<li class="toc-entry toc-h3"><a href="#pythonanywhere">PythonAnywhere</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#ch10-predicting-continuous-target-variables-with-regression-analysis">Ch10. Predicting Continuous Target Variables with Regression Analysis</a>
<ul>
<li class="toc-entry toc-h3"><a href="#linear-regression">Linear Regression</a></li>
<li class="toc-entry toc-h3"><a href="#visualizing-the-important-characteristics-of-a-dataset">Visualizing the important characteristics of a dataset</a></li>
<li class="toc-entry toc-h3"><a href="#looking-at-relationships-using-a-correlation-matrix">Looking at relationships using a correlation matrix</a></li>
<li class="toc-entry toc-h3"><a href="#estimating-the-coefficient-of-a-regression-model-via-scikit-learn">Estimating the coefficient of a regression model via scikit-learn</a></li>
<li class="toc-entry toc-h3"><a href="#fitting-a-robust-regression-model-using-ransac">Fitting a robust regression model using RANSAC</a></li>
<li class="toc-entry toc-h3"><a href="#evaluating-the-performance-of-linear-regression-models">Evaluating the performance of linear regression models</a></li>
<li class="toc-entry toc-h3"><a href="#using-regularized-methods-for-regression">Using regularized methods for regression</a></li>
<li class="toc-entry toc-h3"><a href="#dealing-with-nonlinear-relationships-using-random-forests">Dealing with nonlinear relationships using random forests</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#ch11-working-with-unlabeled-data---clustering-analysis">Ch11. Working with Unlabeled Data - Clustering Analysis</a>
<ul>
<li class="toc-entry toc-h3"><a href="#grouping-objects-by-similarity-using-k-means">Grouping objects by similarity using k-means</a>
<ul>
<li class="toc-entry toc-h4"><a href="#hard-versus-soft-clustering">Hard versus soft clustering</a></li>
<li class="toc-entry toc-h4"><a href="#using-the-elbow-method-to-find-the-optimal-number-of-clusters">Using the elbow method to find the optimal number of clusters</a></li>
<li class="toc-entry toc-h4"><a href="#quantifying-the-quality-of-clustering-via-silhouette-plots">Quantifying the quality of clustering via silhouette plots</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#organizing-clusters-as-a-hierarchical-tree">Organizing clusters as a hierarchical tree</a>
<ul>
<li class="toc-entry toc-h4"><a href="#grouping-clusters-in-a-bottom-up-fashion">Grouping clusters in a bottom-up fashion</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#locating-regions-of-high-density-via-dbscan">Locating regions of high density via DBSCAN</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#ch12-implementing-a-multilayer-artificial-neural-network-from-scratch">Ch12. Implementing a Multilayer Artificial Neural Network from Scratch</a>
<ul>
<li class="toc-entry toc-h2"><a href="#introducing-the-multilayer-neural-network-architecture">Introducing the multilayer neural network architecture</a></li>
<li class="toc-entry toc-h2"><a href="#activating-a-neural-network-via-forward-propagation">Activating a neural network via forward propagation</a></li>
<li class="toc-entry toc-h2"><a href="#developing-your-understanding-of-backpropagation">Developing your understanding of backpropagation</a></li>
<li class="toc-entry toc-h2"><a href="#about-the-convergence-in-neural-networks">About the convergence in neural networks</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#ch13-parallelizing-neural-network-training-with-tensorflow">Ch13. Parallelizing Neural Network Training with TensorFlow</a></li>
<li class="toc-entry toc-h1"><a href="#ch14-going-deeper---the-mechanics-of-tensorflow">Ch14. Going Deeper - The Mechanics of TensorFlow</a></li>
<li class="toc-entry toc-h1"><a href="#ch15-classifying-images-with-deep-convolutional-neural-networks">Ch15. Classifying Images with Deep Convolutional Neural Networks</a></li>
<li class="toc-entry toc-h1"><a href="#ch16-modeling-sequential-data-using-recurrent-neural-networks">Ch16. Modeling Sequential Data Using Recurrent Neural Networks</a></li>
<li class="toc-entry toc-h1"><a href="#ch17-generative-adversarial-networks-for-synthesizing-new-data">Ch17. Generative Adversarial Networks for Synthesizing New Data</a></li>
<li class="toc-entry toc-h1"><a href="#ch18-reinforcement-learning-for-decision-making-in-complex-environments">Ch18. Reinforcement Learning for Decision Making in Complex Environments</a></li>
</ul><p>My notes and highlights on the book.</p>

<p>Author: Sebastian Rashcka</p>

<h1 id="ch1-giving-computers-the-ability-to-learn-from-data">
<a class="anchor" href="#ch1-giving-computers-the-ability-to-learn-from-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Ch1. Giving Computers the Ability to Learn from Data</h1>

<p>Self-learning algorithms: derive knowledge from data in order to make predictions efficiently</p>

<p>Three types of ML:</p>
<ul>
  <li>Supervised Learning: Labeled data, direct feedback, predict outcome/future</li>
  <li>Unsupervised Learning: No labels, no feedback, find hidden structure in data</li>
  <li>Reinforcement Learning: Decision process, reward system, learn series of actions</li>
</ul>

<h2 id="supervised-learning">
<a class="anchor" href="#supervised-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Supervised Learning</h2>
<ul>
  <li>Discrete class labels -&gt; <strong>classification</strong>
</li>
  <li>Outcome signal is a continuous value -&gt; <strong>regression</strong>
</li>
</ul>

<p>Predictor variables = “features”</p>

<p>Response variable = “target”</p>

<h2 id="solving-interactive-problems-with-reinforcement-learning">
<a class="anchor" href="#solving-interactive-problems-with-reinforcement-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Solving interactive problems with reinforcement learning</h2>
<p>Goal: develop a system (<em>agent</em>) that improve its performance based on interactions with the environment</p>

<blockquote>
  <p>Related to supervised learning, but the feedback is not the ground truth label, but a measure of how well the action was measured by a reward function</p>
</blockquote>

<p>The agent tries to maximize the reward through a series of interactions with the environment</p>

<h2 id="discovering-hidden-structures-with-unsupervised-learning">
<a class="anchor" href="#discovering-hidden-structures-with-unsupervised-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Discovering hidden structures with unsupervised learning</h2>

<h3 id="clustering">
<a class="anchor" href="#clustering" aria-hidden="true"><span class="octicon octicon-link"></span></a>Clustering</h3>
<p>Allows us to organize data into meaningful subgroups (<em>clusters</em>) without having any prior knowledge of their group memberships. Sometimes called <strong>unsupervised classification</strong></p>

<h3 id="dimensionality-reduction-for-data-compression">
<a class="anchor" href="#dimensionality-reduction-for-data-compression" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dimensionality reduction for data compression</h3>
<p>Commonly used approach in feature preprocessing to remove noise from data, which can degrade the predictive performance of certain algorithms, and compress the data onto a smaller dimensional subspace while retaining most of the relevant information</p>

<p>Useful for visualizing the data -&gt; High-dimensional feature set to 2D or 3D</p>

<h2 id="a-roadmap-for-building-ml-systems">
<a class="anchor" href="#a-roadmap-for-building-ml-systems" aria-hidden="true"><span class="octicon octicon-link"></span></a>A roadmap for building ML systems</h2>
<ul>
  <li>
<strong>Preprocessing - getting data into shape</strong>: scaling, dimensionality reduction, randomly divide the dataset into training and test sets</li>
  <li>
<strong>Training and selecting a predictive model</strong>: decide a metric to measure performance, compare a handful of different algorithms in order to train and select the best performing model, cross-validation, hyperparameter optimization</li>
  <li>
<strong>Evaluating models and predicting unseen data instances</strong>: how well the model performs on unseen data (generalization error)</li>
</ul>

<h1 id="ch2-training-simple-machine-learning-algorithms-for-classification">
<a class="anchor" href="#ch2-training-simple-machine-learning-algorithms-for-classification" aria-hidden="true"><span class="octicon octicon-link"></span></a>Ch2. Training Simple Machine Learning Algorithms for Classification</h1>

<h2 id="perceptron">
<a class="anchor" href="#perceptron" aria-hidden="true"><span class="octicon octicon-link"></span></a>Perceptron</h2>
<p>The convergence of the perceptron is only guaranteed if the two classes are linearly separable and the learning rate is sufficiently small</p>

<blockquote>
  <p>Numpy x Python for loop structures: <strong>Vectorization</strong> means that an elemental arithmetic operation is automatically applied to all elements in an array. By formulating our arithmetic operations as a sequence of instructions on an array, rather than performing a set of operations for each element at a time, we can make better use of our modern CPU architectures with single instruction, multiple data (SIMD) support. Furthermore, NumPy uses highly optimized linear algebra libraries, such as Basic Linear Algebra Subprograms (BLAS) and Linear Algebra Package (LAPACK), that have been written in C or Fortran</p>
</blockquote>

<h2 id="adaptive-linear-neurons-and-the-convergence-of-learning">
<a class="anchor" href="#adaptive-linear-neurons-and-the-convergence-of-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Adaptive linear neurons and the convergence of learning</h2>
<p><strong>ADAptive LInear NEuron (Adaline)</strong>: weights are updated based on a linear activation function rather than a unit step function -&gt; Widrow-Hoff rule</p>

<h2 id="minimizing-cost-functions-with-gradient-descent">
<a class="anchor" href="#minimizing-cost-functions-with-gradient-descent" aria-hidden="true"><span class="octicon octicon-link"></span></a>Minimizing cost functions with gradient descent</h2>
<p><strong>Objective function</strong>: often a cost function that we want to minimize</p>

<p><strong>Gradient descent</strong>: powerful optimization algorithm to find the weights that minimize the cost function -&gt; climbing down a hill until a local or global cost minimum is reached -&gt; take steps in the opposite direction of the gradient. Step size defined by the learning rate and slope of the gradient</p>

<blockquote>
  <p>A logistic regression model is closely related to Adaline, the only difference being its activation and cost function</p>
</blockquote>

<h2 id="improving-gradient-descent-through-feature-scaling">
<a class="anchor" href="#improving-gradient-descent-through-feature-scaling" aria-hidden="true"><span class="octicon octicon-link"></span></a>Improving gradient descent through feature scaling</h2>
<p>Gradient descent is one of the many algorithms that benefit from feature scaling</p>

<p><strong>Standardization</strong>: gives the data properties of a standard normal distribution -&gt; zero-mean and unit variance</p>

<h2 id="large-scale-machine-learning-and-stochastic-gradient-descent">
<a class="anchor" href="#large-scale-machine-learning-and-stochastic-gradient-descent" aria-hidden="true"><span class="octicon octicon-link"></span></a>Large-scale machine learning and stochastic gradient descent</h2>
<ul>
  <li>
<strong>Batch gradient descent</strong>: gradient is calculated from the whole training dataset</li>
  <li>
<strong>Stochastic gradient descent (SGD)</strong>: iterative/online gradient descent. Each gradient is calculated on a single training example.</li>
</ul>

<p>Advantages:</p>
<ul>
  <li>typically reaches convergence much faster because of more frequent weight updates</li>
  <li>can escape shallow local minima more readily if we are working with nonlinear cost functions</li>
  <li>can be used for online learning -&gt; model trained on the fly as new data arrives</li>
</ul>

<blockquote>
  <p>With SGD it is important to present training data in a random order; also shuffle the training dataset for every epoch to prevent cycles</p>
</blockquote>

<p><strong>Mini-batch gradient descent</strong>: batch gradient descent to smaller subsets of the training data. Compromise between SGD and batch -&gt; vectorized operations can improve the computational efficiency</p>

<h1 id="ch3-a-tour-of-machine-learning-classifiers-using-scikit-learn">
<a class="anchor" href="#ch3-a-tour-of-machine-learning-classifiers-using-scikit-learn" aria-hidden="true"><span class="octicon octicon-link"></span></a>Ch3. A Tour of Machine Learning Classifiers Using scikit-learn</h1>

<h1 id="ch4-building-good-training-datasets--data-preprocessing">
<a class="anchor" href="#ch4-building-good-training-datasets--data-preprocessing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Ch4. Building Good Training Datasets – Data Preprocessing</h1>

<h3 id="missing-data">
<a class="anchor" href="#missing-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Missing Data</h3>

<p>Unfortunately, most computational tools are unable to handle such missing values or will produce unpredictable results if we simply ignore them. Therefore, it is crucial that we take care of those missing values before we proceed with further analyses.</p>

<p>Nowadays, most scikit-learn functions support  DataFrame  objects as inputs, but since NumPy array handling is more mature in the scikit-learn API, it is recommended to use NumPy arrays when possible.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># only drop rows where NaN appear in specific columns (here: 'C') 
</span> <span class="o">&gt;&gt;&gt;</span>   <span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s">'C'</span><span class="p">])</span>
</code></pre></div></div>

<h3 id="categorical-features">
<a class="anchor" href="#categorical-features" aria-hidden="true"><span class="octicon octicon-link"></span></a>Categorical Features</h3>

<p>When we are talking about  categorical  data, we have to further distinguish between  ordinal  and  nominal  features. Unfortunately, there is no convenient function that can automatically derive the correct order of the labels of our  size  feature, so we have to define the mapping manually.</p>

<blockquote>
  <p>We can simply define a reverse-mapping dictionary,  inv_size_mapping = {v: k for k, v in size_mapping.items()}</p>
</blockquote>

<p>Most estimators for classification in scikit-learn convert class labels to integers internally, but it is considered good practice to provide class labels as integer arrays to avoid technical glitches.</p>

<p>Although the color values don’t come in any particular order, a learning algorithm will now assume that  green  is larger than  blue , and  red  is larger than  green . Although this assumption is incorrect, the algorithm could still produce useful results. However, those results would not be optimal.</p>

<p>A common workaround for this problem is to use a technique called <strong>one-hot encoding</strong>. The idea behind this approach is to create a new dummy feature for each unique value in the nominal feature column.</p>

<p>When we are using one-hot encoding datasets, we have to keep in mind that this introduces  multicollinearity, which can be an issue for certain methods (for instance, methods that require matrix inversion). If features are  highly correlated, matrices are computationally difficult to invert, which can lead to numerically unstable estimates. To reduce the correlation among variables, we can simply remove one feature column from the one-hot encoded array.</p>

<blockquote>
  <p>Providing the class label array  y  as an argument to  stratify  ensures that both training and test datasets have the same class proportions as the original dataset.</p>
</blockquote>

<p>Instead of discarding the allocated test data after model training and evaluation, it is a common practice to retrain a classifier on the entire dataset, as it can improve the predictive performance of the model.</p>

<h3 id="feature-scaling">
<a class="anchor" href="#feature-scaling" aria-hidden="true"><span class="octicon octicon-link"></span></a>Feature Scaling</h3>

<p><strong>Feature scaling</strong>  is a crucial step  in our preprocessing pipeline that can easily be forgotten.  Decision trees  and  random forests  are two of the very few machine learning  algorithms  where we don’t need to worry about feature scaling. Those algorithms are scale invariant. However, the majority of machine learning and optimization algorithms behave much better if features are on the same scale</p>

<p>There are two common approaches to bringing different features onto the same scale:  <strong>normalization</strong>  and  <strong>standardization</strong></p>

<p><strong>Standardization</strong> can be more practical for many machine learning algorithms, especially for optimization algorithms such as gradient descent. The reason is that many linear models initialize the weights to 0 or small random values close to 0. Using standardization, we center the feature columns at mean 0 with standard deviation 1 so that the feature columns have the same parameters as a standard normal distribution (zero mean and unit variance), which makes it easier to learn the weights.</p>

<p>Standardization maintains useful information about outliers and makes the algorithm less sensitive to them in contrast to min-max scaling, which scales the data to a limited range of values</p>

<blockquote>
  <p>We fit the  StandardScaler  class only once—on the training data—and use those parameters to transform the test dataset or any new data point</p>
</blockquote>

<p>The  <strong>RobustScaler</strong>  is especially helpful and recommended if we are working with small datasets that contain  many outliers. If the machine learning algorithm applied to this dataset is prone to  overfitting , the  RobustScaler  can be a good choice</p>

<p>Overfitting means the model fits the parameters too closely with regard to the particular observations in the training dataset, but does not generalize well to new data; we say that the  model has a  high variance . The reason for the overfitting is that our model is too complex for the given training data.</p>

<p>Common ways to reduce overfitting by regularization and dimensionality reduction via <strong>feature selection</strong>, which leads to simpler models by requiring fewer parameters to be fitted to the data.</p>

<h3 id="regularization">
<a class="anchor" href="#regularization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Regularization</h3>

<ul>
  <li>
    <p><strong>L1 regularization</strong> usually yields sparse feature vectors and most feature weights will be zero. Sparsity can be useful in practice if we have a high-dimensional dataset with many features that are irrelevant, especially in cases where we have more irrelevant dimensions than training examples.</p>
  </li>
  <li>
    <p><strong>L2 regularization</strong> adds a penalty term to the cost function that effectively results in less extreme weight values compared to a model trained with an unregularized cost function.</p>
  </li>
</ul>

<p>There are two main categories of dimensionality reduction  techniques:  <strong>feature selection</strong>  and  <strong>feature extraction</strong> . Via feature selection, we select a subset of the original features, whereas in feature extraction, we derive information from the feature set to construct a new feature subspace.</p>

<p><strong>Greedy algorithms</strong>  make  locally optimal choices at each stage of a combinatorial search problem and generally yield a suboptimal solution to the problem, in contrast to  exhaustive search algorithms , which  evaluate all possible combinations and are guaranteed to find the optimal  solution</p>

<p>By reducing the number of features, we shrank the size of the dataset, which can be useful in real-world applications that may involve expensive data collection steps. Also, by substantially reducing the number of features, we obtain simpler models, which are easier to interpret.</p>

<h3 id="random-forest-feature-importance">
<a class="anchor" href="#random-forest-feature-importance" aria-hidden="true"><span class="octicon octicon-link"></span></a>Random Forest feature importance</h3>

<p>Using a random forest, we can measure the feature importance as the averaged impurity decrease computed from all decision trees in the forest, without making any assumptions about whether our data is linearly separable or not.</p>

<blockquote>
  <p>As far as interpretability is concerned, the random forest technique comes with an important  gotcha  that is worth mentioning. <strong>If two or more features are highly correlated, one feature may be ranked very highly while the information on the other feature(s) may not be fully captured.</strong></p>
</blockquote>

<p><strong>SelectFromModel</strong>  object that selects features based on a user-specified threshold after model fitting, which is useful if we want to use the  RandomForestClassifier  as a feature selector and intermediate step in a scikit-learn  Pipeline  object,</p>

<h1 id="ch5-compressing-data-via-dimensionality-reduction">
<a class="anchor" href="#ch5-compressing-data-via-dimensionality-reduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Ch5. Compressing Data via Dimensionality Reduction</h1>

<h3 id="feature-extraction">
<a class="anchor" href="#feature-extraction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Feature Extraction</h3>
<p>Feature extraction can be understood as an approach to data compression with the goal of maintaining most of the relevant information.</p>

<blockquote>
  <p>Feature  extraction is not only used to improve storage space or the computational efficiency of the learning  algorithm, but can also improve the predictive performance by reducing the  curse of dimensionality —especially if we are working with non-regularized models.</p>
</blockquote>

<h3 id="pca">
<a class="anchor" href="#pca" aria-hidden="true"><span class="octicon octicon-link"></span></a>PCA</h3>

<p>PCA aims to find the directions of maximum variance in high-dimensional data and projects the data onto a new subspace with equal or fewer dimensions than the original one</p>

<p>Even if the  input features are correlated, the resulting principal components will be mutually orthogonal (uncorrelated).</p>

<blockquote>
  <p>PCA directions are highly sensitive to data scaling, and we need to standardize the features  prior  to PCA if the features were measured on different scales and we want to assign equal importance to all features</p>
</blockquote>

<h3 id="lda">
<a class="anchor" href="#lda" aria-hidden="true"><span class="octicon octicon-link"></span></a>LDA</h3>

<p>The general concept behind LDA is very similar to PCA, but whereas PCA attempts to find the orthogonal component axes of maximum variance in a dataset, the goal in LDA is to find the feature subspace that optimizes class separability</p>

<h3 id="kernel-pca">
<a class="anchor" href="#kernel-pca" aria-hidden="true"><span class="octicon octicon-link"></span></a>Kernel PCA</h3>

<p>If we are dealing with nonlinear problems, which we may encounter rather  frequently in real-world  applications, linear transformation techniques for dimensionality reduction, such as PCA and LDA, may not be the best choice.</p>

<blockquote>
  <p>Using the kernel trick, we can compute the similarity between two high-dimension feature vectors in the original feature space</p>
</blockquote>

<h1 id="ch6-learning-best-practices-for-model-evaluation-and-hyperparameter-tuning">
<a class="anchor" href="#ch6-learning-best-practices-for-model-evaluation-and-hyperparameter-tuning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Ch6. Learning Best Practices for Model Evaluation and Hyperparameter Tuning</h1>

<blockquote>
  <p>We have to reuse the parameters that were obtained during the fitting of the training data to scale and compress any new data, such as the examples in the separate test dataset</p>
</blockquote>

<h3 id="combining-transformers-and-estimators-in-a-pipeline">
<a class="anchor" href="#combining-transformers-and-estimators-in-a-pipeline" aria-hidden="true"><span class="octicon octicon-link"></span></a>Combining transformers and estimators in a pipeline</h3>

<p>There is no limit to the number of intermediate steps in a pipeline; however, the last pipeline element has to be an estimator.</p>

<p>If we reuse the same test dataset over and over again  during model selection, it will become part of our training data and thus the model will be more likely to overfit. <strong>Despite this issue, many people still use the test dataset for model selection, which is not a good machine learning practice</strong></p>

<h3 id="using-k-fold-cross-validation-to-assess-model-performance">
<a class="anchor" href="#using-k-fold-cross-validation-to-assess-model-performance" aria-hidden="true"><span class="octicon octicon-link"></span></a>Using k-fold cross-validation to assess model performance</h3>

<p>A better way of using the holdout method for model selection is to separate the data into three parts: a training dataset, a validation dataset, and a test dataset. The training dataset is used to fit the different models, and the performance on the validation dataset is then used for the model selection. The advantage of having a test dataset that the model hasn’t seen before during the training and model selection steps is that we can obtain a less biased estimate of its ability to generalize to new data.</p>

<p>A disadvantage of the holdout method is that the performance estimate may be very sensitive to how we partition the training dataset into the training and validation subsets; the estimate will vary for different examples of the data</p>

<h3 id="k-fold-cross-validation">
<a class="anchor" href="#k-fold-cross-validation" aria-hidden="true"><span class="octicon octicon-link"></span></a>K-fold cross-validation</h3>

<p>In k-fold cross-validation, we randomly split the training dataset into  k  folds without replacement, where  k  – 1 folds  are used for the model training, and one fold is used for performance evaluation. This procedure is repeated  k  times so that we obtain  k  models and performance estimates.</p>

<p>We use k-fold cross-validation for model tuning, that is, finding the optimal hyperparameter values that yield a satisfying generalization performance, which is estimated from evaluating the model performance on the test folds.</p>

<blockquote>
  <p>Once we have found satisfactory hyperparameter values, we can retrain the model on the complete training dataset and obtain a final performance estimate using the independent test  dataset. The rationale behind fitting a model to the whole training dataset after k-fold cross-validation is that providing more training examples to a learning algorithm usually results in a more accurate and robust model.</p>
</blockquote>

<p>Since k-fold cross-validation is a resampling technique without replacement, the advantage of this approach is that each example will be used for training and validation (as part of a test fold) exactly once, which yields a lower-variance estimate of the model performance than the holdout method.</p>

<h2 id="choosing-k">
<a class="anchor" href="#choosing-k" aria-hidden="true"><span class="octicon octicon-link"></span></a>Choosing K</h2>

<p>A good standard value for  k  in k-fold cross-validation is 10, as empirical evidence shows. For instance, experiments by Ron Kohavi on various real-world datasets suggest that 10-fold cross-validation  offers the best tradeoff between bias and variance ( A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection ,  Kohavi, Ron ,  International Joint Conference on Artificial Intelligence (IJCAI) , 14 (12): 1137-43,  1995 ).</p>

<p><strong>Small training sets</strong> -&gt; increase the number of folds. If we increase the value of  k , more training data will be used in each iteration, which results in a lower pessimistic bias toward estimating the generalization performance by averaging the individual model estimates.</p>

<p><strong>Large datasets</strong> -&gt; smaller value for  k , for example,  k  = 5, and still obtain an accurate estimate of the average performance of the model while reducing the computational cost of refitting and evaluating the model on the different folds</p>

<h3 id="stratified-cross-validation">
<a class="anchor" href="#stratified-cross-validation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Stratified cross-validation</h3>

<p>A slight improvement over the standard k-fold cross-validation approach is stratified k-fold cross-validation, which can yield better bias and variance estimates, especially in cases of unequal class proportions</p>

<p>By plotting the model training and validation accuracies as functions of the training dataset size, we can easily detect whether the model suffers from high variance or high bias, and whether the collection of more data could help to address this problem</p>

<h3 id="bias-x-variance">
<a class="anchor" href="#bias-x-variance" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bias x Variance</h3>

<p><strong>High bias</strong>: This model has both low  training and cross-validation accuracy, which indicates that it underfits the training data. Common ways to address this  issue are to increase the number of parameters of the model, for example, by collecting or constructing additional  features, or by decreasing the degree of regularization</p>

<p><strong>High variance</strong>: which is indicated by the large gap between the training and cross-validation accuracy. To address this problem of overfitting, we can collect more training data, reduce the complexity of the model, or increase the regularization parameter</p>

<p>For unregularized models, it  can also help to decrease  the number of features via feature selection</p>

<blockquote>
  <p>While collecting more training data usually tends to decrease the chance of overfitting, it  may not always help, for example, if the training data is extremely noisy or the model is already very close to optimal.</p>
</blockquote>

<h3 id="debugging-algorithms-with-learning-and-validation-curves">
<a class="anchor" href="#debugging-algorithms-with-learning-and-validation-curves" aria-hidden="true"><span class="octicon octicon-link"></span></a>Debugging algorithms with learning and validation curves</h3>

<p>Validation curves are a useful tool for improving the performance of a model by addressing  issues  such as overfitting or underfitting. Validation curves are related  to learning curves, but instead of plotting the training and test accuracies as functions of the sample size, we vary  the values of the model parameters</p>

<h3 id="fine-tuning-machine-learning-models">
<a class="anchor" href="#fine-tuning-machine-learning-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>Fine-tuning machine learning models</h3>

<p>The grid search approach is quite simple: it’s a brute-force exhaustive search paradigm where  we specify a list of values for different  hyperparameters, and the computer evaluates the model performance for each combination to obtain the optimal combination of values from this set</p>

<p>Randomized search usually performs about as well as grid search but is  much more cost- and time-effective. In particular, if we only sample 60 parameter combinations via randomized search, we already have a 95 percent probability of obtaining solutions within 5 percent of the optimal performance ( Random search for hyper-parameter optimization .  Bergstra J ,  Bengio Y .  Journal of Machine Learning Research . pp. 281-305, 2012).</p>

<h3 id="algorithm-selection-with-nested-cross-validation">
<a class="anchor" href="#algorithm-selection-with-nested-cross-validation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Algorithm selection with nested cross-validation</h3>

<p>If we want to select among different machine learning algorithms, though, another recommended approach is nested cross-validation. In a nice study on the bias in error estimation, Sudhir Varma and Richard Simon concluded that the true error of the estimate is almost unbiased relative to the test dataset when nested cross-validation is used ( Bias in Error Estimation When Using Cross-Validation for Model Selection ,  BMC Bioinformatics ,  S. Varma  and  R. Simon , 7(1): 91,  2006 ).</p>

<p>In nested cross-validation, we have an outer k-fold cross-validation loop to split the data into training and test folds, and an inner loop is used to select the model using k-fold cross-validation on the training fold. After model selection, the test fold is then used to evaluate the model performance</p>

<h3 id="looking-at-different-performance-evaluation-metrics">
<a class="anchor" href="#looking-at-different-performance-evaluation-metrics" aria-hidden="true"><span class="octicon octicon-link"></span></a>Looking at different performance evaluation metrics</h3>

<p>A confusion matrix is simply a  square matrix that reports the counts of the  true positive  ( TP ),  true negative  ( TN ),  false positive  ( FP ), and  false negative  ( FN ) predictions  of a classifier</p>

<p>The  true positive rate  ( TPR ) and  false positive rate  ( FPR ) are performance metrics that are especially  useful for  imbalanced  class problems</p>

<p>Receiver operating characteristic  ( ROC ) graphs are useful tools to select models for classification based  on their performance with respect to the FPR and TPR, which are computed by shifting the decision threshold of the classifier. The diagonal of a ROC graph can be interpreted as  random guessing , and classification models that fall below the diagonal are considered as worse than random guessing. A perfect classifier would fall into the top-left corner of the graph with a TPR of 1 and an  FPR of 0. Based on the ROC curve, we can then compute the so-called  ROC area under the curve  ( ROC AUC ) to characterize the performance of a classification model.</p>

<h4 id="scoring-metrics-for-multiclass-classification">
<a class="anchor" href="#scoring-metrics-for-multiclass-classification" aria-hidden="true"><span class="octicon octicon-link"></span></a>Scoring metrics for multiclass classification</h4>

<blockquote>
  <p>Micro-averaging is useful if we want to weight each instance or prediction equally, whereas macro-averaging weights all classes equally to evaluate the overall performance of a  classifier with regard to the most frequent class labels. If we are using binary performance metrics to evaluate multiclass classification models in scikit-learn, a normalized or weighted variant of the macro-average is used by default</p>
</blockquote>

<h3 id="dealing-with-class-imbalance">
<a class="anchor" href="#dealing-with-class-imbalance" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dealing with class imbalance</h3>

<p>Class imbalance is a quite common problem when working with real-world data—examples from one class or multiple classes are over-represented in a dataset</p>

<p>The algorithm implicitly learns a model that optimizes the predictions based on the most abundant class in the dataset, in order to minimize the cost or maximize the reward during training.</p>

<p>One way to deal with imbalanced class proportions during model fitting is to assign a larger penalty to wrong predictions on the minority class.</p>

<p>Other popular strategies for dealing with class imbalance include upsampling the minority class, downsampling the majority class, and the generation of synthetic training examples. Unfortunately, there’s no universally best solution or technique that works best across different problem domains. Thus, in practice, it is recommended to try out different strategies on  a given problem, evaluate the results, and choose the technique that seems most appropriate</p>

<h3 id="smote">
<a class="anchor" href="#smote" aria-hidden="true"><span class="octicon octicon-link"></span></a>SMOTE</h3>

<p>Synthetic Minority Over-sampling Technique  ( SMOTE ), and you can learn more about this  technique in the original research article by Nitesh Chawla and others:  SMOTE: Synthetic Minority Over-sampling Technique ,  Journal of Artificial Intelligence Research , 16: 321-357,  2002 . It is also highly recommended to check out  imbalanced-learn , a Python library that is entirely focused on imbalanced datasets, including an  implementation of SMOTE</p>

<h1 id="ch7-combining-different-models-for-ensemble-learning">
<a class="anchor" href="#ch7-combining-different-models-for-ensemble-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Ch7. Combining different models for Ensemble Learning</h1>

<p>The  goal of  ensemble methods  is to combine different classifiers into a meta-classifier that has better  generalization performance than each individual classifier alone.</p>

<h3 id="voting">
<a class="anchor" href="#voting" aria-hidden="true"><span class="octicon octicon-link"></span></a>Voting</h3>

<p>Majority  voting simply means that we select the class label that has been predicted by the majority of classifiers, that is, received more than 50 percent of the votes.</p>

<blockquote>
  <p>To predict a  class label via simple majority or plurality voting, we can combine the predicted class labels of each individual classifier,  , and select the class label,  , that received the most votes</p>
</blockquote>

<h3 id="stacking">
<a class="anchor" href="#stacking" aria-hidden="true"><span class="octicon octicon-link"></span></a>Stacking</h3>

<p>The  stacking algorithm can be understood as a two-level ensemble, where the first level consists of individual classifiers that feed their predictions to the second level, where another classifier (typically logistic regression) is fit to the level-one classifier predictions to make the final predictions. The stacking algorithm has been described in more detail by David H. Wolpert in  Stacked generalization ,  Neural Networks , 5(2):241–259,  1992 .</p>

<h3 id="bagging">
<a class="anchor" href="#bagging" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bagging</h3>

<p>Instead of using the same training dataset to fit the individual classifiers in the ensemble, we draw bootstrap samples (random samples with replacement) from the initial training dataset, which is why bagging is also known as  bootstrap aggregating .</p>

<p><strong>Random forests</strong> are a special case of bagging where we also use random feature subsets when fitting the individual decision trees.</p>

<blockquote>
  <p>Bagging was first  proposed by Leo Breiman in a  technical report in 1994; he also showed that bagging can improve the accuracy of unstable models and decrease the degree of overfitting.</p>
</blockquote>

<p>Bagging algorithm can be an effective approach to reducing the variance of a model. However, bagging is ineffective in reducing model bias, that is, models that are too simple to capture the trend in the data well. This is why we want to perform bagging on an ensemble of classifiers with low bias, for example, unpruned decision trees.</p>

<h3 id="boosting">
<a class="anchor" href="#boosting" aria-hidden="true"><span class="octicon octicon-link"></span></a>Boosting</h3>

<p>In boosting, the  ensemble consists of very simple base classifiers, also often referred to as  weak learners , which often only have a slight  performance advantage over random guessing—a typical example of a weak learner is a decision tree stump.</p>

<blockquote>
  <p>The key concept behind boosting is to focus on training examples that are hard to classify, that is, to let the weak learners subsequently learn from misclassified training examples to improve the performance of the ensemble.</p>
</blockquote>

<p>In contrast  to bagging, the initial formulation of the boosting algorithm uses random subsets of training examples drawn from the training dataset without replacement</p>

<p>Boosting can lead to a decrease in bias as well as variance compared to bagging models</p>

<p>Boosting algorithms such as AdaBoost are also known for their high variance, that is, the tendency to overfit the training data</p>

<blockquote>
  <p>It is worth noting that ensemble learning increases the computational complexity compared to individual classifiers. In practice, we need to think carefully about whether we want to pay the price of increased computational costs for an often relatively modest improvement in predictive performance. An often-cited  example of this tradeoff is the famous $1 million  Netflix Prize , which was won using ensemble techniques. The details about  the algorithm were published in  The BigChaos Solution to the Netflix Grand Prize  by  A. Toescher ,  M. Jahrer , and  R. M. Bell ,  Netflix Prize documentation ,  2009</p>
</blockquote>

<h3 id="gradient-boosting">
<a class="anchor" href="#gradient-boosting" aria-hidden="true"><span class="octicon octicon-link"></span></a>Gradient Boosting</h3>

<p>Another popular variant of boosting is  gradient boosting . AdaBoost and gradient boosting share the  main overall concept: boosting weak learners (such as decision tree stumps) to strong learners. The two approaches, adaptive and gradient boosting, differ mainly with regard to how the weights are updated and how the (weak) classifiers are combined</p>

<h4 id="xgboost">
<a class="anchor" href="#xgboost" aria-hidden="true"><span class="octicon octicon-link"></span></a>XGBoost</h4>

<p>XGBoost, which is essentially a computationally efficient implementation of the original gradient boost algorithm ( XGBoost: A scalable tree boosting system .  Tianqi Chen  and  Carlos Guestrin .  Proceeding of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining .  ACM 2016 , pp. 785-794)</p>

<h4 id="histgradientboosting">
<a class="anchor" href="#histgradientboosting" aria-hidden="true"><span class="octicon octicon-link"></span></a>HistGradientBoosting</h4>

<p>Scikit-learn now also includes a substantially faster version of gradient boosting in version 0.21,  HistGradientBoostingClassifier , which is even faster than XGBoost</p>

<blockquote>
  <p>Ensemble methods combine different classification models to cancel out their individual weaknesses, which often results in stable and well-performing models that are very attractive for industrial applications as well as machine learning competitions.</p>
</blockquote>

<h1 id="ch8-applying-machine-learning-to-sentiment-analysis">
<a class="anchor" href="#ch8-applying-machine-learning-to-sentiment-analysis" aria-hidden="true"><span class="octicon octicon-link"></span></a>Ch8. Applying Machine Learning to Sentiment Analysis</h1>

<p>Sentiment analysis, sometimes  also  called  opinion mining , is a popular subdiscipline of the broader field of NLP; it is concerned with analyzing the polarity of documents. A popular task in sentiment analysis is the classification of documents based on the expressed opinions or emotions of the authors with regard to a particular topic.</p>

<blockquote>
  <p>To visualize the progress and estimated time until completion, use the  Python Progress Indicator  ( <strong>PyPrind</strong> ,  https://pypi.python.org/pypi/PyPrind/ )</p>
</blockquote>

<h3 id="bag-of-words">
<a class="anchor" href="#bag-of-words" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bag-of-words</h3>

<p>The idea behind bag-of-words is quite simple and can be summarized as follows: We create a vocabulary of unique tokens—for example, words—from the entire set of documents. We construct a feature vector from each document that contains the counts of how often each word occurs in the particular document.</p>

<blockquote>
  <p>To construct  a bag-of-words model based on the word  counts in the respective documents, we can use the  CountVectorizer  class implemented in scikit-learn</p>
</blockquote>

<p>Values in the feature vectors are also  called the  raw term frequencies :  tf ( t ,  d )—the number of times a term,  t , occurs in a document,  d . It should be noted that, in the bag-of-words model, the word or term order in a sentence or document does not matter. The order in which the term frequencies appear in the feature vector is derived from the vocabulary indices, which are usually assigned alphabetically.</p>

<h4 id="n-grams">
<a class="anchor" href="#n-grams" aria-hidden="true"><span class="octicon octicon-link"></span></a>N-grams</h4>

<p>The sequence of items  in the bag-of-words model that we just created is also  called the  1-gram  or  unigram  model—each item or token in the vocabulary represents a single word. More generally, contiguous sequences of items in NLP—words, letters, or  symbols—are also called  n-grams</p>

<h4 id="tf-idf">
<a class="anchor" href="#tf-idf" aria-hidden="true"><span class="octicon octicon-link"></span></a>TF-IDF</h4>

<p>Frequently occurring words typically don’t contain useful or discriminatory information</p>

<p>Term frequency-inverse document frequency  ( tf-idf ), which can be used to downweight  these frequently occurring words in the feature vectors. The tf-idf can be defined as the product of the term frequency and the inverse document frequency</p>

<h3 id="cleaning-text">
<a class="anchor" href="#cleaning-text" aria-hidden="true"><span class="octicon octicon-link"></span></a>Cleaning text</h3>

<p>The first important step—before we build our bag-of-words model—is to clean the text data by stripping it of all unwanted characters.</p>

<p>One way to  tokenize  documents is to split them into individual words by splitting the cleaned documents at their whitespace characters</p>

<h4 id="stemming">
<a class="anchor" href="#stemming" aria-hidden="true"><span class="octicon octicon-link"></span></a>Stemming</h4>

<p>In the context of tokenization, another  useful technique is  word stemming , which is the process of transforming a word into its root form. It allows us to map related words to the same stem</p>

<p>The Porter stemming algorithm  is probably the oldest and simplest stemming algorithm. Other popular stemming algorithms include the  newer  Snowball stemmer  (Porter2 or English stemmer) and the  Lancaster stemmer  (Paice/Husk stemmer)</p>

<h4 id="lemmatization">
<a class="anchor" href="#lemmatization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Lemmatization</h4>

<p>While stemming can create non-real words, such as  ‘thu’  (from  ‘thus’ ), as shown in the previous example, a technique called  lemmatization  aims to obtain  the canonical (grammatically correct) forms of individual words—the so-called  lemmas .</p>

<blockquote>
  <p>Lemmatization  is computationally more difficult and expensive compared to stemming and, in practice, it has been observed that stemming and lemmatization have little impact on the performance of text classification</p>
</blockquote>

<h3 id="stop-word-removal">
<a class="anchor" href="#stop-word-removal" aria-hidden="true"><span class="octicon octicon-link"></span></a>Stop-word removal</h3>

<p>Stop-words  are simply those words that are extremely common in all sorts of texts and probably bear no (or only a little) useful information that can be used to distinguish between different classes of documents. Examples of stop-words are  is ,  and ,  has , and  like . Removing stop-words can be useful if we are working with raw or normalized term frequencies rather than tf-idfs, which are already downweighting frequently occurring words.</p>

<p>TfidfVectorizer , which combines  CountVectorizer  with the  TfidfTransformer</p>

<h3 id="naive-bayes-classifier">
<a class="anchor" href="#naive-bayes-classifier" aria-hidden="true"><span class="octicon octicon-link"></span></a>Naive Bayes Classifier</h3>

<p>A still very popular classifier for text classification is the naïve Bayes classifier, which  gained popularity in applications of email spam filtering. Naïve Bayes classifiers are easy to implement, computationally efficient, and tend to perform particularly well on relatively small datasets</p>

<h3 id="out-of-core-learning">
<a class="anchor" href="#out-of-core-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Out-of-core learning</h3>

<p>Out-of-core learning allows us to work with large datasets by fitting the classifier incrementally on smaller batches of a dataset.</p>

<p>Unfortunately, we  can’t use  CountVectorizer  for out-of-core learning since it requires holding the complete vocabulary in memory. Also,  TfidfVectorizer  needs to keep all the feature vectors of the training dataset in memory to calculate the inverse document frequencies. However, another useful vectorizer for text processing implemented in scikit-learn is  HashingVectorizer .  HashingVectorizer  is data-independent</p>

<p>Out-of-core learning is very memory efficient</p>

<h3 id="word2vec">
<a class="anchor" href="#word2vec" aria-hidden="true"><span class="octicon octicon-link"></span></a>word2vec</h3>

<p>A more  modern alternative to the bag-of-words model is  word2vec , an algorithm that Google released in 2013 ( Efficient Estimation of Word Representations in Vector Space ,  T. Mikolov ,  K. Chen ,  G. Corrado , and  J. Dean , arXiv preprint arXiv:1301.3781,  2013 ). The word2vec algorithm is an unsupervised learning algorithm based on neural networks that attempts to automatically learn the relationship between words. The idea behind word2vec is to put words that have similar meanings into similar clusters, and via clever vector-spacing, the model can reproduce certain words using simple vector math, for example,  king  –  man  +  woman  =  queen .</p>

<h3 id="topic-modeling">
<a class="anchor" href="#topic-modeling" aria-hidden="true"><span class="octicon octicon-link"></span></a>Topic Modeling</h3>

<p>Topic modeling  describes  the broad task of assigning topics to unlabeled text documents. For example, a typical application would be the categorization of documents in a large text corpus of newspaper articles. In applications of topic modeling, we then aim to  assign category labels to those articles, for example, sports, finance, world news, politics, local news, and so forth</p>

<h3 id="latent-dirichlet-allocation---lda-">
<a class="anchor" href="#latent-dirichlet-allocation---lda-" aria-hidden="true"><span class="octicon octicon-link"></span></a>Latent Dirichlet Allocation  ( LDA )</h3>

<p>LDA is a generative probabilistic model that tries to find groups of words that appear frequently together across different documents</p>

<p>We must define the number of topics beforehand—the number of topics is a hyperparameter of LDA that has to be specified manually.</p>

<blockquote>
  <p>The scikit-learn library’s implementation of LDA uses the  <em>expectation-maximization</em>  ( EM ) algorithm to  update its parameter estimates iteratively</p>
</blockquote>

<h1 id="ch9-embedding-a-machine-learning-model-into-a-web-application">
<a class="anchor" href="#ch9-embedding-a-machine-learning-model-into-a-web-application" aria-hidden="true"><span class="octicon octicon-link"></span></a>Ch9. Embedding a Machine Learning Model into a Web Application</h1>

<h3 id="serializing-fitted-scikit-learn-estimators">
<a class="anchor" href="#serializing-fitted-scikit-learn-estimators" aria-hidden="true"><span class="octicon octicon-link"></span></a>Serializing fitted scikit-learn estimators</h3>
<ul>
  <li>One option for model persistence: Python’s in-built <code class="highlighter-rouge">pickle</code> module</li>
  <li>
<code class="highlighter-rouge">protocol=4</code> to choose the latest and most efficient pickle protocol</li>
  <li>
<code class="highlighter-rouge">joblib</code>: lib, more efficient way to serialize NumPy arrays</li>
</ul>

<blockquote>
  <p><strong>Pickle can be a security risk</strong>: not secured against malicious code. Pickle was designed to serialize arbitraty objects, the unpickling process will execute code that has been stored in a pickle file</p>
</blockquote>

<h3 id="sqlite">
<a class="anchor" href="#sqlite" aria-hidden="true"><span class="octicon octicon-link"></span></a>SQLite</h3>
<p>SQLite database can be understood as a single, self-contained database file that allows us to directly access storage files</p>

<blockquote>
  <p>free DB browser for SQLite app (https://sqlitebrowser.org/dl/) -&gt; nice GUI for working with SQLite databases</p>
</blockquote>

<h3 id="flask">
<a class="anchor" href="#flask" aria-hidden="true"><span class="octicon octicon-link"></span></a>Flask</h3>
<p>Written in Python, provides a convenient interface for embedding existing Python code</p>

<h3 id="jinja2">
<a class="anchor" href="#jinja2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Jinja2</h3>
<p>Web templates</p>

<h3 id="pythonanywhere">
<a class="anchor" href="#pythonanywhere" aria-hidden="true"><span class="octicon octicon-link"></span></a>PythonAnywhere</h3>
<p>Lets us run a single web application free of charge</p>

<h1 id="ch10-predicting-continuous-target-variables-with-regression-analysis">
<a class="anchor" href="#ch10-predicting-continuous-target-variables-with-regression-analysis" aria-hidden="true"><span class="octicon octicon-link"></span></a>Ch10. Predicting Continuous Target Variables with Regression Analysis</h1>

<h3 id="linear-regression">
<a class="anchor" href="#linear-regression" aria-hidden="true"><span class="octicon octicon-link"></span></a>Linear Regression</h3>
<p><strong>Regression Line</strong>: best-fitting line</p>

<p><strong>Offsets/Residuals</strong>: vertical lines from the regression line to the training examples -&gt; errors of our prediction</p>

<h3 id="visualizing-the-important-characteristics-of-a-dataset">
<a class="anchor" href="#visualizing-the-important-characteristics-of-a-dataset" aria-hidden="true"><span class="octicon octicon-link"></span></a>Visualizing the important characteristics of a dataset</h3>
<ul>
  <li>
<strong>Scatterplot matrix</strong>: pair-wise correlations between the different features -&gt; <em>scatterplotmatrix</em> on MLxtend (https://github.com/rasbt/mlxtend)</li>
</ul>

<blockquote>
  <p>Training a linear regression model does <strong>not</strong> require that the explanatory or target variables are normally distributed -&gt; only requirement for certain statistics and hypothesis tests</p>
</blockquote>

<h3 id="looking-at-relationships-using-a-correlation-matrix">
<a class="anchor" href="#looking-at-relationships-using-a-correlation-matrix" aria-hidden="true"><span class="octicon octicon-link"></span></a>Looking at relationships using a correlation matrix</h3>
<ul>
  <li>
<strong>Correlation matrix</strong>: square matrix that contains the <em>Pearson product-moment correlation coefficient</em> (<em>Pearson’s r</em>) -&gt; linear dependence between pairs of features</li>
  <li>Correlation coefficients are in range -1 to 1. 1 -&gt; perfect positive correlation, 0 -&gt; no correlation and -1: perfect negative correlation</li>
</ul>

<blockquote>
  <p>To fit a linear regression model, we are interested in those features that have a high correlation with our target variable</p>
</blockquote>

<h3 id="estimating-the-coefficient-of-a-regression-model-via-scikit-learn">
<a class="anchor" href="#estimating-the-coefficient-of-a-regression-model-via-scikit-learn" aria-hidden="true"><span class="octicon octicon-link"></span></a>Estimating the coefficient of a regression model via scikit-learn</h3>
<p>The linear regression implementation in scikit-learn works better with unstandardized variables</p>

<h3 id="fitting-a-robust-regression-model-using-ransac">
<a class="anchor" href="#fitting-a-robust-regression-model-using-ransac" aria-hidden="true"><span class="octicon octicon-link"></span></a>Fitting a robust regression model using RANSAC</h3>
<ul>
  <li>
    <p>Linear regression models can be heavily impacted by the presence of outliers. In certain situations, a very small subset of our data can have a big effect on the estimated model coefficients</p>
  </li>
  <li>
    <p>As an alternative to throwing out outliers, we will look at a robust method of regression using the <strong>RANdom SAmple Consensus (RANSAC)</strong> algorithm, which fits a regression model to a subset of the data, the so-called <strong>inliers</strong></p>
  </li>
</ul>

<h3 id="evaluating-the-performance-of-linear-regression-models">
<a class="anchor" href="#evaluating-the-performance-of-linear-regression-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>Evaluating the performance of linear regression models</h3>
<ul>
  <li>Plot the residuals (the differences or vertical distances between the actual and predicted values) versus the predicted values to diagnose our regression model.</li>
  <li>
<strong>Residual plots</strong> are a commonly used graphical tool for diagnosing regression models. They can help to detect nonlinearity and outliers, and check whether the errors are randomly distributed</li>
</ul>

<blockquote>
  <p>Good regression model: errors randomly distributed and the residuals randomly scattered around the centerline</p>
</blockquote>

<ul>
  <li>
    <p><strong>MSE</strong>: useful for comparing differente regression models or for tuning their parameters via grid search and cross-validation</p>
  </li>
  <li>
    <p><strong>Rˆ2</strong>: coefficient of determination. Standardized version of the MSE -&gt; better interpretability of the model’s performance. Rˆ2 is the fraction of response variance that is captured by the model</p>
  </li>
</ul>

<h3 id="using-regularized-methods-for-regression">
<a class="anchor" href="#using-regularized-methods-for-regression" aria-hidden="true"><span class="octicon octicon-link"></span></a>Using regularized methods for regression</h3>
<ul>
  <li>
    <p>Regularization is one approach to tackling the problem of overfitting by adding additional information, and thereby shrinking the parameter values of the model to induce a penalty against complexity.</p>
  </li>
  <li>
    <p>The most popular approaches to regularized linear regression are the so-called <strong>Ridge Regression, least absolute shrinkage and selection operator (LASSO), and elastic Net</strong></p>
  </li>
</ul>

<blockquote>
  <p>Saturation of a model occurs if the number of training examples is equal to the number of features, which is a form of overparameterization. As a consequence, a saturated model can always fit the training data perfectly but is merely a form of interpolation and thus is not expected to generalize well</p>
</blockquote>

<h3 id="dealing-with-nonlinear-relationships-using-random-forests">
<a class="anchor" href="#dealing-with-nonlinear-relationships-using-random-forests" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dealing with nonlinear relationships using random forests</h3>
<p>In the context of decision tree regression, the MSE is often referred to as <strong>within-node variance</strong>, which is why the splitting criterion is also better known as <strong>variance reduction</strong></p>

<blockquote>
  <p>If the distribution of the residuals does not seem to be completely random around the zero center point -&gt; the model was not able to capture all the exploratory information</p>
</blockquote>

<ul>
  <li>
    <p>The error of the predictions should not be related to any of the information contained in the explanatory variables; rather, it should reflect the randomness of the real-world distributions or patterns. If we find patterns in the prediction errors, for example, by inspecting the residual plot, it means that the residual plots contain predictive information</p>
  </li>
  <li>
    <p>Improve the model by transforming variables, tuning the hyperparameters of the learning algorithm, choosing simpler or more complex models, removing outliers, or including additional variables</p>
  </li>
</ul>

<h1 id="ch11-working-with-unlabeled-data---clustering-analysis">
<a class="anchor" href="#ch11-working-with-unlabeled-data---clustering-analysis" aria-hidden="true"><span class="octicon octicon-link"></span></a>Ch11. Working with Unlabeled Data - Clustering Analysis</h1>

<p><strong>Clustering</strong>: find natural grouping in data -&gt; items in the same cluster are more similar to each other than to those from different clusters</p>

<h3 id="grouping-objects-by-similarity-using-k-means">
<a class="anchor" href="#grouping-objects-by-similarity-using-k-means" aria-hidden="true"><span class="octicon octicon-link"></span></a>Grouping objects by similarity using k-means</h3>
<p><strong>k-means</strong></p>
<ul>
  <li>belongs to the category of <em>prototype-based clustering</em> -&gt; each cluster is represented by a prototype (<em>centroid</em> for average or <em>medoid</em> for most representative)</li>
  <li>good at identifying clustrs with a spherical shape</li>
  <li>drawback -&gt; have to specify number of clusters, <code class="highlighter-rouge">k</code>, <em>a priori</em>
</li>
  <li>Make sure that the features are measured on the same scale -&gt; apply z-score standardization or min-max scaling</li>
  <li>clusters do not overlap and are not hierarchical</li>
  <li>assume that there is at least one item in each cluster</li>
</ul>

<p><strong>k-means++</strong></p>
<ul>
  <li>smarter way of placing the inital cluster centroids</li>
</ul>

<h4 id="hard-versus-soft-clustering">
<a class="anchor" href="#hard-versus-soft-clustering" aria-hidden="true"><span class="octicon octicon-link"></span></a>Hard versus soft clustering</h4>
<p><strong>Hard clustering</strong>: each example assigned to exactly one cluster. (e.g., k-means)</p>

<p><strong>Soft clustering (fuzzy clustering)</strong>: assign an example to one or more clusters (e.g., fuzzy C-means = FCM = soft k-means = fuzzy k-means)</p>

<blockquote>
  <p>Both k-means and FCM produce very similar clustering outputs</p>
</blockquote>

<h4 id="using-the-elbow-method-to-find-the-optimal-number-of-clusters">
<a class="anchor" href="#using-the-elbow-method-to-find-the-optimal-number-of-clusters" aria-hidden="true"><span class="octicon octicon-link"></span></a>Using the elbow method to find the optimal number of clusters</h4>
<ul>
  <li>withing-cluster SSE (distortion) -&gt; <em>inertia_</em> attribute after fitting KMeans using scikit-learn</li>
  <li>
<strong>elbow method</strong>: identify the value of <em>k</em> where the distortion begins to increase most rapidly</li>
</ul>

<h4 id="quantifying-the-quality-of-clustering-via-silhouette-plots">
<a class="anchor" href="#quantifying-the-quality-of-clustering-via-silhouette-plots" aria-hidden="true"><span class="octicon octicon-link"></span></a>Quantifying the quality of clustering via silhouette plots</h4>
<ul>
  <li>
<strong>silhouette coefficient</strong>: range -1 to 1. 0 if the cluster separation and cohesion are equal. 1 (ideal) if how dissimilar an example is from other cluster » how similar it is to the other examples in its own cluster</li>
  <li>silhouette plot with visibly different lengths and widths -&gt; evidence of bad or at least suboptimal clustering</li>
</ul>

<h3 id="organizing-clusters-as-a-hierarchical-tree">
<a class="anchor" href="#organizing-clusters-as-a-hierarchical-tree" aria-hidden="true"><span class="octicon octicon-link"></span></a>Organizing clusters as a hierarchical tree</h3>
<ul>
  <li>allows us to plot <strong>dendograms</strong>
</li>
  <li>do not need to specify the number of clusters upfront</li>
  <li>can be <strong>agglomerative</strong> (starts with every point as a cluster) or <strong>divisive</strong> (starts with one cluster and split iteratively)</li>
</ul>

<h4 id="grouping-clusters-in-a-bottom-up-fashion">
<a class="anchor" href="#grouping-clusters-in-a-bottom-up-fashion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Grouping clusters in a bottom-up fashion</h4>
<p>Algorithms for agglomerative hierarchical clustering:</p>
<ul>
  <li>single linkage</li>
  <li>complete linkage</li>
  <li>average linkage</li>
  <li>Ward’s linkage</li>
</ul>

<blockquote>
  <p>Dendograms are often used in combination with a <strong>heat map</strong> -&gt; represent individual values in the data array or matrix containing our training examples with a color code</p>
</blockquote>

<h3 id="locating-regions-of-high-density-via-dbscan">
<a class="anchor" href="#locating-regions-of-high-density-via-dbscan" aria-hidden="true"><span class="octicon octicon-link"></span></a>Locating regions of high density via DBSCAN</h3>
<p><strong>DBSCAN</strong> -&gt; <em>density-based spatial clustering of applications with noise</em></p>
<ul>
  <li>no assumptions about spherical clusters</li>
  <li>no partition of the data into hierarchies that require a manual cut-off point</li>
  <li>assigns cluster labels based on dense regions of points -&gt; density: number of points within a specified radius</li>
  <li>doesn’t necessarily assign each point to a cluster -&gt; is capable of removing noise points</li>
  <li>two hyperparameters to be optimized to yield good results -&gt; MinPts and eta</li>
</ul>

<blockquote>
  <p>Disadvantage of the 3 algorithms presented: increasing number of features assuming fixed number of training examples -&gt; <strong>curse of dimensionality</strong> increases</p>
</blockquote>

<p>It is common practice to apply dimensionality reduction techniques prior to performing clustering -&gt; PCA or Kernel-PCA</p>

<p>Also common to compress data down to two-dimensional subspaces -&gt; visualization helps evaluating the results</p>

<blockquote>
  <p><strong>Graph-based clustering</strong>: not covered in the book. e.g, <strong>spectral clustering</strong></p>
</blockquote>

<h1 id="ch12-implementing-a-multilayer-artificial-neural-network-from-scratch">
<a class="anchor" href="#ch12-implementing-a-multilayer-artificial-neural-network-from-scratch" aria-hidden="true"><span class="octicon octicon-link"></span></a>Ch12. Implementing a Multilayer Artificial Neural Network from Scratch</h1>

<blockquote>
  <p>Single-layer naming convention: Adaline consists of two layers, one input, and one output. It is called single-layer network because of its single link between the input and output layers</p>
</blockquote>

<h2 id="introducing-the-multilayer-neural-network-architecture">
<a class="anchor" href="#introducing-the-multilayer-neural-network-architecture" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introducing the multilayer neural network architecture</h2>
<p>Adding additional hidden layers: the error gradients, become increasingly small as more layers are added to a network -&gt; <em>vanishing gradient problem</em></p>

<h2 id="activating-a-neural-network-via-forward-propagation">
<a class="anchor" href="#activating-a-neural-network-via-forward-propagation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Activating a neural network via forward propagation</h2>
<ol>
  <li>Forward propagate the patterns of training data to generate an output</li>
  <li>Calculate the error to minimize using a cost function between outputs and targets</li>
  <li>Backpropagate the error, find its derivative wrt each weight in the network, update the model</li>
  <li>Multiple epochs of 1-3, then forward propagation to calculate the output and apply a threshold function to obtain the predicted class labels</li>
</ol>

<blockquote>
  <p>MLP: typical example of a <strong>feedforward</strong> ANN -&gt; each layer serves as the input to the next layer without loops</p>
</blockquote>

<p>Gradient-based optimization is much more stable under normalized inputs (ranging from -1 to 1). Also, <em>Batch Normalization</em> for improving convergence</p>

<blockquote>
  <p>Efficient method of save multidimensional NumPy arrays to disk -&gt; NumPy’s <code class="highlighter-rouge">savez</code>/<code class="highlighter-rouge">savez_compressed</code> function -&gt; analogous to <code class="highlighter-rouge">pickle</code>, but optimized for np arrays. To load: <code class="highlighter-rouge">np.load(file.npz)</code></p>
</blockquote>

<p>Training (deep) NN is expensive -&gt; stop it early in certain conditions, e.g., starts overfitting, not improving</p>

<p>Common tricks to improve performance:</p>
<ul>
  <li>Skip-connections</li>
  <li>Learning rate schedulers</li>
  <li>Attaching loss functions to earlier layers</li>
</ul>

<h2 id="developing-your-understanding-of-backpropagation">
<a class="anchor" href="#developing-your-understanding-of-backpropagation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Developing your understanding of backpropagation</h2>
<p>Very computationally efficient approach to compute the partial derivatives of a complex cost function in multilayer NNs -&gt; Goal: use those derivatives to learn the weight coefficients for parametrizing such a multilayer artificial NN.</p>

<p>Backpropagation is a special case of a reverse-mode <strong>automatic differentiation</strong>. Matrix-vector multiplication is computationally much cheaper than matrix-matrix multiplication</p>

<h2 id="about-the-convergence-in-neural-networks">
<a class="anchor" href="#about-the-convergence-in-neural-networks" aria-hidden="true"><span class="octicon octicon-link"></span></a>About the convergence in neural networks</h2>
<p>The output function has a rough surface and the optimization algorithm can easily become trapped in local minima</p>

<p>By increasing the learning rate, we can more readily escape such local minima. But, we cal also increase the chance of over-shooting the global optimum if the learning rate is too large</p>

<h1 id="ch13-parallelizing-neural-network-training-with-tensorflow">
<a class="anchor" href="#ch13-parallelizing-neural-network-training-with-tensorflow" aria-hidden="true"><span class="octicon octicon-link"></span></a>Ch13. Parallelizing Neural Network Training with TensorFlow</h1>

<h1 id="ch14-going-deeper---the-mechanics-of-tensorflow">
<a class="anchor" href="#ch14-going-deeper---the-mechanics-of-tensorflow" aria-hidden="true"><span class="octicon octicon-link"></span></a>Ch14. Going Deeper - The Mechanics of TensorFlow</h1>

<h1 id="ch15-classifying-images-with-deep-convolutional-neural-networks">
<a class="anchor" href="#ch15-classifying-images-with-deep-convolutional-neural-networks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Ch15. Classifying Images with Deep Convolutional Neural Networks</h1>

<h1 id="ch16-modeling-sequential-data-using-recurrent-neural-networks">
<a class="anchor" href="#ch16-modeling-sequential-data-using-recurrent-neural-networks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Ch16. Modeling Sequential Data Using Recurrent Neural Networks</h1>

<h1 id="ch17-generative-adversarial-networks-for-synthesizing-new-data">
<a class="anchor" href="#ch17-generative-adversarial-networks-for-synthesizing-new-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Ch17. Generative Adversarial Networks for Synthesizing New Data</h1>

<h1 id="ch18-reinforcement-learning-for-decision-making-in-complex-environments">
<a class="anchor" href="#ch18-reinforcement-learning-for-decision-making-in-complex-environments" aria-hidden="true"><span class="octicon octicon-link"></span></a>Ch18. Reinforcement Learning for Decision Making in Complex Environments</h1>

  </div><a class="u-url" href="/blog/book/machine%20learning/data%20science/2019/12/26/python-ml.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Data Science and Machine Learning blog.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/millengustavo" title="millengustavo"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/millengustavo" title="millengustavo"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
