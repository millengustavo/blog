<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Statistical forecasting: notes on regression and time series analysis | Gustavo Millen</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Statistical forecasting: notes on regression and time series analysis" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="My notes and highlights on the book." />
<meta property="og:description" content="My notes and highlights on the book." />
<link rel="canonical" href="https://millengustavo.github.io/blog/book/time%20series/forecasting/data%20science/machine%20learning/statistics/2020/02/03/statistical-forecasting.html" />
<meta property="og:url" content="https://millengustavo.github.io/blog/book/time%20series/forecasting/data%20science/machine%20learning/statistics/2020/02/03/statistical-forecasting.html" />
<meta property="og:site_name" content="Gustavo Millen" />
<meta property="og:image" content="https://millengustavo.github.io/blog/images/statistical_forecasting/statistical_forecasting.gif" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-02-03T00:00:00-06:00" />
<script type="application/ld+json">
{"image":"https://millengustavo.github.io/blog/images/statistical_forecasting/statistical_forecasting.gif","description":"My notes and highlights on the book.","@type":"BlogPosting","url":"https://millengustavo.github.io/blog/book/time%20series/forecasting/data%20science/machine%20learning/statistics/2020/02/03/statistical-forecasting.html","headline":"Statistical forecasting: notes on regression and time series analysis","dateModified":"2020-02-03T00:00:00-06:00","datePublished":"2020-02-03T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://millengustavo.github.io/blog/book/time%20series/forecasting/data%20science/machine%20learning/statistics/2020/02/03/statistical-forecasting.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://millengustavo.github.io/blog/feed.xml" title="Gustavo Millen" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-192049344-2','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Statistical forecasting: notes on regression and time series analysis | Gustavo Millen</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Statistical forecasting: notes on regression and time series analysis" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="My notes and highlights on the book." />
<meta property="og:description" content="My notes and highlights on the book." />
<link rel="canonical" href="https://millengustavo.github.io/blog/book/time%20series/forecasting/data%20science/machine%20learning/statistics/2020/02/03/statistical-forecasting.html" />
<meta property="og:url" content="https://millengustavo.github.io/blog/book/time%20series/forecasting/data%20science/machine%20learning/statistics/2020/02/03/statistical-forecasting.html" />
<meta property="og:site_name" content="Gustavo Millen" />
<meta property="og:image" content="https://millengustavo.github.io/blog/images/statistical_forecasting/statistical_forecasting.gif" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-02-03T00:00:00-06:00" />
<script type="application/ld+json">
{"image":"https://millengustavo.github.io/blog/images/statistical_forecasting/statistical_forecasting.gif","description":"My notes and highlights on the book.","@type":"BlogPosting","url":"https://millengustavo.github.io/blog/book/time%20series/forecasting/data%20science/machine%20learning/statistics/2020/02/03/statistical-forecasting.html","headline":"Statistical forecasting: notes on regression and time series analysis","dateModified":"2020-02-03T00:00:00-06:00","datePublished":"2020-02-03T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://millengustavo.github.io/blog/book/time%20series/forecasting/data%20science/machine%20learning/statistics/2020/02/03/statistical-forecasting.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://millengustavo.github.io/blog/feed.xml" title="Gustavo Millen" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-192049344-2','auto');ga('require','displayfeatures');ga('send','pageview');</script>



<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Gustavo Millen</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Statistical forecasting: notes on regression and time series analysis</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-02-03T00:00:00-06:00" itemprop="datePublished">
        Feb 3, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      54 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#book">book</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#time series">time series</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#forecasting">forecasting</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#data science">data science</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#machine learning">machine learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#statistics">statistics</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#1-get-to-know-your-data">1. Get to know your data</a>
<ul>
<li class="toc-entry toc-h2"><a href="#principles-and-risks-of-forecasting">Principles and risks of forecasting</a>
<ul>
<li class="toc-entry toc-h3"><a href="#signal-vs-noise">Signal vs. noise</a></li>
<li class="toc-entry toc-h3"><a href="#risks-of-forecasting">Risks of forecasting</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#get-to-know-your-data">Get to know your data</a>
<ul>
<li class="toc-entry toc-h3"><a href="#plot-the-data">PLOT THE DATA!</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#inflation-adjustment-deflation">Inflation adjustment (“deflation”)</a></li>
<li class="toc-entry toc-h2"><a href="#seasonal-adjustment">Seasonal adjustment</a>
<ul>
<li class="toc-entry toc-h3"><a href="#multiplicative-adjustment">Multiplicative adjustment</a></li>
<li class="toc-entry toc-h3"><a href="#additive-adjustment">Additive adjustment</a></li>
<li class="toc-entry toc-h3"><a href="#acronyms">Acronyms</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#stationarity-and-differencing">Stationarity and differencing</a>
<ul>
<li class="toc-entry toc-h3"><a href="#statistical-stationarity">Statistical stationarity</a></li>
<li class="toc-entry toc-h3"><a href="#first-difference">First-difference</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#the-logarithm-transformation">The logarithm transformation</a>
<ul>
<li class="toc-entry toc-h3"><a href="#change-in-natural-log--percentage-change">Change in natural log ≈ percentage change</a></li>
<li class="toc-entry toc-h3"><a href="#linearization-of-exponential-growth-and-inflation">Linearization of exponential growth and inflation</a></li>
<li class="toc-entry toc-h3"><a href="#trend-measured-in-natural-log-units--percentage-growth">Trend measured in natural-log units ≈ percentage growth</a></li>
<li class="toc-entry toc-h3"><a href="#errors-measured-in-natural-log-units--percentage-errors">Errors measured in natural-log units ≈ percentage errors</a></li>
<li class="toc-entry toc-h3"><a href="#coefficients-in-log-log-regressions--proportional-percentage-changes">Coefficients in log-log regressions ≈ proportional percentage changes</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#2-introduction-to-forecasting-the-simplest-models">2. Introduction to forecasting: the simplest models</a>
<ul>
<li class="toc-entry toc-h2"><a href="#review-of-basic-statistics-and-the-simplest-forecasting-model-the-sample-mean">Review of basic statistics and the simplest forecasting model: the sample mean</a>
<ul>
<li class="toc-entry toc-h4"><a href="#why-squared-error">Why squared error?</a></li>
<li class="toc-entry toc-h4"><a href="#fundamental-law-of-forecasting-risk">Fundamental law of forecasting risk</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#notes-on-the-random-walk-model">Notes on the random walk model</a>
<ul>
<li class="toc-entry toc-h3"><a href="#the-geometric-random-walk-model">The geometric random walk model</a></li>
<li class="toc-entry toc-h3"><a href="#reasons-for-using-the-random-walk-model">Reasons for using the random walk model</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#mean-constant-model">Mean (constant) model</a></li>
<li class="toc-entry toc-h2"><a href="#linear-trend-model">Linear trend model</a></li>
<li class="toc-entry toc-h2"><a href="#random-walk-model">Random walk model</a></li>
<li class="toc-entry toc-h2"><a href="#geometric-random-walk-model">Geometric random walk model</a>
<ul>
<li class="toc-entry toc-h3"><a href="#more-general-random-walk-forecasting-models">More general random walk forecasting models</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#three-types-of-forecasts-estimation-validation-and-the-future">Three types of forecasts: estimation, validation, and the future</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#3-averaging-and-smoothing-models">3. Averaging and smoothing models</a>
<ul>
<li class="toc-entry toc-h2"><a href="#simple-moving-averages">Simple moving averages</a></li>
<li class="toc-entry toc-h2"><a href="#comparing-measures-of-forecast-error-between-models">Comparing measures of forecast error between models</a></li>
<li class="toc-entry toc-h2"><a href="#simple-exponential-smoothing">Simple exponential smoothing</a></li>
<li class="toc-entry toc-h2"><a href="#linear-exponential-smoothing-les">Linear Exponential Smoothing (LES)</a></li>
<li class="toc-entry toc-h2"><a href="#out-of-sample-validation">Out-of-sample validation</a></li>
<li class="toc-entry toc-h2"><a href="#moving-average-and-exponential-smoothing-models">Moving average and exponential smoothing models</a></li>
<li class="toc-entry toc-h2"><a href="#forecasting-with-adjustments-for-inflation-and-seasonality">Forecasting with adjustments for inflation and seasonality</a>
<ul>
<li class="toc-entry toc-h3"><a href="#modeling-the-effect-of-inflation">Modeling the effect of inflation</a></li>
<li class="toc-entry toc-h3"><a href="#seasonality">Seasonality</a></li>
<li class="toc-entry toc-h3"><a href="#multiplicative-seasonality">Multiplicative seasonality</a></li>
<li class="toc-entry toc-h3"><a href="#additive-seasonality">Additive seasonality</a></li>
<li class="toc-entry toc-h3"><a href="#seasonal-index">Seasonal index</a></li>
<li class="toc-entry toc-h3"><a href="#winters-seasonal-smoothing">Winters’ Seasonal Smoothing</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#4-linear-regression-models">4. Linear regression models</a>
<ul>
<li class="toc-entry toc-h2"><a href="#introduction-to-linear-regression">Introduction to linear regression</a></li>
<li class="toc-entry toc-h2"><a href="#correlation-and-regression-to-mediocrity">Correlation and regression-to-mediocrity</a></li>
<li class="toc-entry toc-h2"><a href="#mathematics-of-a-regression-model">Mathematics of a regression model</a></li>
<li class="toc-entry toc-h2"><a href="#what-to-look-for-in-regression-model-output">What to look for in regression model output</a>
<ul>
<li class="toc-entry toc-h3"><a href="#standard-error-of-the-regression-root-mean-squared-error-adjusted-for-degrees-of-freedom">Standard error of the regression (root-mean-squared error adjusted for degrees of freedom)</a></li>
<li class="toc-entry toc-h3"><a href="#adjusted-r-squared">Adjusted R-squared</a></li>
<li class="toc-entry toc-h3"><a href="#significance-of-the-estimated-coefficients">Significance of the estimated coefficients</a></li>
<li class="toc-entry toc-h3"><a href="#values-of-the-estimated-coefficients">Values of the estimated coefficients</a></li>
<li class="toc-entry toc-h3"><a href="#plots-of-forecasts-and-residuals">Plots of forecasts and residuals</a></li>
<li class="toc-entry toc-h3"><a href="#out-of-sample-validation-1">Out-of-sample validation</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#whats-the-bottom-line-how-to-compare-models">What’s the bottom line? How to compare models</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#5-arima-models-for-time-series-forecasting">5. ARIMA models for time series forecasting</a>
<ul>
<li class="toc-entry toc-h2"><a href="#what-arima-stands-for">What ARIMA stands for</a></li>
<li class="toc-entry toc-h2"><a href="#arima-models-put-it-all-together">ARIMA models put it all together</a></li>
<li class="toc-entry toc-h2"><a href="#construction-of-an-arima-model">Construction of an ARIMA model</a></li>
<li class="toc-entry toc-h2"><a href="#arima-terminology">ARIMA terminology</a></li>
<li class="toc-entry toc-h2"><a href="#do-you-need-both-ar-and-ma-terms">Do you need both AR and MA terms?</a></li>
<li class="toc-entry toc-h2"><a href="#interpretation-of-ar-terms">Interpretation of AR terms</a></li>
<li class="toc-entry toc-h2"><a href="#interpretation-of-ma-terms">Interpretation of MA terms</a></li>
<li class="toc-entry toc-h2"><a href="#tools-for-identifying-arima-models-acf-and-pacf-plots">Tools for identifying ARIMA models: ACF and PACF plots</a></li>
<li class="toc-entry toc-h2"><a href="#ar-and-ma-signatures">AR and MA “signatures”</a></li>
<li class="toc-entry toc-h2"><a href="#model-fitting-steps">Model-fitting steps</a></li>
<li class="toc-entry toc-h2"><a href="#technical-issues">Technical issues</a></li>
<li class="toc-entry toc-h2"><a href="#seasonal-arima-models">Seasonal ARIMA models</a>
<ul>
<li class="toc-entry toc-h3"><a href="#terminology">Terminology</a></li>
<li class="toc-entry toc-h3"><a href="#model-fitting-steps-1">Model fitting steps</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#bottom-line-suggestion">Bottom-line suggestion</a></li>
<li class="toc-entry toc-h2"><a href="#take-aways">Take-aways</a></li>
<li class="toc-entry toc-h2"><a href="#seasonal-differencing-in-arima-models">Seasonal differencing in ARIMA models</a></li>
<li class="toc-entry toc-h2"><a href="#summary-of-rules-for-identifying-arima-models">Summary of rules for identifying ARIMA models</a>
<ul>
<li class="toc-entry toc-h3"><a href="#identifying-the-order-of-differencing-and-the-constant">Identifying the order of differencing and the constant:</a></li>
<li class="toc-entry toc-h3"><a href="#identifying-the-numbers-of-ar-and-ma-terms">Identifying the numbers of AR and MA terms:</a></li>
<li class="toc-entry toc-h3"><a href="#identifying-the-seasonal-part-of-the-model">Identifying the seasonal part of the model:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#6-choosing-the-right-forecasting-model">6. Choosing the right forecasting model</a>
<ul>
<li class="toc-entry toc-h2"><a href="#steps-in-choosing-a-forecasting-model">Steps in choosing a forecasting model</a>
<ul>
<li class="toc-entry toc-h3"><a href="#deflation">Deflation?</a></li>
<li class="toc-entry toc-h3"><a href="#logarithm-transformation">Logarithm transformation?</a></li>
<li class="toc-entry toc-h3"><a href="#seasonal-adjustment-1">Seasonal adjustment?</a></li>
<li class="toc-entry toc-h3"><a href="#independent-variables">Independent variables?</a></li>
<li class="toc-entry toc-h3"><a href="#smoothing-averaging-or-random-walk">Smoothing, averaging, or random walk?</a></li>
<li class="toc-entry toc-h3"><a href="#winters-seasonal-exponential-smoothing">Winters Seasonal Exponential Smoothing?</a></li>
<li class="toc-entry toc-h3"><a href="#arima">ARIMA?</a>
<ul>
<li class="toc-entry toc-h4"><a href="#steps">Steps</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#forecasting-flow-chart">Forecasting Flow Chart</a></li>
<li class="toc-entry toc-h2"><a href="#automatic-forecasting-software">Automatic Forecasting Software</a></li>
<li class="toc-entry toc-h2"><a href="#how-to-avoid-trouble-principles-of-good-data-analysis">How to avoid trouble: principles of good data analysis</a></li>
</ul>
</li>
</ul><p>My notes and highlights on the book.</p>

<p>Author: Robert Nau</p>

<p><a href="https://people.duke.edu/~rnau/411home.htm">Available here</a></p>
<blockquote>
  <p>“This web site contains notes and materials for an advanced elective course on statistical forecasting that is taught at the Fuqua School of Business, Duke University. It covers linear regression and time series forecasting models as well as general principles of thoughtful data analysis.”</p>
</blockquote>

<blockquote>
  <p>“I have seen the future and it is very much like the present, only longer.” 
– Kehlog Albran, <em>The Profit</em></p>
</blockquote>

<h1 id="1-get-to-know-your-data">
<a class="anchor" href="#1-get-to-know-your-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>1. Get to know your data</h1>
<h2 id="principles-and-risks-of-forecasting">
<a class="anchor" href="#principles-and-risks-of-forecasting" aria-hidden="true"><span class="octicon octicon-link"></span></a>Principles and risks of forecasting</h2>
<p>Statistical forecasting: art and science of forecasting from data, with or without knowing in advance what equation you should use</p>

<h3 id="signal-vs-noise">
<a class="anchor" href="#signal-vs-noise" aria-hidden="true"><span class="octicon octicon-link"></span></a>Signal vs. noise</h3>
<p>Variable you want to predict = signal + noise</p>
<ul>
  <li>Signal: predictable component</li>
  <li>Noise: what is left over</li>
</ul>

<p>Sensitive statistical tests are needed to get a better idea of whether the pattern you see in the data is really random or whether there is some signal yet to be extracted. If you fail to detect a signal that is really there, or falsely detect a signal that isn’t really there, your forecasts will be at best suboptimal and at worst dangerously misleading</p>

<p><strong>random walk model</strong>: the variable takes random steps up and down as it goes forward:</p>
<ul>
  <li>if you transform by taking the period-to-period changes (the “first difference”) it becomes a time series that is described by the mean model</li>
  <li>the confidence limits for the forecasts gets wider at longer forecast horizons</li>
  <li>typical of random walk patterns -&gt; they don’t look random as they are! -&gt; analyze the statistical properties: momentum, mean-reversion, seasonality</li>
</ul>

<h3 id="risks-of-forecasting">
<a class="anchor" href="#risks-of-forecasting" aria-hidden="true"><span class="octicon octicon-link"></span></a>Risks of forecasting</h3>
<blockquote>
  <p>“If you live by the crystal ball you end up eating broken glass”</p>
</blockquote>

<ul>
  <li>
<strong>Intrinsic risk</strong>: random variation beyond explanation with the data and tools available</li>
  <li>
<strong>Parameter risk</strong>: errors in estimating the parameters of the forecasting model, under the assumption that you are fitting the correct model to the data in the first place
    <blockquote>
      <p>When predicting time series, more sample data is not always better -&gt; might include older data that is not as representative of current conditions. <strong>Blur of history</strong> problem: no pattern really stays the same forever</p>
    </blockquote>
  </li>
</ul>

<p>You can’t eliminate instrinsic risk and parameter risk, <em>you can and should try to quantify</em> them in relative terms -&gt; so the appropriate risk-return tradeoffs can be made when decisions are based on the forecast</p>

<ul>
  <li>
<strong>Model risk</strong>: risk of choosing the wrong model. <em>Most serious form of forecast error</em> -&gt; can be reduced by following good statistical practices: Follow good practices for exploring the data, understand the assumptions that are behind the models and test the assumptions.</li>
</ul>

<p>If the errors are not pure noise -&gt; there is some pattern in them, and you could make them smaller by adjusting the model to explain that pattern</p>

<h2 id="get-to-know-your-data">
<a class="anchor" href="#get-to-know-your-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Get to know your data</h2>
<ul>
  <li>Where did it come from?</li>
  <li>Where has it been?</li>
  <li>Is it clean or dirty?</li>
  <li>In what units is it measured?</li>
</ul>

<blockquote>
  <p>Assembling, cleaning, adjusting and documenting the units of the data is often the most tedious step of forecasting</p>
</blockquote>

<h3 id="plot-the-data">
<a class="anchor" href="#plot-the-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>PLOT THE DATA!</h3>
<p>You should graph your data to get a feel for its qualitative properties -&gt; your model must accommodate these features and ideally it should shed light on their underlying causes</p>

<h2 id="inflation-adjustment-deflation">
<a class="anchor" href="#inflation-adjustment-deflation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Inflation adjustment (“deflation”)</h2>
<p>Accomplished by dividing a monetary time series by a price index, such as the Consumer Price Index (CPI) -&gt; uncover the real growth</p>
<ul>
  <li>original series: “nominal dollars” or “current dollars”</li>
  <li>deflated series: “constant dollars”</li>
</ul>

<blockquote>
  <p>Not always necessary, sometimes forecasting the nominal data or log transforming for stabilizing the variance is simpler</p>
</blockquote>

<p>Inflation adjustment is only appropriated for money series. If a non-monetary series shows signs of exponential growth or increasing variance -&gt; try a logarithm transformation</p>

<h2 id="seasonal-adjustment">
<a class="anchor" href="#seasonal-adjustment" aria-hidden="true"><span class="octicon octicon-link"></span></a>Seasonal adjustment</h2>
<h3 id="multiplicative-adjustment">
<a class="anchor" href="#multiplicative-adjustment" aria-hidden="true"><span class="octicon octicon-link"></span></a>Multiplicative adjustment</h3>
<p>Increasing amplitude of seasonal variations is suggestive of a multiplicative seasonal pattern -&gt; can be removed by <strong>multiplicative seasonal adjustment</strong>: dividing each value of the time series by a seasonal index that is representative of normal typically observed in that season</p>

<h3 id="additive-adjustment">
<a class="anchor" href="#additive-adjustment" aria-hidden="true"><span class="octicon octicon-link"></span></a>Additive adjustment</h3>
<p>For time series whose seasonal variations are roughly constant in magnitude, independent of the current average level of the series -&gt; adding or subtracting a quantity that represents the absolute amount by which the value in that season of the year tends to be below or above normal, as estimated from past data</p>

<blockquote>
  <p>Additive seasonal patterns are somewhat rare, but if applying log transform -&gt; you should use additive rather than multiplicative</p>
</blockquote>

<h3 id="acronyms">
<a class="anchor" href="#acronyms" aria-hidden="true"><span class="octicon octicon-link"></span></a>Acronyms</h3>
<ul>
  <li>
<strong>SA</strong>: seasonally adjusted</li>
  <li>
<strong>NSA</strong>: not seasonally adjusted</li>
  <li>
<strong>SAAR</strong>: seasonally adjusted annual rate -&gt; each period’s value has been adjusted for seasonality and then multiplied by the number of periods in a year, as though the same value had been obtained in every period for a whole year</li>
</ul>

<h2 id="stationarity-and-differencing">
<a class="anchor" href="#stationarity-and-differencing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Stationarity and differencing</h2>
<h3 id="statistical-stationarity">
<a class="anchor" href="#statistical-stationarity" aria-hidden="true"><span class="octicon octicon-link"></span></a>Statistical stationarity</h3>
<p>A stationary time series is one whose statistical properties such as mean, variance, autocorrelation, etc. are all constant over time. Most statistical forecasting methods are based on the assumption that the time series can be rendered approximately stationary (i.e., “stationarized”) through the use of mathematical transformations</p>

<ul>
  <li>
<strong>trend-stationary</strong>: series has a stable long-run rend and tends to revert to the trend line following a disturbance -&gt; to stationarize it = detrending</li>
  <li>
<strong>difference-stationary</strong>: if the mean, variance, and autocorrelations of the original series are not constant in time, even after detrending, perhaps the statistics of the changes in the series between periods or between seasons will be constant</li>
</ul>

<blockquote>
  <p><strong>Unit root test</strong>: to understand if a series is trend-stationary or difference-stationary</p>
</blockquote>

<h3 id="first-difference">
<a class="anchor" href="#first-difference" aria-hidden="true"><span class="octicon octicon-link"></span></a>First-difference</h3>
<p>Series of changes from one period to the next</p>
<ul>
  <li>
<strong>random walk model</strong>: if first-difference of a series is stationary and also completely random (not autocorrelated)</li>
  <li>
<strong>ETS or ARIMA</strong>: can be used when the first-difference of a series is stationary but not completely random (its value at period t is autocorrelated with its value at earlier periods)</li>
</ul>

<h2 id="the-logarithm-transformation">
<a class="anchor" href="#the-logarithm-transformation" aria-hidden="true"><span class="octicon octicon-link"></span></a>The logarithm transformation</h2>
<h3 id="change-in-natural-log--percentage-change">
<a class="anchor" href="#change-in-natural-log--percentage-change" aria-hidden="true"><span class="octicon octicon-link"></span></a>Change in natural log ≈ percentage change</h3>
<p><strong>Small</strong> changes in the natural log of a variable are directly interpretable as percentage changes to a very close approximation</p>

<h3 id="linearization-of-exponential-growth-and-inflation">
<a class="anchor" href="#linearization-of-exponential-growth-and-inflation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Linearization of exponential growth and inflation</h3>
<p>The log transformation converts the exponential growth pattern to a linear growth pattern, and it simultaneously converts the multiplicative (proportional-variance) seasonal pattern to an additive (constant-variance) seasonal pattern</p>

<blockquote>
  <p>Logging a series often has an effect very similar to deflating: it straightens out exponential growth patterns and reduces heteroscedasticity (i.e., stabilizes variance). Logging is therefore a “poor man’s deflator” which does not require any external data</p>
</blockquote>

<p><strong>Geometric random walk</strong>: logging the data before fitting a random walk model -&gt; commonly used for stock price data</p>

<h3 id="trend-measured-in-natural-log-units--percentage-growth">
<a class="anchor" href="#trend-measured-in-natural-log-units--percentage-growth" aria-hidden="true"><span class="octicon octicon-link"></span></a>Trend measured in natural-log units ≈ percentage growth</h3>
<p>Usually the trend is estimated more precisely by fitting a statistical model that explicitly includes a local or global trend parameter, such as a linear trend or random-walk-with-drift or linear exponential smoothing model.  When a model of this kind is fitted in conjunction with a log transformation, its trend parameter can be interpreted as a percentage growth rate.</p>

<h3 id="errors-measured-in-natural-log-units--percentage-errors">
<a class="anchor" href="#errors-measured-in-natural-log-units--percentage-errors" aria-hidden="true"><span class="octicon octicon-link"></span></a>Errors measured in natural-log units ≈ percentage errors</h3>
<p>If you look at the error statistics in logged units, you can interpret them as percentages if they are not too large – if the standard deviation is 0.1 or less</p>

<h3 id="coefficients-in-log-log-regressions--proportional-percentage-changes">
<a class="anchor" href="#coefficients-in-log-log-regressions--proportional-percentage-changes" aria-hidden="true"><span class="octicon octicon-link"></span></a>Coefficients in log-log regressions ≈ proportional percentage changes</h3>
<p>In many economic situations (particularly price-demand relationships), the marginal effect of one variable on the expected value of another is linear in terms of percentage changes rather than absolute changes</p>

<h1 id="2-introduction-to-forecasting-the-simplest-models">
<a class="anchor" href="#2-introduction-to-forecasting-the-simplest-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>2. Introduction to forecasting: the simplest models</h1>
<h2 id="review-of-basic-statistics-and-the-simplest-forecasting-model-the-sample-mean">
<a class="anchor" href="#review-of-basic-statistics-and-the-simplest-forecasting-model-the-sample-mean" aria-hidden="true"><span class="octicon octicon-link"></span></a>Review of basic statistics and the simplest forecasting model: the sample mean</h2>
<p>Historical sample mean (or constant model, or intercept-only regression): if the series consists of i.i.d. values, the sample mean should be the next value if the goal is to minimize MSE</p>

<h4 id="why-squared-error">
<a class="anchor" href="#why-squared-error" aria-hidden="true"><span class="octicon octicon-link"></span></a>Why <em>squared</em> error?</h4>
<ul>
  <li>the central value around which the um of squared deviations are minimized is the sample mean</li>
  <li>variances are additive when random variables that are statistically independent are added together</li>
  <li>large errors often have disproportionately worse consequences than small errors, hence the squared error is more representative of the economic consequences of error</li>
  <li>variances and covariances play a key rola in normal distribution theory and regression analysis</li>
</ul>

<blockquote>
  <p>nonlinear transformations of the data (e.g., log or power transformations) can often be used to turn skewed distributions into symmetric (ideally normal) ones, allowing such data to be well fitted by models that focus on mean values.</p>
</blockquote>

<h4 id="fundamental-law-of-forecasting-risk">
<a class="anchor" href="#fundamental-law-of-forecasting-risk" aria-hidden="true"><span class="octicon octicon-link"></span></a>Fundamental law of forecasting risk</h4>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Variance of forecasting risk = 
variance of intrinsic risk + 
variance of parameter risk
</code></pre></div></div>

<p>Confidence intervals: sort like a probability, but not exactly -&gt; there’s an x% probability that your future data will fall in your x% confidence interval for the forecast</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Confidence interval = 
forecast ± 
(critical t-value) × (standard error of forecast)
</code></pre></div></div>

<p>95% confidence interval is (roughly) the forecat “plus-or-minus two standard errors”</p>

<blockquote>
  <p>A rule of thumb: when adjusted R-squared is fairly small (say, less than 20%), the percentage by which the standard error of the regression model is less than the standard error of the mean model is roughly one-half of adjusted R-squared.</p>
</blockquote>

<p>t-stats, P-values, and R-squared, and other test statistics are numbers you should know how to interpret and use, but they are not the most important numbers in your analysis and they are not the bottom line:</p>
<ul>
  <li>what new things have you learned from your data?</li>
  <li>what assumptions does your model make?</li>
  <li>would these assumptions make sense to someone else?</li>
  <li>would a simpler model perform almost as well?</li>
  <li>how accurate are your model’s predictions?</li>
  <li>how accurate it is likely to be to predict the future?</li>
  <li>how good are the inferences and decisions?</li>
</ul>

<h2 id="notes-on-the-random-walk-model">
<a class="anchor" href="#notes-on-the-random-walk-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Notes on the random walk model</h2>
<p>Model assumes that <em>in each period the variable takes a random step away from its previous value, and the steps are independently and identically distributed in size (“i.i.d.”)</em>. This is equivalent to saying that the first difference of the variable is a series to which the mean model should be applied.</p>

<blockquote>
  <p>if you begin with a series that wanders all over the map, the first difference looks i.i.d. sequence -&gt; random walk model is a potentially good candidate</p>
</blockquote>

<ul>
  <li>without drift: all future values will equal the last observed value</li>
  <li>with drift: the average increase from one period to the next (estimated drift = slope = d)</li>
</ul>

<blockquote>
  <p><strong>Square root of time rule</strong>: The confidence interval for a k-period-ahead random walk forecast is wider than that of a 1-period-ahead forecast by a factor of square-root-of-k</p>
</blockquote>

<p>Are the daily changes statistically independent as well having a mean of zero? autocorrelation plot -&gt; <strong>random walk without drift</strong></p>

<h3 id="the-geometric-random-walk-model">
<a class="anchor" href="#the-geometric-random-walk-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>The geometric random walk model</h3>
<p>The natural logarithm of the variable is assumed to walk a random walk, usually with drift</p>

<p>Diff-logs are interpretable as (approximate) percentage changes</p>

<blockquote>
  <p>it is very hard to estimate the trend in a random-walk-with-drift model based on the mean growth that was observed in the sample of data unless the sample size is very large</p>
</blockquote>

<p>Fitting a random-walk-with-drift model to the logged series is equivalent to fitting the geometric random walk model to the original series.</p>

<h3 id="reasons-for-using-the-random-walk-model">
<a class="anchor" href="#reasons-for-using-the-random-walk-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Reasons for using the random walk model</h3>
<ul>
  <li>If you see what looks like pure noise (i.i.d. variations) after performing a 1st -difference or diff-log transformation, then your data is telling you that you that it is a random walk. This isn’t very exciting in terms of the point forecasts you should make (“next month will be the same as last month, plus average growth”), but it has very important implications in terms of how much uncertainty there is in forecasting more than one period ahead.</li>
  <li>benchmark against which to compare more complicated time series models, particularly regression models</li>
</ul>

<h2 id="mean-constant-model">
<a class="anchor" href="#mean-constant-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Mean (constant) model</h2>
<p>Predicting a variable whose values are i.i.d.</p>

<p><strong>Sample mean</strong>: by definition an unbiased predictor and minimizes the mean squared forecasting error regardless of the probability distribution -&gt; it is the value around which the sum of squared deviations of the sample data is minimized</p>

<p><em>Standard error of the mean</em>: how accurate is the estimate of the sample mean -&gt; equals the sample stdev divided by the sqrt of the sample size</p>

<p>Central limit theorem -&gt; large samples: 95% confidence interval = mean +- 2*stdev</p>

<p><em>Standard error of the forecast</em>: equal to the sample stdev * sqrt(1+1/n)</p>

<p>Confidence interval for a forecast is the point forecast plus-or-minus the appropriate critical t-value times the standard error of the forecast</p>

<blockquote>
  <p>The critical t-value for a 50% confidence interval is approximately 2/3, so a 50% confidence interval is one-third the width of a 95% confidence interval. The nice thing about a 50% confidence interval is that it is a <strong>“coin flip”</strong> as to whether the true value will fall inside or outside of it, which is extremely easy to think about</p>
</blockquote>

<p>If we can find some mathematical transformation (e.g., differencing, logging, deflating, etc.) that converts the original time series into a sequence of values that are i.i.d., we can use the mean model to obtain forecasts and confidence limits for the transformed series, and then reverse the transformation to obtain corresponding forecasts and confidence limits for the original series.</p>

<h2 id="linear-trend-model">
<a class="anchor" href="#linear-trend-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Linear trend model</h2>
<p>aka trend-line model: special case of a simple regression model in which the independent variable is just a time index variable.</p>

<blockquote>
  <p>R-squared = 0.143 -&gt; the variance of the regression model’s errors is 14.3% less than the variance of the mean model’s errors, i.e., the model has “explained” 14.3% of the variance in the series</p>
</blockquote>

<p>If the model has succeeded in extracting all the “signal” from the data, there should be no pattern at all in the errors: the error in the next period should not be correlated with any previous errors:</p>
<ul>
  <li>
<strong>lag-1 autocorrelation</strong>: should be very close to zero</li>
  <li>
<strong>Durbin-Watson statistic</strong>: ought to be very close to 2</li>
</ul>

<blockquote>
  <p>trend lines have their use as visual aids, but are often poor for forecasting</p>
</blockquote>

<h2 id="random-walk-model">
<a class="anchor" href="#random-walk-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Random walk model</h2>
<p>Time series with irregular growth -&gt; predict the change from one period to the next (first difference)</p>

<p><strong>autocorrelation at lag k</strong> -&gt; correlation between the variable and itself lagged by k periods</p>

<ul>
  <li>
<strong>random-walk-without-drift</strong>: assumes that at each point, the series merely takes a random step away from its last recorded position, with steps whose mean value is zero -&gt; values of the autocorrelations are not significantly different than zero (95% confidence interval), no change from one period to the next, because past data provides no information about the direction of future movements</li>
  <li>
<strong>random-walk-with-drift</strong>: mean step size is some nonzero value</li>
</ul>

<p>In the random-walk-without-drift model, the standard error of the 1-step ahead forecast is the root-mean-squared-value of the period-to-period changes</p>

<p>For a random-walk-with-drift, the forecast standard error is the sample standard deviation of the period-to-period changes.</p>

<p>“Square root of time” rule for the errors of random walk forecasts: the standard error of a k-step-ahead forecast is larger than that of the 1-step-ahead forecast by a factor of square-root-of-k. This explains the sideways-parabola shape of the confidence bands for long-term forecasts.</p>

<blockquote>
  <p>Random walk may look trivial -&gt; naive model (always predict that tomorrow will be the same as today). The square-root-of-time pattern in its confidence bands for long-term forecasts is of profound importance in finance (it is the basis of the theory of options pricing), and the random walk model often provides a good benchmark against which to judge the performance of more complicated models</p>
</blockquote>

<p>RWM -&gt; special case of an ARIMA model -&gt; ARIMA(0, 1, 0)</p>

<h2 id="geometric-random-walk-model">
<a class="anchor" href="#geometric-random-walk-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Geometric random walk model</h2>
<p>Natural logarithm transformation: linearize exponential growth and stabilize variance of changes (“diff-log”)</p>

<blockquote>
  <p>It can be dangerous to estimate the average rate of return to be expected in the future (let alone anticipate short-term changes in direction), by fitting straight lines to finite samples of data!</p>
</blockquote>

<p><strong>Geometric random walk model</strong>:  Application of the random walk model to the logged series implies that the forecast for the next month’s value of the original series will equal the previous month’s value plus a constant percentage increase.</p>

<p>In unlogged units, the 95% confidence limits for long-term forecasts are noticeably asymmetric</p>

<h3 id="more-general-random-walk-forecasting-models">
<a class="anchor" href="#more-general-random-walk-forecasting-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>More general random walk forecasting models</h3>
<ul>
  <li>RW model 1: basic geometric random walk -&gt; assumes series in different periods are statistically independent (uncorrelated) and also identically distributed</li>
  <li>RW model 2: assumes the series in different periods are statistically independent but not identically distributed</li>
  <li>RW model 3: assumes that returns in different periods are uncorrelated but not otherwise independent. The <strong>ARCH</strong> (autoregressive conditional heteroscedasticity) and <strong>GARCH</strong> (generalized ARCH) models assume that the local volatility follows an autoregressive process, which is characterized by sudden jumps in volatility with a slow reversion to an average volatility</li>
</ul>

<h2 id="three-types-of-forecasts-estimation-validation-and-the-future">
<a class="anchor" href="#three-types-of-forecasts-estimation-validation-and-the-future" aria-hidden="true"><span class="octicon octicon-link"></span></a>Three types of forecasts: estimation, validation, and the future</h2>
<p><strong>Out-of-sample validation</strong>: withhold some of the sample data from the model identification and estimation process, then use the model to make predictions for the hold-out data to see how accurate they are and to determine whether the statistics of their errors are similar to those that the model made within the sample of data that was fitted</p>

<p>Overfitting (likely when):</p>
<ul>
  <li>model with a large number of parameters fitted to a small sample of data</li>
  <li>model has been selected from a large set of potential models precisely by minimizing the MSE in the estimation period</li>
</ul>

<p>Backtests: one-step-ahead forecasts in the validation period (held out during parameter estimation)</p>

<blockquote>
  <p>If you test a great number of models and choose the model whose errors are smallest in the validation period, you may end up overfitting the data within the validation period as well as in the estimation period</p>
</blockquote>

<ul>
  <li>
<strong>Holding data out for validation purposes</strong> is probably the single most important diagnostic test of a model: it gives the best indication of the accuracy that can be expected when forecasting the future</li>
  <li>When you’re ready to forecast the future in real time, you should of course use all the available data for estimation, so that the most recent data is used</li>
</ul>

<p>Forecasts into the future are “true” forecasts that are made for time periods beyond the end of the available data</p>

<p>The model with the tightest confidence intervals is not always the best model -&gt; a bad model not always know it is a bad model</p>

<h1 id="3-averaging-and-smoothing-models">
<a class="anchor" href="#3-averaging-and-smoothing-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>3. Averaging and smoothing models</h1>
<h2 id="simple-moving-averages">
<a class="anchor" href="#simple-moving-averages" aria-hidden="true"><span class="octicon octicon-link"></span></a>Simple moving averages</h2>
<ul>
  <li>mean model: best predictor of tomorrow is the avg of everything that has happened until now</li>
  <li>random walk model: best predictor of tomorrow is what happened today, ignoring previous history</li>
  <li>moving average: take an average of what has happened in some window of the recent past</li>
</ul>

<p><strong>moving average model</strong>: superior to the mean model in adapting to cyclical pattern and superior to the random walk model in not being too sensitive to random shocks from one period to the next</p>

<p>Simple moving average (SMA):</p>
<ul>
  <li>Each of the past m observations gets a weight of <code class="highlighter-rouge">1/m</code> in the averaging formula, so as <code class="highlighter-rouge">m</code> gets larger, each individual observation in the recent past receives less weight. This implies that larger values of <code class="highlighter-rouge">m</code> will filter out more of the period-to-period noise and yield <em>smoother-looking</em> series of forecasts</li>
  <li>average age of the data in the forecast is <code class="highlighter-rouge">(m+1)/2</code> -&gt; amount by which the forecasts will tend to lag behind in trying to follow trends or respond to turning points</li>
</ul>

<blockquote>
  <p>Value of <code class="highlighter-rouge">m</code> tradeoff: filtering out more noise vs. being too slow to respond to trends and turning points</p>
</blockquote>

<h2 id="comparing-measures-of-forecast-error-between-models">
<a class="anchor" href="#comparing-measures-of-forecast-error-between-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>Comparing measures of forecast error between models</h2>
<ul>
  <li>
<strong>RMSE</strong>: root mean squared error: (the most common standard of goodness-of-fit, penalizes big errors relatively more than small errors because it squares them first; it is approximately the standard deviation of the errors if the mean error is close to zero)</li>
  <li>
<strong>MAE</strong>: mean absolute error (the average of the absolute values of the errors, more tolerant of the occasional big error because errors are not squared)</li>
  <li>
<strong>MAPE</strong>: mean absolute percentage error (perhaps better to focus on if the data varies over a wide range due to compound growth or inflation or seasonality, in which case you may be more concerned about measuring errors in percentage terms)</li>
  <li>
<strong>ME</strong>: mean error (this indicates whether forecasts are biased high or low—should be close to 0)</li>
  <li>
<strong>MPE</strong>: mean percentage error (ditto in percentage terms)</li>
</ul>

<p>Best measure for size of error = RMSE</p>

<p>Easier for non-specialists to understand = MAE and MAPE</p>

<p><code class="highlighter-rouge">SMA with a trend = SMD + drift</code> (add a constant to the SMA forecasting equation)</p>

<p><code class="highlighter-rouge">Tapered Moving Average</code>: put only half as much weight on the newest and oldest values -&gt; more robust to outliers in the data</p>

<h2 id="simple-exponential-smoothing">
<a class="anchor" href="#simple-exponential-smoothing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Simple exponential smoothing</h2>
<p>SMA problems:</p>
<ul>
  <li>putting equal weight on the last m observations and no weight on any previous observations is usually not the best way to average values that are arriving consecutively in time</li>
  <li>would make more sense to gradually decrease the weights placed on the older values</li>
  <li>its confidence intervals for long-horizon forecasts do not widen at all</li>
</ul>

<p><strong>Simple exponential smoothing (SES) aka Exponentially weighted moving average model</strong>: addresses these shortcomings of SMA</p>

<p>most used time series model in business applications:</p>
<ul>
  <li>good forecast under a wide range of conditions</li>
  <li>computationally it is extremely simple</li>
</ul>

<blockquote>
  <p>the SES model is an interpolation between the mean model and the random walk model with respect to the way it responds to new data. As such it might be expected to do better than either of them in situations where the random walk model over-responds and the mean model under-responds, and indeed it does</p>
</blockquote>

<p>Overall the SES model is superior to the SMA model in responding a bit more quickly to the newest data while treating older data more even-handedly, when the models otherwise yield the same average age</p>

<blockquote>
  <p>all models are based on assumptions about how the world works, and you need to understand what the assumptions are and (ideally) you should believe in the assumptions of your chosen model and be able to explain and defend them.</p>
</blockquote>

<h2 id="linear-exponential-smoothing-les">
<a class="anchor" href="#linear-exponential-smoothing-les" aria-hidden="true"><span class="octicon octicon-link"></span></a>Linear Exponential Smoothing (LES)</h2>
<p>Generalization of the SES to obtain a model that computes local estimates of both level and trend -&gt; same basic logic, but now you have two smoothing constants, one for the level and one for the trend</p>

<p>Any smoothing model will lag behind to some extent in responding to unforeseen changes in level or trend</p>

<blockquote>
  <p>many time series that arise in business and economics (as well as engineering and the natural sciences) which are inherently non-seasonal or which have been seasonally adjusted display a pattern of <strong>random variations around a local mean value or a local trend line that changes slowly with time</strong>. The first difference of such a series is negatively autocorrelated at lag 1: a positive change tends to be followed by a (smaller) negative one. For time series of this type, a smoothing or averaging model is the appropriate forecasting model.</p>
</blockquote>

<h2 id="out-of-sample-validation">
<a class="anchor" href="#out-of-sample-validation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Out-of-sample validation</h2>
<p>Aka “backtesting”: holding out some of the data while estimating parameters of alternative models, then freezing those parameter estimates and using them to make predictions for the hold-out data</p>

<blockquote>
  <p>You hope to find the statistics of the errors of the predictions for the hold-out data look very similar to those of the predictions for the sample data</p>
</blockquote>

<p>If the data exhibits exponential growth due to compounding or inflation, then it will display greater volatility in absolute terms toward the end of the series, even if the volatility is constant in percentage terms. In situations like this you may want to use a nonlinear transformation such as logging or deflating as part of your model</p>

<blockquote>
  <p>it is usually best to look at <strong>MAPE’s</strong> rather than RMSE’s when asking whether a given model performed about as well in the validation period as in the estimation period.</p>
</blockquote>

<h2 id="moving-average-and-exponential-smoothing-models">
<a class="anchor" href="#moving-average-and-exponential-smoothing-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>Moving average and exponential smoothing models</h2>
<p>Moving beyond mean models, random walk models, and linear trend models, nonseasonal patterns and trends can be extrapolated using a moving-average or smoothing model.</p>

<blockquote>
  <p>Which type of trend-extrapolation is best: horizontal or linear? Empirical evidence suggests that, if the data have already been adjusted (if necessary) for inflation, then it may be imprudent to extrapolate short-term linear trends very far into the future. Trends evident today may slacken in the future due to varied causes such as product obsolescence, increased competition, and cyclical downturns or upturns in an industry. For this reason, simple exponential smoothing often performs better out-of-sample than might otherwise be expected, despite its “naive” horizontal trend extrapolation.  Damped trend modifications of the linear exponential smoothing model are also often used in practice to introduce a note of conservatism into its trend projections.</p>
</blockquote>

<h2 id="forecasting-with-adjustments-for-inflation-and-seasonality">
<a class="anchor" href="#forecasting-with-adjustments-for-inflation-and-seasonality" aria-hidden="true"><span class="octicon octicon-link"></span></a>Forecasting with adjustments for inflation and seasonality</h2>
<ul>
  <li>Deflation with prices indices</li>
  <li>Seasonal decompositon</li>
  <li>Time series forecasting models for seasonal data
    <ul>
      <li>Averaging and smoothing combined with seasonal adjustment</li>
      <li>Winters seasonal exponential smoothing model</li>
    </ul>
  </li>
</ul>

<h3 id="modeling-the-effect-of-inflation">
<a class="anchor" href="#modeling-the-effect-of-inflation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Modeling the effect of inflation</h3>
<p><strong>Why?</strong></p>
<ul>
  <li>to measure real growth and estimate its dependence on other real factors</li>
  <li>to remove much of the trend and stabilize variance before fitting the model</li>
</ul>

<p><strong>How?</strong></p>
<ul>
  <li>to “deflate” a variable, you divide it by an appropriate price index variable</li>
  <li>e.g.: general price index, product-specific index</li>
  <li>to “re-inflate” forecasts of a deflated series, you multiply the forecasts and confidence limits by a forecast of the price index</li>
</ul>

<p><strong>Log vs. deflate</strong></p>
<ul>
  <li>Deflation should be used when you are interested in knowing the forecast in “real” terms and/or if the inflation rate is expected to change</li>
  <li>Logging is sufficient if you just want a forecast in “nominal” terms and inflation is expected to remain constant—inflation just gets lumped with other sources of compound growth in the model.</li>
  <li>Logging also ensures that forecasts and confidence limits have positive values, even in the presence of downward trends and/or high volatility.</li>
  <li>If inflation has been minimal and/or there is little overall trend or change in volatility, neither may be necessary</li>
</ul>

<h3 id="seasonality">
<a class="anchor" href="#seasonality" aria-hidden="true"><span class="octicon octicon-link"></span></a>Seasonality</h3>
<ul>
  <li>repeating, preiodic pattern in the data that is keyed to the calendar of the clock</li>
  <li>!= than “cyclality”, which do not have a predictable periodicity</li>
</ul>

<blockquote>
  <p>Seasonal patterns are complex, because the calendar is not rational: months and years don’t have whole numbers of weeks, a given month does not always have the same number of trading days/weekends, Christmans day can fall on any day of the week, some major holidays are “moveable feasts” that do not occur on the same calendar dates each year</p>
</blockquote>

<ul>
  <li>Quarterly data is easiest to handle: 4 quarters in a year, 3 months in a quarter, trading day adjustments have only minor effects.</li>
  <li>Monthly data is more complicated: 12 months in a year, but not 4 weeks in a month; trading day adjustments may be important.</li>
  <li>Weekly data requires special handling because a year is not exactly 52 weeks.</li>
</ul>

<h3 id="multiplicative-seasonality">
<a class="anchor" href="#multiplicative-seasonality" aria-hidden="true"><span class="octicon octicon-link"></span></a>Multiplicative seasonality</h3>
<p>Most natural seasonal patterns are multiplicative</p>
<ul>
  <li>Seasonal variations are roughly constant in percentage terms</li>
  <li>Seasonal swings get larger or smaller in absolute magnitude as the average level of the series rises or falls due to long-term trends and/or business cycle effects</li>
</ul>

<h3 id="additive-seasonality">
<a class="anchor" href="#additive-seasonality" aria-hidden="true"><span class="octicon octicon-link"></span></a>Additive seasonality</h3>
<p>Additive seasonal pattern has constant-amplitude seasonal swings in the presence of trends and cycles.</p>
<ul>
  <li>A log transformation converts a multiplicative pattern to an additive one, so if your model includes a log transformation, use additive rather than multiplicative seasonal adjustment.</li>
  <li>If the historical data sample has little trend and seasonal variations are not large in relative terms, additive and multiplicative adjustment yield very similar results</li>
</ul>

<h3 id="seasonal-index">
<a class="anchor" href="#seasonal-index" aria-hidden="true"><span class="octicon octicon-link"></span></a>Seasonal index</h3>
<ul>
  <li>Represents the expected percentage of “normal” in a given month or quarter</li>
  <li>When the seasonal indices are assumed to be stable over time, they can be estimated by the “ratio to moving average” (RMA) method</li>
</ul>

<h3 id="winters-seasonal-smoothing">
<a class="anchor" href="#winters-seasonal-smoothing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Winters’ Seasonal Smoothing</h3>
<p>The logic of Holt’s LES model can be extended to recursively estimate time-varying seasonal indices as well as level and trend.</p>

<p><strong>Issues</strong>:</p>
<ul>
  <li>Estimation of Winters’ model is tricky</li>
  <li>There are three separate smoothing constants to be jointly estimated by nonlinear least squares.</li>
  <li>Initialization is also tricky, especially for the seasonal indices.</li>
  <li>Confidence intervals sometimes come out extremely wide because the model “lacks confidence in itself.”</li>
</ul>

<p><strong>In practice</strong>:</p>
<ul>
  <li>The Winters model is popular in “automatic forecasting” software, because it has a little of everything (level, trend, seasonality).</li>
  <li>Often it works very well, but difficulties in initialization &amp; estimation can lead to strange results in some cases.</li>
  <li>It responds to recent changes in the seasonal pattern as well as the trend, but with some danger of unstable long-term trend projections.</li>
</ul>

<h1 id="4-linear-regression-models">
<a class="anchor" href="#4-linear-regression-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>4. Linear regression models</h1>
<h2 id="introduction-to-linear-regression">
<a class="anchor" href="#introduction-to-linear-regression" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction to linear regression</h2>
<p>Regression analysis: art and science of fitting straight lines to patterns of data</p>

<p>Assumptions:</p>
<ol>
  <li>The expected value of Y is a linear function of the X variables</li>
  <li>The unexplained variations of Y are independent random variables (in particular, not “autocorrelated” if the variables are time series)</li>
  <li>The all have the same variance (“homoscedasticity”)</li>
  <li>They are normally distributed</li>
</ol>

<blockquote>
  <p>No model is perfect - these assumptions will never be exactly satisfied by real-world messy data - but you hope that they are not badly wrong. The art of regression modeling is to (most importantly!) collect data that is relevant and informative with respect to your decision or inference problem, and then define your variables and construct your model in such a way that the assumptions listed above are plausible, at least as a first-order approximation to what is really happening</p>
</blockquote>

<h2 id="correlation-and-regression-to-mediocrity">
<a class="anchor" href="#correlation-and-regression-to-mediocrity" aria-hidden="true"><span class="octicon octicon-link"></span></a>Correlation and regression-to-mediocrity</h2>
<p><strong>Regression-to-mediocrity aka regression to the mean</strong>: Purely statistical phenomenon that can be viewed as a form of selection bias. Every quantitative measurement is a combination of signal and noise. When a value above the mean is observed, it is probable that the value of the signal was above average and the value of the noise was above average. Now suppose there is some other quantity (say, some measurable trait of the offspring—not necessarily the same one) whose value depends only on the signal, not the noise, in the first quantity. Then a measurement of the second quantity should also be expected to be above the mean, but less so in relative terms, because only the above-average signal is passed on; the above-average noise is not. In fact, it is the independent noise in the second quantity that prevents variations from ever dying out over generations.</p>

<p>The coefficient of correlation between X and Y is the average product of their standardized values</p>

<p>It is a number that lies somewhere between -1 and +1, where -1 indicates a perfect negative linear relationship, +1 indicates a perfect positive linear relationship, and zero indicates no linear relationship.</p>

<blockquote>
  <p>A correlation of zero between X and Y does not necessarily mean that there is no relationship, just that there is no linear relationship within the historical sample of data that is being analyzed. e.g., y = xˆ2</p>
</blockquote>

<p>When we speak of “regressing” one variable on a group of others, we mean the fitting of a linear equation that minimizes the sum of squared errors in predicting that variable from the others.</p>

<h2 id="mathematics-of-a-regression-model">
<a class="anchor" href="#mathematics-of-a-regression-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Mathematics of a regression model</h2>
<ol>
  <li>The coefficients and error measures for a regression model are entirely determined by the following summary statistics: means, standard deviations, and correlations of the variables, and the sample size</li>
  <li>The correlation between Y and X is equal to the average product of their standardized values</li>
  <li>The slope coefficient in a simple regression of Y on X is the correlation between Y and X multiplied by the ratio of their standard deviations</li>
  <li>In a simple regression model, the percentage of variance “explained” by the model, which is called R-squared, is the square of the correlation between Y and X</li>
  <li>The sample standard deviation of the errors is a downward-biased estimate of the size of the true unexplained deviations in Y because it does not adjust for the additional “degree of freedom” used up by estimating the slope coefficient</li>
  <li>Adjusted R-squared, which is obtained by adjusting R-squared for the degrees if freedom for error in exactly the same way, is an unbiased estimate of the amount of variance explained</li>
  <li>For models fitted to the same sample of the same dependent variable, adjusted R-squared always goes up when the standard error of the regression goes down. A model does not always improve when more variables are added: adjusted R-squared can go down (even go negative) if irrelevant variables are added</li>
  <li>The standard error of a coefficient estimate is the estimated standard deviation of the error in measuring it. And the estimated height of the regression line for a given value of X has its own standard error, which is called the standard error of the mean at X. All of these standard errors are proportional to the standard error of the regression divided by the square root of the sample size</li>
  <li>The standard error of the forecast for Y for a given value of X is the square root of the sum of squares of the standard error of the regression and the standard error of the mean at X</li>
  <li>Two-sided confidence limits for coefficient estimates, means, and forecasts are all equal to their point estimates plus-or-minus the appropriate critical t-value times their respective standard errors.</li>
</ol>

<h2 id="what-to-look-for-in-regression-model-output">
<a class="anchor" href="#what-to-look-for-in-regression-model-output" aria-hidden="true"><span class="octicon octicon-link"></span></a>What to look for in regression model output</h2>

<h3 id="standard-error-of-the-regression-root-mean-squared-error-adjusted-for-degrees-of-freedom">
<a class="anchor" href="#standard-error-of-the-regression-root-mean-squared-error-adjusted-for-degrees-of-freedom" aria-hidden="true"><span class="octicon octicon-link"></span></a>Standard error of the regression (root-mean-squared error adjusted for degrees of freedom)</h3>
<p>It is a lower bound on the standard error of any forecast generated from the model</p>

<p>In time series forecasting, also common to look the mean absolute error (MAE) and, for positive data, the mean absolute percentage error (MAPE)</p>

<p>Mean absolute scaled error -&gt; measures improvement in mean absolute error relative to a random-walk-without-drift model</p>

<h3 id="adjusted-r-squared">
<a class="anchor" href="#adjusted-r-squared" aria-hidden="true"><span class="octicon octicon-link"></span></a>Adjusted R-squared</h3>
<p>R-squared (the fraction by which the variance of the errors is less than the variance of the dependent variable) adjusted for the number of coefficients in the model relative to the sample size in order to correct it for bias.</p>

<p>Adjusted R-squared is the fraction by which the square of the standard error of the regression is less than the variance of the dependent variable</p>

<p>Unitless statistic, but there is no absolute standard for what is a “good” value</p>

<h3 id="significance-of-the-estimated-coefficients">
<a class="anchor" href="#significance-of-the-estimated-coefficients" aria-hidden="true"><span class="octicon octicon-link"></span></a>Significance of the estimated coefficients</h3>
<p>Are the t-statistics greater than 2 in magnitude, corresponding to p-values less than 0.05?  If they are not, you should probably try to refit the model with the least significant variable excluded, which is the “backward stepwise” approach to model refinement.</p>

<h3 id="values-of-the-estimated-coefficients">
<a class="anchor" href="#values-of-the-estimated-coefficients" aria-hidden="true"><span class="octicon octicon-link"></span></a>Values of the estimated coefficients</h3>
<p>In general you are interested not only in the statistical significance of an independent variable, you are also interested in its practical significance. In theory, the coefficient of a given independent variable is its proportional effect on the average value of the dependent variable, others things being equal -&gt; “bang for the buck”.</p>

<h3 id="plots-of-forecasts-and-residuals">
<a class="anchor" href="#plots-of-forecasts-and-residuals" aria-hidden="true"><span class="octicon octicon-link"></span></a>Plots of forecasts and residuals</h3>
<p>DO NOT FAIL TO LOOK AT PLOTS OF THE FORECASTS AND ERRORS. Do the forecasts “track” the data in a satisfactory way, apart from the inevitable regression-to-the mean? (In the case of time series data, you are especially concerned with how the model fits the data at the “business end”, i.e., the most recent values.) Do the residuals appear random, or do you see some systematic patterns in their signs or magnitudes?  Are they free from trends, autocorrelation, and heteroscedasticity? Are they normally distributed? There are a variety of statistical tests for these sorts of problems, but the best way to determine whether they are present and whether they are serious is to look at the pictures.</p>

<h3 id="out-of-sample-validation-1">
<a class="anchor" href="#out-of-sample-validation-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Out-of-sample validation</h3>
<p>A good model should have small error measures in both the estimation and validation periods, compared to other models, and its validation period statistics should be similar to its own estimation period statistics. Regression models with many independent variables are especially susceptible to overfitting the data in the estimation period, so watch out for models that have suspiciously low error measures in the estimation period and disappointingly high error measures in the validation period.</p>

<blockquote>
  <p>Be aware that if you test a large number of models and rigorously rank them on the basis of their validation period statistics, you may end up with just as much “data snooping bias” as if you had only looked at estimation-period statistics–i.e., you may end up picking a model that is more lucky than good! <strong>The best defense against this is to choose the simplest and most intuitively plausible model that gives comparatively good results.</strong></p>
</blockquote>

<h2 id="whats-the-bottom-line-how-to-compare-models">
<a class="anchor" href="#whats-the-bottom-line-how-to-compare-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>What’s the bottom line? How to compare models</h2>
<p>After fitting a number of different regression or time series forecasting models to a given data set, you have many criteria by which they can be compared:</p>

<ul>
  <li>Error measures in the estimation period: root mean squared error, mean absolute error, mean absolute percentage error, mean absolute scaled error, mean error, mean percentage error</li>
  <li>Error measures in the validation period (if you have done out-of-sample testing): Ditto</li>
  <li>Residual diagnostics and goodness-of-fit tests: plots of actual and predicted values; plots of residuals versus time, versus predicted values, and versus other variables; residual autocorrelation plots, cross-correlation plots, and tests for normally distributed errors; measures of extreme or influential observations; tests for excessive runs, changes in mean, or changes in variance (lots of things that can be “OK” or “not OK”)</li>
  <li>Qualitative considerations: intuitive reasonableness of the model, simplicity of the model, and above all, usefulness for decision making!</li>
</ul>

<p>The bottom line is that you should put the most weight on the error measures in the estimation period–most often the RMSE, but sometimes MAE or MAPE–when comparing among models.</p>

<p>The MASE statistic provides a very useful reality check for a model fitted to time series data: is it any better than a naive model?</p>

<p>You may also want to look at Cp, AIC or BIC, which more heavily penalize model complexity. But you should keep an eye on the residual diagnostic tests, cross-validation tests (if available), and qualitative considerations such as the intuitive reasonableness and simplicity of your model.</p>

<p><strong>K.I.S.S. (keep it simple…)</strong>: If two models are generally similar in terms of their error statistics and other diagnostics, you should prefer the one that is simpler and/or easier to understand.</p>

<h1 id="5-arima-models-for-time-series-forecasting">
<a class="anchor" href="#5-arima-models-for-time-series-forecasting" aria-hidden="true"><span class="octicon octicon-link"></span></a>5. ARIMA models for time series forecasting</h1>

<p><strong>A</strong>uto-<strong>R</strong>egressive <strong>I</strong>ntegrated <strong>M</strong>oving <strong>A</strong>verage</p>

<h2 id="what-arima-stands-for">
<a class="anchor" href="#what-arima-stands-for" aria-hidden="true"><span class="octicon octicon-link"></span></a>What ARIMA stands for</h2>
<ul>
  <li>Series which needs to be differenced to be made stationary = an “integrated” (<strong>I</strong>) series</li>
  <li>Lags of the stationarized series are called “auto-regressive” (<strong>AR</strong>) terms</li>
  <li>Lags of the forecast errors are called “moving average” (<strong>MA</strong>) terms</li>
</ul>

<h2 id="arima-models-put-it-all-together">
<a class="anchor" href="#arima-models-put-it-all-together" aria-hidden="true"><span class="octicon octicon-link"></span></a>ARIMA models put it all together</h2>
<ul>
  <li>Generalized random walk models fine-tuned to eliminate all residual autocorrelation</li>
  <li>Generalized exponential smoothing models that can incorporate long-term trends and seasonality</li>
  <li>Stationarized regression models that use lags of the dependent variables and/or lags of the forecast errors as regressors</li>
  <li>The most general class of forecasting models for time series that can be stationarized* by transformations such as differencing, logging, and or deflating</li>
</ul>

<h2 id="construction-of-an-arima-model">
<a class="anchor" href="#construction-of-an-arima-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Construction of an ARIMA model</h2>
<ol>
  <li>Stationarize the series, if necessary, by differencing (&amp; perhaps also logging, deflating, etc.)</li>
  <li>Study the pattern of autocorrelations and partial autocorrelations to determine if lags of the stationarized series and/or lags of the forecast errors should be included in the forecasting equation</li>
  <li>Fit the model that is suggested and check its residual diagnostics, particularly the residual ACF and PACF plots, to see if all coefficients are significant and all of the pattern has been explained.</li>
  <li>Patterns that remain in the ACF and PACF may suggest the need for additional AR or MA terms</li>
</ol>

<h2 id="arima-terminology">
<a class="anchor" href="#arima-terminology" aria-hidden="true"><span class="octicon octicon-link"></span></a>ARIMA terminology</h2>
<p>ARIMA(p,d,q) -&gt; non-seasonal ARIMA:</p>

<p>p = number of autoregressive terms</p>

<p>q = number of non-seasonal differences</p>

<p>d = number of moving-average terms</p>

<h2 id="do-you-need-both-ar-and-ma-terms">
<a class="anchor" href="#do-you-need-both-ar-and-ma-terms" aria-hidden="true"><span class="octicon octicon-link"></span></a>Do you need both AR and MA terms?</h2>
<ul>
  <li>in general, you don’t</li>
  <li>If the stationarized series has positive autocorrelation at lag 1, AR terms often work best. If it has negative autocorrelation at lag 1, MA terms often work best.</li>
  <li>An MA(1) term often works well to fine-tune the effect of a nonseasonal difference, while an AR(1) term often works well to compensate for the lack of a nonseasonal difference, so the choice between them may depend on whether a difference has been used.</li>
</ul>

<h2 id="interpretation-of-ar-terms">
<a class="anchor" href="#interpretation-of-ar-terms" aria-hidden="true"><span class="octicon octicon-link"></span></a>Interpretation of AR terms</h2>
<p>Autoregressive (AR) behavior: apparently feels a “restoring force” that tends to pull it back toward its mean</p>

<h2 id="interpretation-of-ma-terms">
<a class="anchor" href="#interpretation-of-ma-terms" aria-hidden="true"><span class="octicon octicon-link"></span></a>Interpretation of MA terms</h2>
<p>Moving average (MA) behavior: apparently undergoes random “shocks” whose effects are felt in two or more consecutive periods</p>

<h2 id="tools-for-identifying-arima-models-acf-and-pacf-plots">
<a class="anchor" href="#tools-for-identifying-arima-models-acf-and-pacf-plots" aria-hidden="true"><span class="octicon octicon-link"></span></a>Tools for identifying ARIMA models: ACF and PACF plots</h2>
<ul>
  <li>Autocorrelation function (ACF) plot: shows the correlation of the series with itself at different lags</li>
  <li>Partial autocorrelation function (PACF) plot: shows the amount of autocorrelation at lag <code class="highlighter-rouge">k</code> that is not explained by lower-order autocorrelations</li>
</ul>

<h2 id="ar-and-ma-signatures">
<a class="anchor" href="#ar-and-ma-signatures" aria-hidden="true"><span class="octicon octicon-link"></span></a>AR and MA “signatures”</h2>
<ul>
  <li>ACF that dies out gradually and PACF that cuts off sharply after a few lags -&gt; <strong>AR signature</strong>
    <ul>
      <li>AR series is usually <em>positive autocorrelated at lag 1</em>
</li>
    </ul>
  </li>
  <li>ACF that cuts off sharply after a few lags and PACF that dies out more gradually -&gt; <strong>MA signature</strong>
    <ul>
      <li>MA series is usually <em>negatively autocorrelated at lag 1</em>
</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>Whether a series displays AR or MA behavior often depends on the extent to which it has been differenced. “Underdifferenced” series -&gt; AR signature (positive autocorrelation). After one or more orders of differencing, the autocorrelation will become more negative and an MA signature will emerge</p>
</blockquote>

<h2 id="model-fitting-steps">
<a class="anchor" href="#model-fitting-steps" aria-hidden="true"><span class="octicon octicon-link"></span></a>Model-fitting steps</h2>
<ol>
  <li>Determine the order of differencing</li>
  <li>Determine the numbers of AR &amp; MA terms</li>
  <li>Fit the model—check to see if residuals are “white noise,” highest-order coefficients are significant (w/ no “unit “roots”), and forecasts look reasonable. If not, return to step 1 or 2.</li>
</ol>

<h2 id="technical-issues">
<a class="anchor" href="#technical-issues" aria-hidden="true"><span class="octicon octicon-link"></span></a>Technical issues</h2>
<ul>
  <li>
<strong>Backforecasting</strong>: Estimation algorithm begins by forecasting backward into the past to get start-up values</li>
  <li>
<strong>Unit roots</strong>: Look at sum of AR coefficients and sum of MA coefficients—if they are too close to 1 you may want to consider higher or lower of differencing</li>
  <li>
<strong>Overdifferencing</strong>: A series that has been differenced one too many times will show very strong negative autocorrelation and a strong MA signature, probably with a unit root in MA coefficients</li>
</ul>

<h2 id="seasonal-arima-models">
<a class="anchor" href="#seasonal-arima-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>Seasonal ARIMA models</h2>
<p>Rely on seasonal lags and differences to fit the seasonal pattern. Generalizes the regression approach.</p>

<h3 id="terminology">
<a class="anchor" href="#terminology" aria-hidden="true"><span class="octicon octicon-link"></span></a>Terminology</h3>
<p>Seasonal part of an ARIMA model is summarized by three additional numbers:</p>
<ul>
  <li>P = # of seasonal autoregressive terms</li>
  <li>D = # of seasonal differences</li>
  <li>Q = # of seasonal moving average terms</li>
</ul>

<p>“ARIMA(p,d,q)x(P,D,Q)” model</p>

<blockquote>
  <p>P, D and Q should never be larger than 1</p>
</blockquote>

<h3 id="model-fitting-steps-1">
<a class="anchor" href="#model-fitting-steps-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Model fitting steps</h3>
<ul>
  <li>Start by trying various combinations of one seasonal difference and/or one non-seasonal difference to stationarize the series and remove gross features of seasonal pattern.</li>
  <li>If the seasonal pattern is strong and stable, you MUST use a seasonal difference (otherwise it will “die out” in long-term forecasts)</li>
  <li>After differencing, inspect the ACF and PACF at
multiples of the seasonal period (s):
    <ul>
      <li>Positive spikes in ACF at lag s, 2s, 3s…, single positive spike in PACF at lag s -&gt; SAR=1</li>
      <li>Negative spike in ACF at lag s, negative spikes in PACF at lags s, 2s, 3s,… -&gt; SMA=1</li>
      <li>SMA=1 often works well in conjunction with a seasonal difference.</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>Same principles as for non-seasonal models, except focused on what happens at multiples of lag <code class="highlighter-rouge">s</code> in ACF and PACF.</p>
</blockquote>

<h2 id="bottom-line-suggestion">
<a class="anchor" href="#bottom-line-suggestion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bottom-line suggestion</h2>
<p>Strong seasonal pattern, try:</p>
<ul>
  <li>ARIMA(0,1,q)x(0,1,1) model (q=1 or 2)</li>
  <li>ARIMA(p,0,0)x(0,1,1)+c model (p=1, 2 or 3)</li>
  <li>If there is a significant trend and/or the seasonal pattern is multiplicative, you should also try a natural log transformation.</li>
</ul>

<h2 id="take-aways">
<a class="anchor" href="#take-aways" aria-hidden="true"><span class="octicon octicon-link"></span></a>Take-aways</h2>
<ul>
  <li>
<strong>Advantages</strong>: solid underlying theory, stable estimation of time-varying trends and seasonal patterns, relatively few parameters.</li>
  <li>
<strong>Drawbacks</strong>: no explicit seasonal indices, hard to interpret coefficients or explain “how the model works”, danger of overfitting or mis-identification if not used with care.</li>
</ul>

<h2 id="seasonal-differencing-in-arima-models">
<a class="anchor" href="#seasonal-differencing-in-arima-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>Seasonal differencing in ARIMA models</h2>
<p><strong>Seasonal difference</strong>: series of changes from one season to the next. i.e. 12 periods in a season -&gt; seasonal difference = <code class="highlighter-rouge">Y(t)-Y(t-12)</code></p>

<p>Usually removes the gross features of seasonality from a series, as well as most of the trend.</p>

<p><strong>First difference of the seasonal difference</strong> = <code class="highlighter-rouge">(Y(t)-Y(t-12))-(Y(t-1)-Y(t-13))</code></p>

<h2 id="summary-of-rules-for-identifying-arima-models">
<a class="anchor" href="#summary-of-rules-for-identifying-arima-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>Summary of rules for identifying ARIMA models</h2>
<p><a href="https://people.duke.edu/~rnau/arimrule.htm">Direct Source for the text below</a></p>

<h3 id="identifying-the-order-of-differencing-and-the-constant">
<a class="anchor" href="#identifying-the-order-of-differencing-and-the-constant" aria-hidden="true"><span class="octicon octicon-link"></span></a>Identifying the order of differencing and the constant:</h3>

<ul>
  <li>
<strong>Rule 1</strong>: If the series has positive autocorrelations out to a high number of lags (say, 10 or more), then it probably needs a higher order of differencing.</li>
  <li>
<strong>Rule 2</strong>: If the lag-1 autocorrelation is zero or negative, or the autocorrelations are all small and patternless, then the series does not need a higher order of differencing. If the lag-1 autocorrelation is -0.5 or more negative, the series may be overdifferenced. <strong>BEWARE OF OVERDIFFERENCING</strong>.</li>
  <li>
<strong>Rule 3</strong>: The optimal order of differencing is often the order of differencing at which the standard deviation is lowest. (Not always, though. Slightly too much or slightly too little differencing can also be corrected with AR or MA terms. See rules 6 and 7.)</li>
  <li>
<strong>Rule 4</strong>: A model with no orders of differencing assumes that the original series is stationary (among other things, mean-reverting). A model with one order of differencing assumes that the original series has a constant average trend (e.g. a random walk or SES-type model, with or without growth). A model with two orders of total differencing assumes that the original series has a time-varying trend (e.g. a random trend or LES-type model).</li>
  <li>
<strong>Rule 5</strong>: A model with no orders of differencing normally includes a constant term (which allows for a non-zero mean value). A model with two orders of total differencing normally does not include a constant term. In a model with one order of total differencing, a constant term should be included if the series has a non-zero average trend.</li>
</ul>

<h3 id="identifying-the-numbers-of-ar-and-ma-terms">
<a class="anchor" href="#identifying-the-numbers-of-ar-and-ma-terms" aria-hidden="true"><span class="octicon octicon-link"></span></a>Identifying the numbers of AR and MA terms:</h3>

<ul>
  <li>
<strong>Rule 6</strong>: If the partial autocorrelation function (PACF) of the differenced series displays a sharp cutoff and/or the lag-1 autocorrelation is positive–i.e., if the series appears slightly “underdifferenced”–then consider adding one or more AR terms to the model. The lag beyond which the PACF cuts off is the indicated number of AR terms.</li>
  <li>
<strong>Rule 7</strong>: If the autocorrelation function (ACF) of the differenced series displays a sharp cutoff and/or the lag-1 autocorrelation is negative–i.e., if the series appears slightly “overdifferenced”–then consider adding an MA term to the model. The lag beyond which the ACF cuts off is the indicated number of MA terms.</li>
  <li>
<strong>Rule 8</strong>: It is possible for an AR term and an MA term to cancel each other’s effects, so if a mixed AR-MA model seems to fit the data, also try a model with one fewer AR term and one fewer MA term–particularly if the parameter estimates in the original model require more than 10 iterations to converge. <strong>BEWARE OF USING MULTIPLE AR TERMS AND MULTIPLE MA TERMS IN THE SAME MODEL</strong>.</li>
  <li>
<strong>Rule 9</strong>: If there is a unit root in the AR part of the model–i.e., if the sum of the AR coefficients is almost exactly 1–you should reduce the number of AR terms by one and increase the order of differencing by one.</li>
  <li>
<strong>Rule 10</strong>: If there is a unit root in the MA part of the model–i.e., if the sum of the MA coefficients is almost exactly 1–you should reduce the number of MA terms by one and reduce the order of differencing by one.</li>
  <li>
<strong>Rule 11</strong>: If the long-term forecasts* appear erratic or unstable, there may be a unit root in the AR or MA coefficients.</li>
</ul>

<h3 id="identifying-the-seasonal-part-of-the-model">
<a class="anchor" href="#identifying-the-seasonal-part-of-the-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Identifying the seasonal part of the model:</h3>

<ul>
  <li>
<strong>Rule 12</strong>: If the series has a strong and consistent seasonal pattern, then you must use an order of seasonal differencing (otherwise the model assumes that the seasonal pattern will fade away over time). However, never use more than one order of seasonal differencing or more than 2 orders of total differencing (seasonal+nonseasonal).</li>
  <li>
<strong>Rule 13</strong>: If the autocorrelation of the appropriately differenced series is positive at lag s, where s is the number of periods in a season, then consider adding an SAR term to the model. If the autocorrelation of the differenced series is negative at lag s, consider adding an SMA term to the model. The latter situation is likely to occur if a seasonal difference has been used, which should be done if the data has a stable and logical seasonal pattern. The former is likely to occur if a seasonal difference has not been used, which would only be appropriate if the seasonal pattern is not stable over time. You should try to avoid using more than one or two seasonal parameters (SAR+SMA) in the same model, as this is likely to lead to overfitting of the data and/or problems in estimation.</li>
</ul>

<blockquote>
  <p><strong>A caveat about long-term forecasting in general</strong>: linear time series models such as ARIMA and exponential smoothing models predict the more distant future by making a series of one-period-ahead forecasts and plugging them in for unknown future values as they look farther ahead. However, the models are identified and optimized based on their one-period-ahead forecasting performance, and rigid extrapolation of them may not be the best way to forecast many periods ahead (say, more than one year when working with monthly or quarterly business data), particularly when the modeling assumptions are at best only approximately satisfied (which is nearly always the case). If one of your objectives is to generate long-term forecasts, it would be good to also draw on other sources of information during the model selection process and/or to optimize the parameter estimates for multi-period forecasting if your software allows it and/or use an auxiliary model (possibly one that incorporates expert opinion) for long-term forecasting.</p>
</blockquote>

<h1 id="6-choosing-the-right-forecasting-model">
<a class="anchor" href="#6-choosing-the-right-forecasting-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>6. Choosing the right forecasting model</h1>
<h2 id="steps-in-choosing-a-forecasting-model">
<a class="anchor" href="#steps-in-choosing-a-forecasting-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Steps in choosing a forecasting model</h2>
<h3 id="deflation">
<a class="anchor" href="#deflation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Deflation?</h3>
<p>If the series show inflationary growth. Help to account for the growth pattern and reduce heteroscedasticity in the residuals.</p>
<h3 id="logarithm-transformation">
<a class="anchor" href="#logarithm-transformation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Logarithm transformation?</h3>
<p>If the series shows compound growth and/or a multiplicative seasonal pattern, a logarithm transformation may be helpful in addition to or lieu of deflation. Logging the data will not flatten an inflationary growth pattern, but it will straighten it out it so that it can be fitted by a linear model (e.g., a random walk or ARIMA model with constant growth, or a linear exponential smoothing model).</p>

<p>Convert multiplicative seasonal patterns to additive patterns, so that if you perform seasonal adjustment after logging, you should use the additive type.</p>

<p>Another important use for the log transformation is linearizing relationships among variables in a regression model</p>

<h3 id="seasonal-adjustment-1">
<a class="anchor" href="#seasonal-adjustment-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Seasonal adjustment?</h3>
<p>If the series has a strong seasonal pattern which is believed to be constant from year to year, seasonal adjustment may be an appropriate way to estimate and extrapolate the pattern.</p>

<p>The advantage of seasonal adjustment is that it models the seasonal pattern explicitly, giving you the option of studying the seasonal indices and the seasonally adjusted data.</p>

<p>The disadvantage is that it requires the estimation of a large number of additional parameters (particularly for monthly data), and it provides no theoretical rationale for the calculation of “correct” confidence intervals</p>

<h3 id="independent-variables">
<a class="anchor" href="#independent-variables" aria-hidden="true"><span class="octicon octicon-link"></span></a>Independent variables?</h3>
<p>If there are other time series which you believe to have explanatory power with respect to your series of interest (e.g., leading economic indicators or policy variables such as price, advertising, promotions, etc.)</p>

<h3 id="smoothing-averaging-or-random-walk">
<a class="anchor" href="#smoothing-averaging-or-random-walk" aria-hidden="true"><span class="octicon octicon-link"></span></a>Smoothing, averaging, or random walk?</h3>
<p>If you have chosen to seasonally adjust the data–or if the data are not seasonal to begin with–then you may wish to use an <strong>averaging or smoothing model</strong> to fit the nonseasonal pattern which remains in the data at this point</p>

<p>If smoothing or averaging does not seem to be helpful–i.e., if the best predictor of the next value of the time series is simply its previous value–then a <strong>random walk model</strong> is indicated.</p>

<p><strong>Brown’s linear exponential smoothing</strong> can be used to fit a series with slowly time-varying linear trends, but be cautious about extrapolating such trends very far into the future.</p>

<p><strong>Holt’s linear smoothing</strong> also estimates time-varying trends, but uses separate parameters for smoothing the level and trend, which usually provides a better fit to the data than Brown’s model.</p>

<h3 id="winters-seasonal-exponential-smoothing">
<a class="anchor" href="#winters-seasonal-exponential-smoothing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Winters Seasonal Exponential Smoothing?</h3>
<p>Extension of exponential smoothing that simultaneously estimates time-varying level, trend, and seasonal factors using recursive equations. (Thus, if you use this model, you would not first seasonally adjust the data.)</p>

<p>The Winters seasonal factors can be either multiplicative or additive: normally you should choose the multiplicative option unless you have logged the data. Although the Winters model is clever and reasonably intuitive, it can be tricky to apply in practice: it has three smoothing parameters–alpha, beta, and gamma–for separately smoothing the level, trend, and seasonal factors, which must be estimated simultaneously.</p>

<h3 id="arima">
<a class="anchor" href="#arima" aria-hidden="true"><span class="octicon octicon-link"></span></a>ARIMA?</h3>
<p>If you do not choose seasonal adjustment (or if the data are non-seasonal), you may wish to use the ARIMA model framework.</p>

<p>ARIMA models are a very general class of models that includes random walk, random trend, exponential smoothing, and autoregressive models as special cases.</p>

<p>The conventional wisdom is that a series is a good candidate for an ARIMA model if:</p>
<ul>
  <li>(i) it can be stationarized by a combination of differencing and other mathematical transformations such as logging, and</li>
  <li>(ii) you have a substantial amount of data to work with: at least 4 full seasons in the case of seasonal data. (If the series cannot be adequately stationarized by differencing–e.g., if it is very irregular or seems to be qualitatively changing its behavior over time–or if you have fewer than 4 seasons of data, then you might be better off with a model that uses seasonal adjustment and some kind of simple averaging or smoothing.)</li>
</ul>

<h4 id="steps">
<a class="anchor" href="#steps" aria-hidden="true"><span class="octicon octicon-link"></span></a>Steps</h4>
<ol>
  <li>Determine the appropriate order of differencing needed to stationarize the series and remove the gross features of seasonality</li>
  <li>Determine whether to include a constant term in the model: usually you do include a constant term if the total order of differencing is 1 or less, otherwise you don’t</li>
  <li>Choose the numbers of autoregressive and moving average parameters (p, d, q, P, D, Q) that are needed to eliminate any autocorrelation that remains in the residuals of the naive model (i.e., any correlation that remains after mere differencing).</li>
</ol>

<blockquote>
  <p>It is usually better to proceed in a forward stepwise rather than backward stepwise fashion when tweaking the model specifications: start with simpler models and only add more terms if there is a clear need.</p>
</blockquote>

<h2 id="forecasting-flow-chart">
<a class="anchor" href="#forecasting-flow-chart" aria-hidden="true"><span class="octicon octicon-link"></span></a>Forecasting Flow Chart</h2>
<p>Source: <a href="https://people.duke.edu/~rnau/411flow.gif">https://people.duke.edu/~rnau/411flow.gif</a></p>

<p><img src="https://people.duke.edu/~rnau/411flow.gif" title="flow_chart" width="800"></p>

<h2 id="automatic-forecasting-software">
<a class="anchor" href="#automatic-forecasting-software" aria-hidden="true"><span class="octicon octicon-link"></span></a>Automatic Forecasting Software</h2>
<blockquote>
  <p><strong>WARNING</strong>: Properly used, and guided by experience with “manual” model-fitting, the best of such software can put more data-analysis power at your fingertips and speed up routine forecasting applications. Carelessly used, it merely allows you to foul things up in a bigger way, obtaining results without insight while getting a false sense of security that “the computer knows best.” <strong>Automatic forecasting software is a complement to, not a substitute for, your own forecasting expertise.</strong></p>
</blockquote>

<h2 id="how-to-avoid-trouble-principles-of-good-data-analysis">
<a class="anchor" href="#how-to-avoid-trouble-principles-of-good-data-analysis" aria-hidden="true"><span class="octicon octicon-link"></span></a>How to avoid trouble: principles of good data analysis</h2>
<p>Source: <a href="https://people.duke.edu/~rnau/notroubl.htm">https://people.duke.edu/~rnau/notroubl.htm</a></p>

<ul>
  <li>Do some background research before running any numbers.  Be sure you understand the objective, the theory, the jargon, and the conventional wisdom. Ask others what they know about the problem. Do more of the same web-searching that brought you to this site, and be critical of everything you find.</li>
  <li>Be thorough in your search for data.  In the end, your forecasts will contain no information that is not hidden in it somewhere.  In some cases (say, designed experiments) the data may be limited in scope and already in hand, but in many cases it will be up to you to identify and collect it, and this may the most important and time-consuming part of the project.  We now live in the era of “megadata”, but often the data that you need is not so easy to find.</li>
  <li>Check the data carefully once you have it:  make sure you know where the numbers came from, what they mean, how they were measured, how they are aligned in time, and whether they are accurate. A lot of cleaning and merging of data may be needed before any analysis can begin, and you should know how to do it. You may find that you need yet-more-data or better-quality data to answer the key questions, and sometimes the most useful lesson is that the organization will need to do a better job of managing its data in the future.</li>
  <li>Use descriptive, self-explanatory names for your variables (not Y’s and X’s or cryptological character strings) so that their meaning and units are clear and so that your computer output is self-documenting to the greatest extent possible.</li>
  <li>Once you begin your analysis, follow good modeling practices:  perform exploratory data analysis, use appropriate model types, interpret their estimated parameters, check their residual diagnostics, question their assumptions, and validate them with out-of-sample testing where possible.</li>
  <li>Make effective use of statistical graphics in your own analysis and in your presentation of your results to others, and follow rules of good graphics.  Among other things: graphs should be self-explanatory and not contain visual puzzles, they should have titles that are specific to the variables and models, axes should be well-scaled and well-labeled, the data area should have a white background, grid lines (if any are needed) should not be too dark, point and line widths should be sized appropriately for the density of the data, self-promoting artwork (3-D perspective, etc.) should be avoided, and above all, the data should tell its own story.</li>
  <li>Keep in mind that not all relationships are linear and additive, not all randomness is normally distributed, and regression models are not magic boxes that can predict anything from anything.  Be aware that it may be necessary to transform some of your variables (through deflating, logging, differencing, etc.) in order to match their patterns up with each other in the way that linear models require.</li>
  <li>When comparing models, focus on the right objectives, which are usually making the smallest possible errors in the future and deriving inferences that are genuinely useful for decision making.  A good fit to past data does not always guarantee an equally good prediction of what will happen next, and statistical significance is not always the same as practical significance.</li>
  <li>Other things being equal, KEEP IT SIMPLE and intuitively reasonable. If others don’t understand the model, they may not use it, and perhaps they shouldn’t: simple models often outperform complicated models in practice.</li>
  <li>If you use automatic forecasting software, you are still responsible for the model that is chosen, and you should be able to explain its logic to others.  The availability of such software does not make it unnecessary to know how the models work.  Rather, it makes that knowledge even more important. Be aware that in automatic rankings of models, there may be only tiny differences in error stats between the winner and its near competitors, and you may need to take other factors into account in your final selection, such as simplicity, clarity, and intuition. And again, your software cannot make something out of nothing. If your data is not informative or not properly organized to begin with, automatic methods will not turn it into gold.</li>
  <li>Leave a paper trail, i.e., keep well-annotated records of your model-fitting efforts.  Don’t just save your computer files:  write up notes as you go along.  Someone else (who may be a sharp-penciled auditor or perhaps only yourself 12 months hence) may need to reconstruct what you did and why you did it. Intelligent naming of variables and labeling of tables and charts will make this easier.</li>
  <li>Neither overstate nor understate the accuracy of your forecast, and do not merely give a point value. Always report standard errors and/or confidence intervals.</li>
  <li>If different forecasting approaches lead to different results, focus on differences in their underlying assumptions, their sources of data, their intrinsic biases, and their respective margins for error.  Don’t just argue over the outputs.  Look for opportunities to combine independent viewpoints and information.</li>
  <li>After all has been said, if you believe your model is more useful and better supported than the alternatives, stand up for it.  The future may still be very uncertain, but decisions ought to be based on the best understanding of that uncertainty. If you don’t have confidence in your model (or anyone else’s), admit it, and keep digging.</li>
</ul>

  </div><a class="u-url" href="/blog/book/time%20series/forecasting/data%20science/machine%20learning/statistics/2020/02/03/statistical-forecasting.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Data Science and Machine Learning blog.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/millengustavo" title="millengustavo"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/millengustavo" title="millengustavo"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/millengustavo" title="millengustavo"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
